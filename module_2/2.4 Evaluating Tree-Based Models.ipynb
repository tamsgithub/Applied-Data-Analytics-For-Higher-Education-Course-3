{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Evaluating Tree-Based Models\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nIn this notebook, we perform a **thorough evaluation** of our three tuned tree-based models. We go beyond simple accuracy to examine ROC curves, precision-recall trade-offs, confusion matrices, calibration, and feature importance\u2014all critical for deploying models in a higher education context.\n\n### Learning Objectives\n\n1. Generate and interpret ROC and Precision-Recall curves\n2. Analyze confusion matrices for each model\n3. Compare feature importances across models\n4. Understand probability calibration and threshold selection\n5. Make an informed recommendation for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n    confusion_matrix, classification_report, brier_score_loss)\n\nRANDOM_STATE = 42\n\n# Load and prepare data\ntrain_df = pd.read_csv('../../data/training.csv')\ntest_df = pd.read_csv('../../data/testing.csv')\ntrain_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\ntest_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n\nnumeric_features = ['HS_GPA','HS_MATH_GPA','HS_ENGL_GPA','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2',\n    'UNITS_COMPLETED_1','UNITS_COMPLETED_2','DFW_UNITS_1','DFW_UNITS_2','GPA_1','GPA_2',\n    'DFW_RATE_1','DFW_RATE_2','GRADE_POINTS_1','GRADE_POINTS_2']\ncategorical_features = ['RACE_ETHNICITY','GENDER','FIRST_GEN_STATUS','COLLEGE']\n\ntrain_enc = pd.get_dummies(train_df[numeric_features + categorical_features],\n                           columns=categorical_features, drop_first=True)\ntest_enc = pd.get_dummies(test_df[numeric_features + categorical_features],\n                          columns=categorical_features, drop_first=True)\ntrain_enc, test_enc = train_enc.align(test_enc, join='left', axis=1, fill_value=0)\ntrain_enc = train_enc.fillna(train_enc.median())\ntest_enc = test_enc.fillna(test_enc.median())\n\nX_train, y_train = train_enc, train_df['DEPARTED']\nX_test, y_test = test_enc, test_df['DEPARTED']\n\n# Train models with good hyperparameters\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(max_depth=8, min_samples_split=20,\n        min_samples_leaf=10, class_weight='balanced', random_state=RANDOM_STATE),\n    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=12,\n        min_samples_leaf=5, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE),\n    'XGBoost': XGBClassifier(n_estimators=150, learning_rate=0.1, max_depth=5,\n        subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='logloss',\n        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n        random_state=RANDOM_STATE)\n}\n\npredictions, probabilities = {}, {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    predictions[name] = model.predict(X_test)\n    probabilities[name] = model.predict_proba(X_test)[:, 1]\n\nprint(\"All models trained and predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\n\nfor i, (name, prob) in enumerate(probabilities.items()):\n    fpr, tpr, _ = roc_curve(y_test, prob)\n    auc = roc_auc_score(y_test, prob)\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n        name=f'{name} (AUC={auc:.3f})', line=dict(color=colors[i], width=2)))\n\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random',\n    line=dict(color='gray', dash='dash', width=1)))\n\nfig.update_layout(title='ROC Curves: Tree-Based Models', height=500,\n    xaxis_title='False Positive Rate', yaxis_title='True Positive Rate',\n    yaxis=dict(scaleanchor='x', scaleratio=1))\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\nfor i, (name, prob) in enumerate(probabilities.items()):\n    prec, rec, _ = precision_recall_curve(y_test, prob)\n    ap = average_precision_score(y_test, prob)\n    fig.add_trace(go.Scatter(x=rec, y=prec, mode='lines',\n        name=f'{name} (AP={ap:.3f})', line=dict(color=colors[i], width=2)))\n\nprevalence = y_test.mean()\nfig.add_hline(y=prevalence, line_dash='dash', line_color='gray',\n              annotation_text=f'Baseline ({prevalence:.1%})')\n\nfig.update_layout(title='Precision-Recall Curves: Tree-Based Models', height=500,\n    xaxis_title='Recall', yaxis_title='Precision')\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3, subplot_titles=list(models.keys()))\n\nfor col, (name, pred) in enumerate(predictions.items(), 1):\n    cm = confusion_matrix(y_test, pred)\n    labels = [['TN', 'FP'], ['FN', 'TP']]\n    text = [[f'{labels[i][j]}<br>{cm[i][j]}' for j in range(2)] for i in range(2)]\n\n    fig.add_trace(go.Heatmap(z=cm, text=text, texttemplate='%{text}',\n        colorscale='Blues', showscale=False,\n        x=['Predicted Enrolled', 'Predicted Departed'],\n        y=['Actual Enrolled', 'Actual Departed']), row=1, col=col)\n\nfig.update_layout(title='Confusion Matrices', height=400)\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({'Feature': X_train.columns})\nfor name, model in models.items():\n    importance_df[name] = model.feature_importances_\n\n# Top 10 by average importance\nimportance_df['Average'] = importance_df[list(models.keys())].mean(axis=1)\ntop_features = importance_df.nlargest(10, 'Average')\n\nfig = go.Figure()\nfor i, name in enumerate(models.keys()):\n    sorted_df = top_features.sort_values(name)\n    fig.add_trace(go.Bar(y=sorted_df['Feature'], x=sorted_df[name],\n        orientation='h', name=name, marker_color=colors[i]))\n\nfig.update_layout(title='Top 10 Features by Importance', barmode='group',\n    height=450, xaxis_title='Importance')\nfig.show()\n\nprint(\"Top features consistently important across all models:\")\nfor _, row in top_features.nlargest(5, 'Average').iterrows():\n    print(f\"  {row['Feature']}: avg importance = {row['Average']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations\n\n### Model Comparison Summary\n\n| Criterion | Decision Tree | Random Forest | XGBoost |\n|:----------|:-------------|:-------------|:--------|\n| **Best for** | Explainability | Reliability | Performance |\n| **Use when** | Advisors need to understand | You want a safe default | Accuracy matters most |\n| **Avoid when** | Performance is critical | Training speed matters | Simplicity is required |\n\n### For Higher Education Deployment\n\n1. **Advisor-facing tools**: Use Decision Tree (or extract rules from it)\n2. **Batch risk scoring**: Use Random Forest or XGBoost\n3. **Research/grants**: Use XGBoost (best metrics for publications)\n4. **Early warning systems**: Use Random Forest (reliable, handles missing data gracefully)\n\n### Next Module\n\nWe will compare these tree-based models against Regularized Logistic Regression in a systematic model comparison framework.\n\n**Proceed to:** `Module 3: Model Comparison and Selection`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}