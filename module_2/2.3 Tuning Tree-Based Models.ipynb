{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Tuning Tree-Based Models\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nIn notebook 2.2, we built all three tree-based models using default or reasonable hyperparameters. Now we learn to **tune** these models systematically to improve performance. Again, the approach is consistent across all three models\u2014scikit-learn provides a unified tuning API.\n\n### Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Use `GridSearchCV` and `RandomizedSearchCV` with any tree-based model\n2. Identify the most impactful hyperparameters for each model\n3. Apply early stopping with XGBoost to prevent overfitting\n4. Select optimal models using cross-validated performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import (GridSearchCV, RandomizedSearchCV,\n                                      StratifiedKFold, cross_val_score)\nfrom sklearn.metrics import roc_auc_score, make_scorer\nimport time\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Load and prepare data (same as 2.2)\ntrain_df = pd.read_csv('../../data/training.csv')\ntest_df = pd.read_csv('../../data/testing.csv')\ntrain_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\ntest_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n\nnumeric_features = ['HS_GPA','HS_MATH_GPA','HS_ENGL_GPA','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2',\n    'UNITS_COMPLETED_1','UNITS_COMPLETED_2','DFW_UNITS_1','DFW_UNITS_2','GPA_1','GPA_2',\n    'DFW_RATE_1','DFW_RATE_2','GRADE_POINTS_1','GRADE_POINTS_2']\ncategorical_features = ['RACE_ETHNICITY','GENDER','FIRST_GEN_STATUS','COLLEGE']\n\ntrain_enc = pd.get_dummies(train_df[numeric_features + categorical_features],\n                           columns=categorical_features, drop_first=True)\ntest_enc = pd.get_dummies(test_df[numeric_features + categorical_features],\n                          columns=categorical_features, drop_first=True)\ntrain_enc, test_enc = train_enc.align(test_enc, join='left', axis=1, fill_value=0)\ntrain_enc = train_enc.fillna(train_enc.median())\ntest_enc = test_enc.fillna(test_enc.median())\n\nX_train, y_train = train_enc, train_df['DEPARTED']\nX_test, y_test = test_enc, test_df['DEPARTED']\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nprint(f\"Data loaded: {X_train.shape[0]:,} training, {X_test.shape[0]:,} testing samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Universal Tuning Pattern\n\nJust like `fit/predict`, hyperparameter tuning follows the same pattern for all models:\n\n```python\n# 1. Define the model\nmodel = SomeClassifier()\n\n# 2. Define the search space\nparam_grid = {'param1': [val1, val2], 'param2': [val3, val4]}\n\n# 3. Run the search\nsearch = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')\nsearch.fit(X_train, y_train)\n\n# 4. Get the best model\nbest_model = search.best_estimator_\nprint(search.best_params_)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuning Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree hyperparameter search\nprint(\"Tuning Decision Tree...\")\nstart = time.time()\n\ndt_param_grid = {\n    'max_depth': [3, 5, 8, 12],\n    'min_samples_split': [10, 20, 50],\n    'min_samples_leaf': [5, 10, 20],\n    'class_weight': ['balanced', None]\n}\n\ndt_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=RANDOM_STATE),\n    dt_param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, refit=True\n)\ndt_search.fit(X_train, y_train)\n\nprint(f\"Completed in {time.time()-start:.1f}s\")\nprint(f\"Best AUC: {dt_search.best_score_:.4f}\")\nprint(f\"Best params: {dt_search.best_params_}\")\n\n# Evaluate on test set\ndt_best = dt_search.best_estimator_\ndt_prob = dt_best.predict_proba(X_test)[:, 1]\nprint(f\"Test AUC: {roc_auc_score(y_test, dt_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameter search (use RandomizedSearchCV for efficiency)\nprint(\"Tuning Random Forest...\")\nstart = time.time()\n\nrf_param_dist = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [8, 12, 16, None],\n    'min_samples_split': [5, 10, 20],\n    'min_samples_leaf': [3, 5, 10],\n    'max_features': ['sqrt', 'log2'],\n    'class_weight': ['balanced', 'balanced_subsample']\n}\n\nrf_search = RandomizedSearchCV(\n    RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE),\n    rf_param_dist, n_iter=30, cv=cv, scoring='roc_auc', n_jobs=-1,\n    random_state=RANDOM_STATE, refit=True\n)\nrf_search.fit(X_train, y_train)\n\nprint(f\"Completed in {time.time()-start:.1f}s\")\nprint(f\"Best AUC: {rf_search.best_score_:.4f}\")\nprint(f\"Best params: {rf_search.best_params_}\")\n\nrf_best = rf_search.best_estimator_\nrf_prob = rf_best.predict_proba(X_test)[:, 1]\nprint(f\"Test AUC: {roc_auc_score(y_test, rf_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameter search\nprint(\"Tuning XGBoost...\")\nstart = time.time()\n\nxgb_param_dist = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 7],\n    'min_child_weight': [1, 3, 5],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\nxgb_search = RandomizedSearchCV(\n    XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n                  scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n                  random_state=RANDOM_STATE),\n    xgb_param_dist, n_iter=30, cv=cv, scoring='roc_auc', n_jobs=-1,\n    random_state=RANDOM_STATE, refit=True\n)\nxgb_search.fit(X_train, y_train)\n\nprint(f\"Completed in {time.time()-start:.1f}s\")\nprint(f\"Best AUC: {xgb_search.best_score_:.4f}\")\nprint(f\"Best params: {xgb_search.best_params_}\")\n\nxgb_best = xgb_search.best_estimator_\nxgb_prob = xgb_best.predict_proba(X_test)[:, 1]\nprint(f\"Test AUC: {roc_auc_score(y_test, xgb_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBoost Early Stopping\n\nXGBoost has a powerful feature: **early stopping**. It automatically stops training when performance on a validation set stops improving, preventing overfitting without you having to guess the right `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with early stopping\nfrom sklearn.model_selection import train_test_split\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2,\n                                              stratify=y_train, random_state=RANDOM_STATE)\n\nxgb_early = XGBClassifier(\n    n_estimators=1000,  # Set high \u2014 early stopping will find the right number\n    learning_rate=0.05,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    early_stopping_rounds=20,  # Stop if no improvement for 20 rounds\n    random_state=RANDOM_STATE\n)\n\nxgb_early.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n\nprint(f\"Best iteration: {xgb_early.best_iteration}\")\nprint(f\"Stopped at: {xgb_early.best_iteration} out of 1000 max rounds\")\nprint(f\"Test AUC: {roc_auc_score(y_test, xgb_early.predict_proba(X_test)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Comparison: Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all tuned models\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ntuned_results = []\nfor name, model, prob in [('Decision Tree', dt_best, dt_prob),\n                           ('Random Forest', rf_best, rf_prob),\n                           ('XGBoost', xgb_best, xgb_prob)]:\n    pred = (prob >= 0.5).astype(int)\n    tuned_results.append({\n        'Model': name,\n        'ROC-AUC': roc_auc_score(y_test, prob),\n        'F1': f1_score(y_test, pred),\n        'Precision': precision_score(y_test, pred),\n        'Recall': recall_score(y_test, pred)\n    })\n\ntuned_df = pd.DataFrame(tuned_results)\nprint(\"=\" * 70)\nprint(\"TUNED MODELS: FINAL COMPARISON\")\nprint(\"=\" * 70)\nprint(tuned_df.to_string(index=False))\nprint(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n\n### Tuning Cheat Sheet\n\n| Model | Most Impactful Parameters | Tuning Strategy |\n|:------|:------------------------|:----------------|\n| **Decision Tree** | `max_depth`, `min_samples_leaf` | GridSearchCV (small space) |\n| **Random Forest** | `n_estimators`, `max_depth`, `max_features` | RandomizedSearchCV |\n| **XGBoost** | `learning_rate`, `max_depth`, `n_estimators` | RandomizedSearchCV + early stopping |\n\n### Key Takeaways\n\n1. **Same tuning API** for all three: `GridSearchCV` or `RandomizedSearchCV`\n2. **Decision Trees** are fast to tune but have a performance ceiling\n3. **Random Forests** are robust\u2014defaults are often near-optimal\n4. **XGBoost early stopping** is the most efficient way to find optimal rounds\n5. Always evaluate on a **held-out test set** after tuning\n\n### Next Steps\n\nIn the next notebook, we'll build a comprehensive evaluation comparing these tuned models with deeper analysis.\n\n**Proceed to:** `2.4 Evaluating Tree-Based Models`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}