{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Introduction to Tree-Based Models\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nIn Module 1, we extended logistic regression with regularization. Now we turn to **tree-based models**\u2014a family of algorithms that are among the most widely used and successful in applied machine learning. This module covers three core methods that share a common foundation:\n\n1. **Decision Trees** \u2014 the building block\n2. **Random Forests** \u2014 combining many trees via bagging\n3. **XGBoost** \u2014 combining trees via gradient boosting\n\nThese three models represent the practical toolkit you will use most often. They share a common scikit-learn API (`instantiate \u2192 fit \u2192 predict`) and are the go-to models for tabular data like student records.\n\n### Why These Three?\n\n| Model | Strategy | Key Strength |\n|:------|:---------|:-------------|\n| **Decision Tree** | Single tree with learned rules | Highly interpretable, great for stakeholder communication |\n| **Random Forest** | Many trees trained in parallel (bagging) | Robust, reduces overfitting, good default choice |\n| **XGBoost** | Trees trained sequentially (boosting) | Top performance on tabular data, competition winner |\n\nTogether, these three models cover the spectrum from maximum interpretability (Decision Tree) to maximum predictive power (XGBoost), with Random Forest as a reliable middle ground.\n\n### Learning Objectives\n\nBy the end of this module, you will be able to:\n\n1. Explain how decision trees partition feature space using impurity measures\n2. Understand how Random Forests reduce variance through bagging\n3. Understand how XGBoost reduces bias through sequential boosting\n4. **Build all three models using the same scikit-learn pattern**: `instantiate \u2192 fit \u2192 predict`\n5. Tune hyperparameters for each model family\n6. Compare models on the same student departure dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Common Pattern: Instantiate, Fit, Predict\n\nBefore diving into theory, let's establish the most important practical insight of this module: **all three models follow the exact same scikit-learn workflow**. This is by design\u2014scikit-learn's consistent API means that once you learn the pattern, switching between models is trivial.\n\n```python\n# The universal scikit-learn pattern:\n\n# Step 1: Instantiate the model with hyperparameters\nmodel = ModelClass(param1=value1, param2=value2)\n\n# Step 2: Fit the model to training data\nmodel.fit(X_train, y_train)\n\n# Step 3: Make predictions\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Step 4: Evaluate\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_test, y_prob):.3f}\")\n```\n\nThe **only thing that changes** between models is Step 1\u2014the class name and its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: The same pattern, three different models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# All three follow the EXACT same API:\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'XGBoost': XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n                              use_label_encoder=False, eval_metric='logloss', random_state=42)\n}\n\n# Same workflow for each:\nfor name, model in models.items():\n    print(f\"{name}:\")\n    print(f\"  model.fit(X_train, y_train)\")\n    print(f\"  model.predict(X_test)\")\n    print(f\"  model.predict_proba(X_test)\")\n    print()\n\nprint(\"That's it. Same three lines. Different model, same interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Decision Tree?\n\nA **decision tree** is a supervised learning algorithm that makes predictions by learning a series of simple decision rules from the data. Think of it as a flowchart:\n\n- Each **internal node** asks a yes/no question about a feature (e.g., \"Is GPA \u2264 2.5?\")\n- Each **branch** represents the answer\n- Each **leaf node** provides a prediction\n\n**Higher Education Example**: An advisor might assess student risk by asking:\n1. \"Is their GPA below 2.0?\" \u2192 High risk\n2. \"Did they fail courses in semester 1?\" \u2192 Concerning\n3. \"Are they taking fewer than 12 units?\" \u2192 May need support\n\nA decision tree automates exactly this kind of reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Visualize a simple decision tree structure\nfig = go.Figure()\n\n# Node positions\nnodes = {\n    'root': (0.5, 1.0), 'left1': (0.25, 0.7), 'right1': (0.75, 0.7),\n    'left2': (0.125, 0.4), 'right2': (0.375, 0.4),\n    'left3': (0.625, 0.4), 'right3': (0.875, 0.4)\n}\n\n# Draw edges\nedges = [('root','left1'),('root','right1'),('left1','left2'),('left1','right2'),\n         ('right1','left3'),('right1','right3')]\nfor s, e in edges:\n    fig.add_trace(go.Scatter(x=[nodes[s][0],nodes[e][0]], y=[nodes[s][1],nodes[e][1]],\n                             mode='lines', line=dict(color='gray',width=2), showlegend=False))\n\n# Decision nodes\nfig.add_trace(go.Scatter(\n    x=[nodes[n][0] for n in ['root','left1','right1']],\n    y=[nodes[n][1] for n in ['root','left1','right1']],\n    mode='markers+text', marker=dict(size=50, color='lightblue', line=dict(color='blue',width=2)),\n    text=['GPA_1 <= 2.5?', 'DFW > 0.3?', 'UNITS < 12?'],\n    textposition='middle center', textfont=dict(size=10), name='Decision Nodes'))\n\n# Leaf nodes\nfig.add_trace(go.Scatter(\n    x=[nodes[n][0] for n in ['left2','right2','left3','right3']],\n    y=[nodes[n][1] for n in ['left2','right2','left3','right3']],\n    mode='markers+text', marker=dict(size=50, color='lightgreen', symbol='square',\n    line=dict(color='green',width=2)),\n    text=['Not Enrolled','Enrolled','Not Enrolled','Enrolled'],\n    textposition='middle center', textfont=dict(size=9), name='Leaf Nodes'))\n\nfig.add_annotation(x=0.35, y=0.88, text='Yes', showarrow=False, font=dict(color='green'))\nfig.add_annotation(x=0.65, y=0.88, text='No', showarrow=False, font=dict(color='red'))\n\nfig.update_layout(title='Anatomy of a Decision Tree', height=450,\n    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 How Trees Make Splits: Gini Impurity\n\nDecision trees choose splits by finding the feature and threshold that best separates the classes. The default measure in scikit-learn is **Gini Impurity**:\n\n$$Gini = 1 - \\sum_{i=1}^{C} p_i^2$$\n\n- A **pure node** (all one class) has Gini = 0\n- **Maximum impurity** (50-50 split) has Gini = 0.5\n\nThe tree picks the split that reduces Gini the most at each step. This is a **greedy** algorithm\u2014it makes the locally optimal choice without looking ahead.\n\n### 2.2 Controlling Overfitting\n\nWithout constraints, a tree will keep splitting until every leaf is pure\u2014memorizing the training data. Key hyperparameters to prevent this:\n\n| Parameter | What It Does | Typical Range |\n|:----------|:-------------|:-------------|\n| `max_depth` | Maximum tree depth | 3\u201315 |\n| `min_samples_split` | Min samples to split a node | 5\u201350 |\n| `min_samples_leaf` | Min samples in a leaf | 3\u201320 |\n| `max_features` | Features considered per split | `'sqrt'`, `'log2'` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From One Tree to Many: Random Forests\n\nA single decision tree is interpretable but **unstable**\u2014small changes in data can produce very different trees. **Random Forests** solve this by training many trees and averaging their predictions.\n\n### The Bagging Strategy\n\n1. Create `n_estimators` bootstrap samples (random samples with replacement)\n2. Train one tree on each sample **independently**\n3. Each tree also considers only a random subset of features at each split\n4. Combine predictions by majority vote (classification) or averaging (regression)\n\n```\nTraining Data\n     |\n     +---> Bootstrap Sample 1 ---> Tree 1 ---+\n     +---> Bootstrap Sample 2 ---> Tree 2 ---+--> Majority Vote = Final Prediction\n     +---> Bootstrap Sample 3 ---> Tree 3 ---+\n     ...\n     +---> Bootstrap Sample N ---> Tree N ---+\n```\n\n**Why it works**: Each tree overfits in a different way. Averaging many diverse, overfitting trees produces a model that generalizes well. This is the variance-reduction power of ensembles.\n\n### Key Hyperparameters\n\n| Parameter | What It Does | Typical Range |\n|:----------|:-------------|:-------------|\n| `n_estimators` | Number of trees | 100\u2013500 |\n| `max_depth` | Max depth per tree | 8\u201320, or None |\n| `max_features` | Features per split | `'sqrt'` (default) |\n| `min_samples_leaf` | Min samples in leaf | 1\u201310 |\n| `class_weight` | Handle class imbalance | `'balanced'` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequential Improvement: XGBoost\n\nWhile Random Forests train trees **in parallel** and average them, **XGBoost** (eXtreme Gradient Boosting) trains trees **sequentially**, where each new tree corrects the errors of the previous ones.\n\n### The Boosting Strategy\n\n1. Start with a simple prediction (e.g., the overall departure rate)\n2. Calculate the errors (residuals)\n3. Train a small tree to predict those errors\n4. Update predictions by adding the new tree's output (scaled by a learning rate)\n5. Repeat\n\n```\nData --> Tree 1 --> Errors --> Tree 2 --> Errors --> Tree 3 --> ...\n           |                     |                     |\n        Predicts              Predicts              Predicts\n        the data          Tree 1's errors      remaining errors\n```\n\n**Key equation**: $F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$\n\nWhere $\\eta$ is the learning rate (typically 0.01\u20130.3) and $h_m$ is the new tree.\n\n### Why XGBoost Dominates Tabular Data\n\nXGBoost adds several innovations beyond basic gradient boosting:\n\n- **Built-in regularization** (L1 + L2 on leaf weights)\n- **Handles missing values** automatically\n- **Column subsampling** (like Random Forest) for diversity\n- **Early stopping** to prevent overfitting\n\n### Key Hyperparameters\n\n| Parameter | What It Does | Typical Range |\n|:----------|:-------------|:-------------|\n| `n_estimators` | Number of boosting rounds | 100\u20131000 |\n| `learning_rate` | Step size shrinkage | 0.01\u20130.3 |\n| `max_depth` | Max depth per tree (keep shallow!) | 3\u20138 |\n| `subsample` | Row sampling ratio | 0.7\u20131.0 |\n| `colsample_bytree` | Column sampling ratio | 0.7\u20131.0 |\n| `scale_pos_weight` | Handle class imbalance | ratio of neg/pos |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing the Three Approaches\n\n| Aspect | Decision Tree | Random Forest | XGBoost |\n|:-------|:-------------|:-------------|:--------|\n| **Strategy** | Single tree | Many trees in parallel | Trees in sequence |\n| **Reduces** | \u2014 | Variance (overfitting) | Bias (underfitting) |\n| **Interpretability** | Excellent | Moderate | Lower |\n| **Overfitting risk** | High | Low | Moderate (use early stopping) |\n| **Preprocessing needed** | None | None | None |\n| **Handles missing data** | No (sklearn) | No (sklearn) | Yes (native) |\n| **Training speed** | Fast | Moderate | Moderate |\n| **Typical performance** | Good baseline | Strong | Often best |\n\n### When to Use Each\n\n- **Decision Tree**: When you need to explain predictions to non-technical stakeholders (advisors, administrators)\n- **Random Forest**: When you want a reliable, robust model with minimal tuning\n- **XGBoost**: When you want the best possible predictive performance on tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n\n### Key Takeaways\n\n1. **All three models follow the same scikit-learn API**: `instantiate \u2192 fit \u2192 predict`\n2. **Decision Trees** are the building block\u2014interpretable but prone to overfitting\n3. **Random Forests** reduce variance by averaging many independent trees (bagging)\n4. **XGBoost** reduces bias by sequentially correcting errors (boosting)\n5. **No preprocessing needed**\u2014tree-based models handle raw features directly\n6. For higher education: start with Random Forest (reliable), use Decision Trees for stakeholder communication, and XGBoost when performance matters most\n\n### Connection to the ML Cycle\n\n| ML Cycle Step | Tree-Based Models |\n|:-------------|:-----------------|\n| **Build** | Choose model class and hyperparameters |\n| **Train** | `model.fit(X_train, y_train)` |\n| **Predict** | `model.predict(X_test)` / `model.predict_proba(X_test)` |\n| **Evaluate** | Classification metrics (AUC, F1, etc.) |\n| **Improve** | Tune hyperparameters, try different model |\n\n### Next Steps\n\nIn the next notebook, we will put all three models to work on our student departure dataset using the consistent scikit-learn pattern.\n\n**Proceed to:** `2.2 Building Tree-Based Models in Practice`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}