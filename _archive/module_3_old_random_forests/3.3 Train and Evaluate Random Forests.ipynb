{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 **Train and Evaluate** Random Forests - Predict Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the Random Forest pipeline.  \n",
    "### **2. Train the Model : Fit the model on the training data.**  \n",
    "### **3. Generate Predictions : Use the trained model to make predictions.**  \n",
    "### **4. Evaluate the Model : Assess performance using evaluation metrics.**  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we built several Random Forest classification pipelines. Now we train these models and evaluate their performance using multiple techniques:\n",
    "\n",
    "1. **Out-of-Bag (OOB) Error**: A \"free\" validation metric unique to bagging methods\n",
    "2. **Cross-Validation**: Standard technique for assessing generalization\n",
    "3. **Feature Importance**: Understanding which features drive predictions\n",
    "4. **Test Set Evaluation**: Final performance assessment\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Train Random Forest models on the student departure dataset\n",
    "2. Understand and use Out-of-Bag (OOB) error for model evaluation\n",
    "3. Extract and interpret feature importance from Random Forests\n",
    "4. Evaluate model performance using confusion matrices, ROC curves, and various metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "models_path = f'{course3_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "print(f\"Training data: {df_training.shape}\")\n",
    "print(f\"Testing data: {df_testing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "X_test = df_testing\n",
    "y_test = df_testing['SEM_3_STATUS']\n",
    "\n",
    "print(f\"Training target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved models\n",
    "model_files = {\n",
    "    'RF Baseline': 'rf_baseline_model.pkl',\n",
    "    'RF Large (500)': 'rf_large_500_model.pkl',\n",
    "    'RF Constrained': 'rf_constrained_model.pkl',\n",
    "    'RF Log2': 'rf_log2_model.pkl'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, filename in model_files.items():\n",
    "    filepath = f'{models_path}{filename}'\n",
    "    models[name] = pickle.load(open(filepath, 'rb'))\n",
    "    print(f\"Loaded: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training the Baseline Model\n",
    "\n",
    "Let's first train and examine the baseline Random Forest model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model\n",
    "print(\"Training RF Baseline model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "models['RF Baseline'].fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the trained model\n",
    "rf_classifier = models['RF Baseline'].named_steps['classifier']\n",
    "\n",
    "print(\"Trained Random Forest Properties:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of trees: {rf_classifier.n_estimators}\")\n",
    "print(f\"Number of features seen: {rf_classifier.n_features_in_}\")\n",
    "print(f\"Classes: {rf_classifier.classes_}\")\n",
    "print(f\"OOB Score available: {hasattr(rf_classifier, 'oob_score_')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training All Models\n",
    "\n",
    "Now let's train all our Random Forest models and record their training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and record times\n",
    "training_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_times[name] = time.time() - start_time\n",
    "    print(f\"  Completed in {training_times[name]:.2f} seconds\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training times\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(training_times.keys()),\n",
    "    y=list(training_times.values()),\n",
    "    marker_color=['darkblue', 'blue', 'lightblue', 'steelblue']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest Training Times',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Training Time (seconds)',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Out-of-Bag (OOB) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is OOB Error?\n",
    "\n",
    "**Out-of-Bag (OOB) error** is a unique validation technique for bagging methods:\n",
    "\n",
    "1. Each tree in the forest is trained on a bootstrap sample (~63% of data)\n",
    "2. The remaining ~37% (out-of-bag samples) can be used for validation\n",
    "3. For each sample, average predictions from trees that didn't see it during training\n",
    "\n",
    "**Benefits of OOB:**\n",
    "- \"Free\" validation without holdout set\n",
    "- Uses all available training data\n",
    "- Generally good estimate of test error\n",
    "\n",
    "```\n",
    "For sample i:\n",
    "  - Trees trained with sample i: Cannot evaluate\n",
    "  - Trees trained WITHOUT sample i: Can predict and average\n",
    "  - OOB prediction = average of predictions from trees that didn't see i\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOB concept\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trees = 5\n",
    "n_samples = 10\n",
    "\n",
    "# Simulate which samples are in each tree's bootstrap\n",
    "in_bootstrap = np.random.binomial(1, 0.63, (n_trees, n_samples))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Heatmap: 1 = in bootstrap, 0 = out-of-bag\n",
    "fig.add_trace(go.Heatmap(\n",
    "    z=in_bootstrap,\n",
    "    x=[f'Sample {i+1}' for i in range(n_samples)],\n",
    "    y=[f'Tree {i+1}' for i in range(n_trees)],\n",
    "    colorscale=[[0, 'lightcoral'], [1, 'lightgreen']],\n",
    "    showscale=False\n",
    "))\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_trees):\n",
    "    for j in range(n_samples):\n",
    "        text = 'Train' if in_bootstrap[i, j] else 'OOB'\n",
    "        fig.add_annotation(\n",
    "            x=j, y=i, text=text,\n",
    "            showarrow=False, font=dict(size=10)\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Out-of-Bag (OOB) Samples: Green=Training, Red=Out-of-Bag',\n",
    "    xaxis_title='Samples',\n",
    "    yaxis_title='Trees',\n",
    "    height=350\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Show OOB ratio per sample\n",
    "oob_ratio = 1 - in_bootstrap.mean(axis=0)\n",
    "print(f\"\\nProportion of trees where each sample is OOB:\")\n",
    "for i in range(n_samples):\n",
    "    print(f\"  Sample {i+1}: {oob_ratio[i]*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OOB Scores for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OOB scores for all models\n",
    "oob_scores = {}\n",
    "\n",
    "print(\"Out-of-Bag Scores:\")\n",
    "print(\"=\"*50)\n",
    "for name, model in models.items():\n",
    "    rf = model.named_steps['classifier']\n",
    "    if hasattr(rf, 'oob_score_'):\n",
    "        oob_scores[name] = rf.oob_score_\n",
    "        print(f\"{name}: {rf.oob_score_:.4f}\")\n",
    "    else:\n",
    "        print(f\"{name}: OOB score not available (oob_score=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOB scores\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(oob_scores.keys()),\n",
    "    y=list(oob_scores.values()),\n",
    "    marker_color='darkgreen',\n",
    "    text=[f'{v:.4f}' for v in oob_scores.values()],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Out-of-Bag (OOB) Accuracy Scores',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='OOB Accuracy',\n",
    "    yaxis=dict(range=[0.5, 1.0]),\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stratified K-Fold Cross-Validation\n",
    "\n",
    "While OOB provides a free validation metric, we'll also use cross-validation for comparison with our logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate all models with multiple metrics\n",
    "cv_results = {}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "print(\"Running cross-validation for all models...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nCross-validating {name}...\")\n",
    "    \n",
    "    results = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    cv_results[name] = results\n",
    "    \n",
    "    print(f\"  Accuracy: {results['test_accuracy'].mean():.4f} (+/- {results['test_accuracy'].std()*2:.4f})\")\n",
    "    print(f\"  ROC-AUC: {results['test_roc_auc'].mean():.4f} (+/- {results['test_roc_auc'].std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "metrics_summary = []\n",
    "\n",
    "for name in models.keys():\n",
    "    results = cv_results[name]\n",
    "    metrics_summary.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{results['test_accuracy'].mean():.4f} (+/- {results['test_accuracy'].std()*2:.4f})\",\n",
    "        'Precision': f\"{results['test_precision'].mean():.4f} (+/- {results['test_precision'].std()*2:.4f})\",\n",
    "        'Recall': f\"{results['test_recall'].mean():.4f} (+/- {results['test_recall'].std()*2:.4f})\",\n",
    "        'F1-Score': f\"{results['test_f1'].mean():.4f} (+/- {results['test_f1'].std()*2:.4f})\",\n",
    "        'ROC-AUC': f\"{results['test_roc_auc'].mean():.4f} (+/- {results['test_roc_auc'].std()*2:.4f})\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "metric_names = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "model_names = list(models.keys())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, metric in enumerate(metric_names):\n",
    "    means = [cv_results[name][f'test_{metric}'].mean() for name in model_names]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric.upper(),\n",
    "        x=model_names,\n",
    "        y=means,\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cross-Validation Metrics Comparison',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train vs test scores to check for overfitting\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Accuracy: Train vs Test',\n",
    "    'ROC-AUC: Train vs Test'\n",
    "))\n",
    "\n",
    "for i, metric in enumerate(['accuracy', 'roc_auc']):\n",
    "    train_scores = [cv_results[name][f'train_{metric}'].mean() for name in model_names]\n",
    "    test_scores = [cv_results[name][f'test_{metric}'].mean() for name in model_names]\n",
    "    \n",
    "    fig.add_trace(go.Bar(name='Train', x=model_names, y=train_scores, \n",
    "                         marker_color='lightblue', showlegend=(i==0)), row=1, col=i+1)\n",
    "    fig.add_trace(go.Bar(name='Test', x=model_names, y=test_scores, \n",
    "                         marker_color='darkblue', showlegend=(i==0)), row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Checking for Overfitting: Train vs Test Scores',\n",
    "    height=400,\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: If train scores are much higher than test scores, the model may be overfitting. The constrained model (with limited depth) typically shows smaller gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Understanding Feature Importance\n",
    "\n",
    "Random Forests provide **feature importance** scores based on how much each feature contributes to reducing impurity (Gini or entropy) across all trees.\n",
    "\n",
    "**Mean Decrease in Impurity (MDI)**:\n",
    "- For each feature, sum the impurity decrease at all splits using that feature\n",
    "- Average across all trees\n",
    "- Normalize so all importances sum to 1\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher importance = feature is more useful for predictions\n",
    "- Does NOT indicate direction of effect (unlike logistic regression coefficients)\n",
    "- Can be biased toward high-cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from the preprocessor\n",
    "preprocessor = models['RF Baseline'].named_steps['preprocessing']\n",
    "\n",
    "# Get feature names from each transformer\n",
    "minmax_features = preprocessor.transformers_[0][2]  # minmax columns\n",
    "standard_features = preprocessor.transformers_[1][2]  # standard columns\n",
    "onehot_features = list(preprocessor.transformers_[2][1].get_feature_names_out(\n",
    "    preprocessor.transformers_[2][2]\n",
    "))\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = list(minmax_features) + list(standard_features) + onehot_features\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")\n",
    "print(f\"Feature names: {all_feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from baseline model\n",
    "rf_baseline = models['RF Baseline'].named_steps['classifier']\n",
    "importances = rf_baseline.feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances (RF Baseline):\")\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig = go.Figure()\n",
    "\n",
    "# Sort for visualization\n",
    "sorted_df = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=sorted_df['Feature'],\n",
    "    x=sorted_df['Importance'],\n",
    "    orientation='h',\n",
    "    marker_color='darkgreen'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest Feature Importances (Baseline Model)',\n",
    "    xaxis_title='Importance (Mean Decrease in Impurity)',\n",
    "    yaxis_title='Feature',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances across models\n",
    "importance_comparison = pd.DataFrame({'Feature': all_feature_names})\n",
    "\n",
    "for name, model in models.items():\n",
    "    rf = model.named_steps['classifier']\n",
    "    importance_comparison[name] = rf.feature_importances_\n",
    "\n",
    "# Display top 10 features for each model\n",
    "print(\"Top 5 Features by Model:\")\n",
    "print(\"=\"*60)\n",
    "for name in models.keys():\n",
    "    print(f\"\\n{name}:\")\n",
    "    top5 = importance_comparison.nlargest(5, name)[['Feature', name]]\n",
    "    for _, row in top5.iterrows():\n",
    "        print(f\"  {row['Feature']}: {row[name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize importance comparison for top features\n",
    "top_features = importance_df.head(10)['Feature'].tolist()\n",
    "comparison_subset = importance_comparison[importance_comparison['Feature'].isin(top_features)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['darkblue', 'blue', 'lightblue', 'steelblue']\n",
    "for i, name in enumerate(models.keys()):\n",
    "    subset = comparison_subset.sort_values('Feature')\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=name,\n",
    "        x=subset['Feature'],\n",
    "        y=subset[name],\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance Comparison Across Models (Top 10 Features)',\n",
    "    xaxis_title='Feature',\n",
    "    yaxis_title='Importance',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    xaxis_tickangle=45\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparing to Logistic Regression\n",
    "\n",
    "Random Forest importance tells us which features are useful, but not the *direction* of their effect. For that interpretation, logistic regression coefficients are more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpretation summary\n",
    "print(\"Feature Importance Interpretation:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTop 5 Most Important Features for Predicting Student Departure:\")\n",
    "print()\n",
    "\n",
    "for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
    "    feature = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    \n",
    "    # Add interpretation\n",
    "    if 'GPA' in feature:\n",
    "        interpretation = \"Academic performance indicator\"\n",
    "    elif 'DFW' in feature:\n",
    "        interpretation = \"Course failure/withdrawal rate\"\n",
    "    elif 'UNITS' in feature:\n",
    "        interpretation = \"Course load measure\"\n",
    "    else:\n",
    "        interpretation = \"Demographic characteristic\"\n",
    "    \n",
    "    print(f\"{i}. {feature}\")\n",
    "    print(f\"   Importance: {importance:.4f} ({importance*100:.1f}% of total)\")\n",
    "    print(f\"   Interpretation: {interpretation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate and Evaluate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all models on test data\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    probabilities[name] = model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "    \n",
    "print(\"Predictions generated for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics for all models\n",
    "test_metrics = []\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = predictions[name]\n",
    "    y_prob = probabilities[name]\n",
    "    \n",
    "    test_metrics.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob)\n",
    "    })\n",
    "\n",
    "test_metrics_df = pd.DataFrame(test_metrics)\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for all models\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=list(models.keys()))\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # Add heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm,\n",
    "            x=['Predicted Retained', 'Predicted Departed'],\n",
    "            y=['Actual Retained', 'Actual Departed'],\n",
    "            colorscale='Blues',\n",
    "            showscale=False,\n",
    "            text=cm,\n",
    "            texttemplate='%{text}',\n",
    "            textfont=dict(size=14)\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrices for All Models',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for baseline model\n",
    "print(\"Classification Report - RF Baseline:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, predictions['RF Baseline'], \n",
    "                           target_names=['Retained', 'Departed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['darkblue', 'blue', 'lightblue', 'steelblue']\n",
    "\n",
    "for idx, (name, y_prob) in enumerate(probabilities.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{name} (AUC={auc:.4f})',\n",
    "        line=dict(color=colors[idx], width=2)\n",
    "    ))\n",
    "\n",
    "# Add diagonal reference line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random (AUC=0.5)',\n",
    "    line=dict(color='gray', width=1, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curves - Random Forest Models',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "final_comparison = []\n",
    "\n",
    "for name in models.keys():\n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'OOB Score': f\"{oob_scores.get(name, 'N/A'):.4f}\" if name in oob_scores else 'N/A',\n",
    "        'CV Accuracy': f\"{cv_results[name]['test_accuracy'].mean():.4f}\",\n",
    "        'CV ROC-AUC': f\"{cv_results[name]['test_roc_auc'].mean():.4f}\",\n",
    "        'Test Accuracy': f\"{test_metrics_df[test_metrics_df['Model']==name]['Accuracy'].values[0]:.4f}\",\n",
    "        'Test ROC-AUC': f\"{test_metrics_df[test_metrics_df['Model']==name]['ROC-AUC'].values[0]:.4f}\",\n",
    "        'Training Time (s)': f\"{training_times[name]:.2f}\"\n",
    "    })\n",
    "\n",
    "final_comparison_df = pd.DataFrame(final_comparison)\n",
    "print(\"Model Comparison Summary:\")\n",
    "final_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_to_plot = ['OOB Score', 'CV Accuracy', 'CV ROC-AUC', 'Test ROC-AUC']\n",
    "model_names = final_comparison_df['Model'].tolist()\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    if metric == 'OOB Score':\n",
    "        values = [oob_scores.get(name, 0) for name in model_names]\n",
    "    elif metric == 'CV Accuracy':\n",
    "        values = [cv_results[name]['test_accuracy'].mean() for name in model_names]\n",
    "    elif metric == 'CV ROC-AUC':\n",
    "        values = [cv_results[name]['test_roc_auc'].mean() for name in model_names]\n",
    "    else:\n",
    "        values = test_metrics_df['ROC-AUC'].tolist()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=model_names,\n",
    "        y=values\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest Model Performance Comparison',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0.5, 1.0]),\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model_name = test_metrics_df.loc[test_metrics_df['ROC-AUC'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best model based on test ROC-AUC: {best_model_name}\")\n",
    "\n",
    "# Save the trained best model\n",
    "best_model_path = f'{models_path}rf_best_trained_model.pkl'\n",
    "pickle.dump(best_model, open(best_model_path, 'wb'))\n",
    "print(f\"Saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we trained and evaluated Random Forest models for predicting student departure.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **OOB Scores**: Provided quick validation without a holdout set\n",
    "2. **Cross-Validation**: Confirmed model generalization across folds\n",
    "3. **Feature Importance**: Academic performance features (GPA, DFW rates) were most important\n",
    "4. **Test Performance**: Models achieved competitive accuracy and ROC-AUC scores\n",
    "\n",
    "### Evaluation Methods Summary\n",
    "\n",
    "| Method | Description | When to Use |\n",
    "|:-------|:------------|:------------|\n",
    "| OOB Score | Free validation from bootstrap | Quick initial assessment |\n",
    "| Cross-Validation | K-fold with stratification | Robust generalization estimate |\n",
    "| Test Set | Final holdout evaluation | Unbiased final performance |\n",
    "\n",
    "### Feature Importance Insights\n",
    "\n",
    "The most important features for predicting student departure were:\n",
    "1. GPA measures (first and second semester)\n",
    "2. DFW rates (course failure/withdrawal)\n",
    "3. Units attempted\n",
    "\n",
    "This aligns with domain knowledge: academic performance is the strongest predictor of student persistence.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will tune Random Forest hyperparameters to optimize model performance.\n",
    "\n",
    "**Proceed to:** `3.4 Tune Random Forest Hyperparameters`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}