{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Introduction to Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Modules 1 and 2, we explored logistic regression with regularization for predicting student departure. While logistic regression is powerful and interpretable, it makes certain assumptions about the data (e.g., linear relationships between features and log-odds). In this module, we introduce **Random Forests**—a fundamentally different approach that makes no such assumptions.\n",
    "\n",
    "Random Forests belong to a class of algorithms called **ensemble methods**, which combine multiple models to achieve better performance than any single model could. This notebook explains the theory behind ensemble learning and how Random Forests work.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the concept of ensemble learning and why it works\n",
    "2. Describe the bootstrap aggregating (bagging) technique\n",
    "3. Understand how decision trees serve as base learners\n",
    "4. Explain how Random Forests combine bagging with feature randomness\n",
    "5. Compare Random Forests to logistic regression for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Wisdom of Crowds: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Why Ensemble Methods Work\n",
    "\n",
    "The fundamental insight behind ensemble methods is that **a group of diverse learners can outperform any individual learner**. This is analogous to the \"wisdom of crowds\" phenomenon:\n",
    "\n",
    "- A single person guessing the number of jellybeans in a jar will likely be off\n",
    "- The average of 100 guesses is often remarkably close to the truth\n",
    "\n",
    "In machine learning terms:\n",
    "- A single decision tree might make systematic errors\n",
    "- Hundreds of trees, each trained slightly differently, can average out those errors\n",
    "\n",
    "**Key Insight**: For ensemble methods to work, the individual models must:\n",
    "1. Have some predictive power (be better than random guessing)\n",
    "2. Make different errors (be diverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Simulate the wisdom of crowds\n",
    "np.random.seed(42)\n",
    "true_value = 500  # True number of jellybeans\n",
    "\n",
    "# Individual guesses with some accuracy\n",
    "n_guessers = 100\n",
    "individual_guesses = np.random.normal(true_value, 100, n_guessers)  # Some variance\n",
    "\n",
    "# Calculate running average as we add more guessers\n",
    "ensemble_averages = np.cumsum(individual_guesses) / np.arange(1, n_guessers + 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Individual guesses\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, n_guessers + 1),\n",
    "    y=individual_guesses,\n",
    "    mode='markers',\n",
    "    name='Individual Guesses',\n",
    "    marker=dict(color='lightblue', size=6, opacity=0.6)\n",
    "))\n",
    "\n",
    "# Running average\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, n_guessers + 1),\n",
    "    y=ensemble_averages,\n",
    "    mode='lines',\n",
    "    name='Ensemble Average',\n",
    "    line=dict(color='darkblue', width=3)\n",
    "))\n",
    "\n",
    "# True value\n",
    "fig.add_hline(y=true_value, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"True Value (500)\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='The Wisdom of Crowds: Ensemble Averaging Reduces Error',\n",
    "    xaxis_title='Number of Guessers (Models)',\n",
    "    yaxis_title='Prediction',\n",
    "    height=450,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: As we add more guessers (models), the ensemble average converges toward the true value, even though individual guesses vary widely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Types of Ensemble Methods\n",
    "\n",
    "There are two main approaches to building ensembles:\n",
    "\n",
    "| Method | Description | Examples |\n",
    "|:-------|:------------|:---------|\n",
    "| **Bagging** | Train models in parallel on bootstrapped samples | Random Forest, Bagged Trees |\n",
    "| **Boosting** | Train models sequentially, each fixing previous errors | AdaBoost, Gradient Boosting, XGBoost |\n",
    "\n",
    "In this module, we focus on **Bagging** and **Random Forests**. We'll cover boosting methods in a later module.\n",
    "\n",
    "**Bagging** (Bootstrap Aggregating):\n",
    "- Train many models independently\n",
    "- Each model sees a different random sample of the data\n",
    "- Combine predictions by voting (classification) or averaging (regression)\n",
    "\n",
    "**Boosting**:\n",
    "- Train models sequentially\n",
    "- Each new model focuses on samples the previous models got wrong\n",
    "- Combine with weighted voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap Aggregating (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is Bootstrapping?\n",
    "\n",
    "**Bootstrapping** is a statistical technique where we create new datasets by sampling **with replacement** from the original data.\n",
    "\n",
    "- Original dataset: [A, B, C, D, E]\n",
    "- Bootstrap sample 1: [B, B, D, A, E] (B appears twice, C is missing)\n",
    "- Bootstrap sample 2: [A, C, C, C, D] (C appears three times)\n",
    "\n",
    "Key properties:\n",
    "- Each bootstrap sample has the same size as the original\n",
    "- On average, about **63.2%** of original samples appear in each bootstrap (some appear multiple times)\n",
    "- About **36.8%** of samples are left out—these are called **Out-of-Bag (OOB)** samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bootstrapping\n",
    "np.random.seed(42)\n",
    "original_data = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n",
    "n_samples = len(original_data)\n",
    "\n",
    "print(\"Original Dataset:\", list(original_data))\n",
    "print(\"\\nBootstrap Samples (with replacement):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(5):\n",
    "    # Sample with replacement\n",
    "    bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    bootstrap_sample = original_data[bootstrap_indices]\n",
    "    \n",
    "    # Find OOB samples\n",
    "    oob_indices = set(range(n_samples)) - set(bootstrap_indices)\n",
    "    oob_samples = original_data[list(oob_indices)]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: {list(bootstrap_sample)}\")\n",
    "    print(f\"  Out-of-Bag: {list(oob_samples)} ({len(oob_samples)/n_samples*100:.0f}% left out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How Bagging Works\n",
    "\n",
    "The bagging algorithm:\n",
    "\n",
    "1. Create B bootstrap samples from the training data\n",
    "2. Train one model on each bootstrap sample (in parallel)\n",
    "3. Combine predictions:\n",
    "   - **Classification**: Majority voting\n",
    "   - **Regression**: Average predictions\n",
    "\n",
    "```\n",
    "Training Data\n",
    "     |\n",
    "     |---> Bootstrap Sample 1 ---> Tree 1 ---\\\n",
    "     |---> Bootstrap Sample 2 ---> Tree 2 ----\\---> Aggregate ---> Final Prediction\n",
    "     |---> Bootstrap Sample 3 ---> Tree 3 ----/\n",
    "     |---> ...                 ---> ...   ---/\n",
    "     |---> Bootstrap Sample B ---> Tree B --/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the bagging process\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Training data box\n",
    "fig.add_shape(type=\"rect\", x0=0, y0=4, x1=2, y1=5,\n",
    "              fillcolor=\"lightblue\", line=dict(color=\"darkblue\", width=2))\n",
    "fig.add_annotation(x=1, y=4.5, text=\"Training Data<br>(n samples)\", showarrow=False, font=dict(size=12))\n",
    "\n",
    "# Bootstrap samples\n",
    "for i, y_pos in enumerate([3.5, 2.5, 1.5, 0.5]):\n",
    "    # Bootstrap sample box\n",
    "    fig.add_shape(type=\"rect\", x0=3, y0=y_pos-0.3, x1=5, y1=y_pos+0.3,\n",
    "                  fillcolor=\"lightyellow\", line=dict(color=\"orange\", width=2))\n",
    "    label = f\"Bootstrap {i+1}\" if i < 3 else \"...\"\n",
    "    fig.add_annotation(x=4, y=y_pos, text=label, showarrow=False)\n",
    "    \n",
    "    # Tree box\n",
    "    fig.add_shape(type=\"rect\", x0=6, y0=y_pos-0.3, x1=8, y1=y_pos+0.3,\n",
    "                  fillcolor=\"lightgreen\", line=dict(color=\"green\", width=2))\n",
    "    tree_label = f\"Tree {i+1}\" if i < 3 else \"...\"\n",
    "    fig.add_annotation(x=7, y=y_pos, text=tree_label, showarrow=False)\n",
    "    \n",
    "    # Arrows\n",
    "    fig.add_annotation(x=3, y=y_pos, ax=2, ay=4.5, axref=\"x\", ayref=\"y\",\n",
    "                      xref=\"x\", yref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\")\n",
    "    fig.add_annotation(x=6, y=y_pos, ax=5, ay=y_pos, axref=\"x\", ayref=\"y\",\n",
    "                      xref=\"x\", yref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\")\n",
    "\n",
    "# Aggregation box\n",
    "fig.add_shape(type=\"rect\", x0=9, y0=1.7, x1=11, y1=2.3,\n",
    "              fillcolor=\"lightcoral\", line=dict(color=\"red\", width=2))\n",
    "fig.add_annotation(x=10, y=2, text=\"Aggregate<br>(Vote/Average)\", showarrow=False)\n",
    "\n",
    "# Final prediction\n",
    "fig.add_shape(type=\"rect\", x0=12, y0=1.7, x1=14, y1=2.3,\n",
    "              fillcolor=\"lavender\", line=dict(color=\"purple\", width=2))\n",
    "fig.add_annotation(x=13, y=2, text=\"Final<br>Prediction\", showarrow=False)\n",
    "\n",
    "# Arrows to aggregation and final\n",
    "for y_pos in [3.5, 2.5, 1.5, 0.5]:\n",
    "    fig.add_annotation(x=9, y=2, ax=8, ay=y_pos, axref=\"x\", ayref=\"y\",\n",
    "                      xref=\"x\", yref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\")\n",
    "fig.add_annotation(x=12, y=2, ax=11, ay=2, axref=\"x\", ayref=\"y\",\n",
    "                  xref=\"x\", yref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='The Bagging Algorithm: Bootstrap Aggregating',\n",
    "    xaxis=dict(visible=False, range=[-0.5, 14.5]),\n",
    "    yaxis=dict(visible=False, range=[-0.5, 5.5]),\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Variance Reduction Through Bagging\n",
    "\n",
    "Bagging primarily reduces **variance** in predictions. If we have B independent models, each with variance $\\sigma^2$, the variance of their average is:\n",
    "\n",
    "$$\\text{Var}(\\bar{f}) = \\frac{\\sigma^2}{B}$$\n",
    "\n",
    "In practice, the bootstrap samples aren't truly independent (they all come from the same data), but significant variance reduction still occurs.\n",
    "\n",
    "**Why does this matter for student departure prediction?**\n",
    "- A single decision tree might be very sensitive to which students are in the training set\n",
    "- Bagging stabilizes predictions by averaging across many trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance reduction with increasing ensemble size\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate predictions from different numbers of trees\n",
    "n_experiments = 100\n",
    "tree_counts = [1, 5, 10, 25, 50, 100, 200]\n",
    "true_prob = 0.3  # True probability for a student\n",
    "\n",
    "variance_by_count = []\n",
    "\n",
    "for n_trees in tree_counts:\n",
    "    ensemble_predictions = []\n",
    "    for _ in range(n_experiments):\n",
    "        # Simulate each tree's prediction (noisy around true probability)\n",
    "        tree_preds = np.random.normal(true_prob, 0.15, n_trees)\n",
    "        tree_preds = np.clip(tree_preds, 0, 1)\n",
    "        # Ensemble prediction is the average\n",
    "        ensemble_predictions.append(np.mean(tree_preds))\n",
    "    variance_by_count.append(np.var(ensemble_predictions))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=tree_counts,\n",
    "    y=variance_by_count,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='darkblue', width=3),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Variance Reduction as Ensemble Size Increases',\n",
    "    xaxis_title='Number of Trees in Ensemble',\n",
    "    yaxis_title='Variance of Predictions',\n",
    "    xaxis_type='log',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Variance decreases rapidly at first, then levels off. This is why 100-500 trees is often sufficient for Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Trees: The Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 How Decision Trees Work\n",
    "\n",
    "A **decision tree** makes predictions by asking a series of yes/no questions about the features.\n",
    "\n",
    "For student departure prediction, a tree might ask:\n",
    "- Is GPA_1 < 2.5?\n",
    "  - Yes: Is DFW_RATE_1 > 0.3?\n",
    "    - Yes: Predict DEPARTED\n",
    "    - No: Is FIRST_GEN = Yes?\n",
    "      - Yes: Predict DEPARTED\n",
    "      - No: Predict RETAINED\n",
    "  - No: Predict RETAINED\n",
    "\n",
    "**How trees learn**: They recursively split the data to maximize \"purity\"—separating students who departed from those who were retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a simple decision tree structure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Tree structure visualization\n",
    "nodes = [\n",
    "    # (x, y, text, color)\n",
    "    (5, 5, \"GPA_1 < 2.5?\", \"lightblue\"),\n",
    "    (2.5, 3.5, \"DFW_RATE > 0.3?\", \"lightblue\"),\n",
    "    (7.5, 3.5, \"RETAINED\", \"lightgreen\"),\n",
    "    (1, 2, \"DEPARTED\", \"lightcoral\"),\n",
    "    (4, 2, \"FIRST_GEN?\", \"lightblue\"),\n",
    "    (3, 0.5, \"DEPARTED\", \"lightcoral\"),\n",
    "    (5, 0.5, \"RETAINED\", \"lightgreen\")\n",
    "]\n",
    "\n",
    "# Add nodes\n",
    "for x, y, text, color in nodes:\n",
    "    fig.add_shape(type=\"rect\", x0=x-0.8, y0=y-0.4, x1=x+0.8, y1=y+0.4,\n",
    "                  fillcolor=color, line=dict(color=\"darkgray\", width=1))\n",
    "    fig.add_annotation(x=x, y=y, text=text, showarrow=False, font=dict(size=10))\n",
    "\n",
    "# Add edges\n",
    "edges = [\n",
    "    ((5, 4.6), (2.5, 3.9), \"Yes\"),\n",
    "    ((5, 4.6), (7.5, 3.9), \"No\"),\n",
    "    ((2.5, 3.1), (1, 2.4), \"Yes\"),\n",
    "    ((2.5, 3.1), (4, 2.4), \"No\"),\n",
    "    ((4, 1.6), (3, 0.9), \"Yes\"),\n",
    "    ((4, 1.6), (5, 0.9), \"No\")\n",
    "]\n",
    "\n",
    "for (x1, y1), (x2, y2), label in edges:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x1, x2], y=[y1, y2],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=2),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    mid_x, mid_y = (x1+x2)/2, (y1+y2)/2\n",
    "    fig.add_annotation(x=mid_x, y=mid_y, text=label, showarrow=False,\n",
    "                      font=dict(size=9, color='darkblue'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decision Tree for Student Departure Prediction',\n",
    "    xaxis=dict(visible=False, range=[0, 10]),\n",
    "    yaxis=dict(visible=False, range=[0, 6]),\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Strengths and Weaknesses of Decision Trees\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "|:----------|:-----------|\n",
    "| No assumptions about data distribution | High variance (small data changes = different tree) |\n",
    "| Handle non-linear relationships | Prone to overfitting |\n",
    "| Handle interactions naturally | Unstable (greedy splitting) |\n",
    "| Easy to interpret | Can be biased toward high-cardinality features |\n",
    "| No scaling required | |\n",
    "\n",
    "**The key weakness**: A single decision tree has **high variance**. It can drastically change based on small changes to the training data.\n",
    "\n",
    "**The solution**: Combine many trees via bagging to reduce variance while keeping the strengths!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forests: Bagging + Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The Random Forest Algorithm\n",
    "\n",
    "A **Random Forest** is a bagged ensemble of decision trees with an additional twist: **feature randomness** at each split.\n",
    "\n",
    "**Random Forest Algorithm**:\n",
    "\n",
    "1. For b = 1 to B (number of trees):\n",
    "   - Draw a bootstrap sample from the training data\n",
    "   - Grow a decision tree on this sample, but:\n",
    "     - At each split, only consider a **random subset of features**\n",
    "     - Select the best split among this subset\n",
    "   - Grow the tree fully (no pruning)\n",
    "\n",
    "2. Aggregate predictions:\n",
    "   - **Classification**: Majority vote across all trees\n",
    "   - **Regression**: Average predictions across all trees\n",
    "\n",
    "The name \"Random Forest\" comes from the randomness in:\n",
    "1. **Rows**: Bootstrap sampling of observations\n",
    "2. **Columns**: Random feature subset at each split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Randomness: The Secret Ingredient\n",
    "\n",
    "**Why add feature randomness?**\n",
    "\n",
    "Without feature randomness, if one feature is very strong (e.g., GPA), all trees would split on it first. The trees would be highly correlated, limiting variance reduction.\n",
    "\n",
    "By only considering a random subset of features at each split:\n",
    "- Different trees use different features at the top\n",
    "- Trees become less correlated\n",
    "- Variance reduction is more effective\n",
    "\n",
    "**Typical values for `max_features` (features considered at each split)**:\n",
    "- Classification: $\\sqrt{p}$ (square root of total features)\n",
    "- Regression: $p/3$ (one-third of total features)\n",
    "\n",
    "Where $p$ is the total number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of feature randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate tree correlation with and without feature randomness\n",
    "n_sims = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Without feature randomness: all trees use same top feature\n",
    "# Results are highly correlated\n",
    "predictions_no_random = np.random.normal(0.5, 0.1, (n_sims, 100))\n",
    "for i in range(100):\n",
    "    predictions_no_random[:, i] += np.random.normal(0, 0.05)  # Small perturbation\n",
    "\n",
    "# With feature randomness: trees use different features\n",
    "# Results are less correlated\n",
    "predictions_with_random = np.random.normal(0.5, 0.1, (n_sims, 100))\n",
    "for i in range(100):\n",
    "    predictions_with_random[:, i] += np.random.normal(0, 0.15)  # Larger perturbation\n",
    "\n",
    "# Calculate correlation matrices\n",
    "corr_no_random = np.corrcoef(predictions_no_random[:, :20].T)\n",
    "corr_with_random = np.corrcoef(predictions_with_random[:, :20].T)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Without Feature Randomness<br>(High Tree Correlation)',\n",
    "    'With Feature Randomness<br>(Low Tree Correlation)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=corr_no_random, colorscale='RdBu', zmid=0,\n",
    "                         showscale=False), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=corr_with_random, colorscale='RdBu', zmid=0,\n",
    "                         showscale=True, colorbar=dict(title='Correlation')), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Randomness Reduces Tree Correlation',\n",
    "    height=400\n",
    ")\n",
    "fig.update_xaxes(title='Tree Index')\n",
    "fig.update_yaxes(title='Tree Index')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Left: Without feature randomness, trees are highly correlated (red = high correlation)\n",
    "- Right: With feature randomness, trees are more independent (blue = lower correlation)\n",
    "\n",
    "Lower correlation means better variance reduction when averaging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Making Predictions\n",
    "\n",
    "For **classification** (our student departure problem):\n",
    "\n",
    "- Each tree votes for a class (RETAINED or DEPARTED)\n",
    "- The final prediction is the class with the most votes\n",
    "- **Probability estimates**: Proportion of trees voting for each class\n",
    "\n",
    "```\n",
    "New Student: GPA=2.3, DFW_RATE=0.2, ...\n",
    "\n",
    "Tree 1: DEPARTED (0.7 prob)\n",
    "Tree 2: RETAINED (0.4 prob)\n",
    "Tree 3: DEPARTED (0.8 prob)\n",
    "Tree 4: DEPARTED (0.6 prob)\n",
    "Tree 5: RETAINED (0.3 prob)\n",
    "...\n",
    "Tree 100: DEPARTED (0.55 prob)\n",
    "\n",
    "Votes: 65 DEPARTED, 35 RETAINED\n",
    "Final Prediction: DEPARTED\n",
    "Probability: 0.65 (65% of trees voted DEPARTED)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate voting across trees\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trees = 100\n",
    "# Simulate tree votes (0=Retained, 1=Departed)\n",
    "# True probability is 0.65 for this student\n",
    "tree_votes = np.random.binomial(1, 0.65, n_trees)\n",
    "\n",
    "# Running vote proportion\n",
    "vote_proportion = np.cumsum(tree_votes) / np.arange(1, n_trees + 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, n_trees + 1),\n",
    "    y=vote_proportion,\n",
    "    mode='lines',\n",
    "    line=dict(color='darkblue', width=2),\n",
    "    name='Vote Proportion'\n",
    "))\n",
    "\n",
    "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"gray\",\n",
    "              annotation_text=\"Decision Boundary (50%)\")\n",
    "fig.add_hline(y=0.65, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=\"True Probability (65%)\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest Voting: Probability Converges as Trees Increase',\n",
    "    xaxis_title='Number of Trees',\n",
    "    yaxis_title='Proportion Voting \"Departed\"',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Final vote: {sum(tree_votes)} Departed, {n_trees - sum(tree_votes)} Retained\")\n",
    "print(f\"Predicted probability of departure: {sum(tree_votes)/n_trees:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forests vs. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do Random Forests compare to the logistic regression models we built in Modules 1-2?\n",
    "\n",
    "| Aspect | Logistic Regression | Random Forest |\n",
    "|:-------|:--------------------|:--------------|\n",
    "| **Model Type** | Parametric (linear in log-odds) | Non-parametric (tree-based) |\n",
    "| **Assumptions** | Linear decision boundary | No distributional assumptions |\n",
    "| **Feature Interactions** | Must be explicitly added | Captured automatically |\n",
    "| **Scaling Required** | Yes (affects regularization) | No |\n",
    "| **Interpretability** | High (coefficients = log-odds) | Moderate (feature importance) |\n",
    "| **Handles Non-linearity** | Limited | Excellent |\n",
    "| **Risk of Overfitting** | Lower (especially with regularization) | Higher (but bagging helps) |\n",
    "| **Training Speed** | Fast | Slower (many trees) |\n",
    "| **Prediction Speed** | Very fast | Fast |\n",
    "| **Works with Small Data** | Yes | Less reliable |\n",
    "\n",
    "**When to use each**:\n",
    "\n",
    "- **Logistic Regression**: When interpretability is crucial, relationships are approximately linear, or you have limited data\n",
    "- **Random Forest**: When capturing complex patterns is more important than interpretability, or when you suspect non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize linear vs non-linear decision boundaries\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data with non-linear boundary\n",
    "n_points = 200\n",
    "X1 = np.random.uniform(0, 4, n_points)\n",
    "X2 = np.random.uniform(0, 4, n_points)\n",
    "# Non-linear boundary: circle\n",
    "y = ((X1 - 2)**2 + (X2 - 2)**2 < 1.5).astype(int)\n",
    "y = y + np.random.binomial(1, 0.1, n_points) - np.random.binomial(1, 0.1, n_points)\n",
    "y = np.clip(y, 0, 1)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Logistic Regression<br>(Linear Boundary)',\n",
    "    'Random Forest<br>(Non-linear Boundary)'\n",
    "))\n",
    "\n",
    "# Scatter plot\n",
    "for col in [1, 2]:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X1[y==0], y=X2[y==0],\n",
    "        mode='markers', marker=dict(color='blue', size=8),\n",
    "        name='Retained', showlegend=(col==1)\n",
    "    ), row=1, col=col)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X1[y==1], y=X2[y==1],\n",
    "        mode='markers', marker=dict(color='red', size=8),\n",
    "        name='Departed', showlegend=(col==1)\n",
    "    ), row=1, col=col)\n",
    "\n",
    "# Linear boundary (Logistic)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 4], y=[1, 3],\n",
    "    mode='lines', line=dict(color='green', width=3, dash='dash'),\n",
    "    name='Decision Boundary', showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# Non-linear boundary (Random Forest) - circle\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=2 + 1.22*np.cos(theta), y=2 + 1.22*np.sin(theta),\n",
    "    mode='lines', line=dict(color='green', width=3, dash='dash'),\n",
    "    name='Decision Boundary', showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decision Boundary Comparison: Linear vs. Non-linear',\n",
    "    height=400\n",
    ")\n",
    "fig.update_xaxes(title='GPA (scaled)', range=[0, 4])\n",
    "fig.update_yaxes(title='DFW Rate (scaled)', range=[0, 4])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Left: Logistic regression can only draw a straight line (or hyperplane in higher dimensions)\n",
    "- Right: Random forests can capture complex, non-linear patterns\n",
    "\n",
    "In student departure prediction, the relationship between features and departure probability may not be strictly linear. For example, the effect of GPA on departure risk might be different for students with very high vs. very low DFW rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Ensemble Learning**: Combining multiple models to achieve better performance\n",
    "   - Works through the \"wisdom of crowds\" effect\n",
    "   - Requires diverse models that make different errors\n",
    "\n",
    "2. **Bagging (Bootstrap Aggregating)**:\n",
    "   - Create bootstrap samples (sample with replacement)\n",
    "   - Train one model per bootstrap sample\n",
    "   - Aggregate via voting/averaging\n",
    "   - Reduces variance while keeping bias similar\n",
    "\n",
    "3. **Decision Trees**:\n",
    "   - Make predictions via a series of if-then rules\n",
    "   - Powerful but high variance (unstable)\n",
    "   - Perfect building blocks for ensembles\n",
    "\n",
    "4. **Random Forests**:\n",
    "   - Bagged ensemble of decision trees\n",
    "   - Added feature randomness at each split\n",
    "   - Feature randomness decorrelates trees for better variance reduction\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Concept | Key Point |\n",
    "|:--------|:----------|\n",
    "| Ensemble | Combine many weak learners into one strong learner |\n",
    "| Bagging | Parallel training on bootstrap samples |\n",
    "| Out-of-Bag | ~37% of data not in each bootstrap sample |\n",
    "| Feature Randomness | Consider only subset of features at each split |\n",
    "| max_features | Typically sqrt(p) for classification |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will build a Random Forest pipeline for our student departure prediction problem using scikit-learn.\n",
    "\n",
    "**Proceed to:** `3.2 Build a Random Forest Classification Model`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}