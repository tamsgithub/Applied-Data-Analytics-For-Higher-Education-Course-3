{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 **Build** a Random Forest Classification Model - Predict Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### **1. Build the Model : Create the Random Forest pipeline.**  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we learned about ensemble learning, bagging, and how Random Forests work. Now we put theory into practice by building a Random Forest classification pipeline for predicting student departure.\n",
    "\n",
    "One key advantage of Random Forests: **they don't require feature scaling**. Decision trees make splits based on thresholds, not distances, so scaling doesn't affect the model. However, we'll keep our preprocessing pipeline for handling categorical variables.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Create Random Forest models using scikit-learn's `RandomForestClassifier`\n",
    "2. Understand the key hyperparameters and their effects\n",
    "3. Build a complete pipeline combining preprocessing and classification\n",
    "4. Configure alternative Random Forest setups for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_training['SEM_3_STATUS'].value_counts(normalize=True))\n",
    "print(f\"\\nClass imbalance ratio: {df_training['SEM_3_STATUS'].value_counts()[0] / df_training['SEM_3_STATUS'].value_counts()[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available features\n",
    "print(\"Available columns:\")\n",
    "print(df_training.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forests in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The RandomForestClassifier Class\n",
    "\n",
    "Scikit-learn provides `RandomForestClassifier` for classification tasks. It implements the Random Forest algorithm we discussed in the previous notebook.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=None,        # Maximum tree depth (None = unlimited)\n",
    "    max_features='sqrt',   # Features to consider at each split\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Key Hyperparameters\n",
    "\n",
    "Random Forests have several important hyperparameters that control the ensemble and individual trees:\n",
    "\n",
    "#### Ensemble-Level Parameters\n",
    "\n",
    "| Parameter | Description | Default | Effect |\n",
    "|:----------|:------------|:--------|:-------|\n",
    "| `n_estimators` | Number of trees in the forest | 100 | More trees = more stable but slower |\n",
    "| `bootstrap` | Whether to use bootstrap sampling | True | False = each tree sees all data |\n",
    "| `oob_score` | Calculate out-of-bag score | False | Free validation metric |\n",
    "\n",
    "#### Tree-Level Parameters\n",
    "\n",
    "| Parameter | Description | Default | Effect |\n",
    "|:----------|:------------|:--------|:-------|\n",
    "| `max_depth` | Maximum depth of each tree | None (unlimited) | Limits tree complexity |\n",
    "| `max_features` | Features considered at each split | 'sqrt' | Controls randomness |\n",
    "| `min_samples_split` | Min samples to split a node | 2 | Prevents overly specific splits |\n",
    "| `min_samples_leaf` | Min samples in a leaf node | 1 | Controls leaf size |\n",
    "\n",
    "#### Class Imbalance Parameters\n",
    "\n",
    "| Parameter | Description | Default | Effect |\n",
    "|:----------|:------------|:--------|:-------|\n",
    "| `class_weight` | Weight for each class | None | Handle imbalanced data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of n_estimators\n",
    "n_estimators_values = [1, 10, 50, 100, 200, 500]\n",
    "stability = [0.3, 0.55, 0.75, 0.85, 0.90, 0.92]  # Simulated stability scores\n",
    "training_time = [0.1, 0.5, 2, 4, 8, 20]  # Simulated relative training time\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Model Stability vs. n_estimators',\n",
    "    'Training Time vs. n_estimators'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_values, y=stability,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='darkblue', width=3),\n",
    "    marker=dict(size=10)\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_values, y=training_time,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='darkred', width=3),\n",
    "    marker=dict(size=10)\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title='Number of Trees (n_estimators)')\n",
    "fig.update_yaxes(title='Stability Score', row=1, col=1)\n",
    "fig.update_yaxes(title='Relative Training Time', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Trade-off: More Trees = More Stable but Slower',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Stability increases rapidly with more trees, then levels off. Training time increases roughly linearly. 100-500 trees is usually a good balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of max_features\n",
    "max_features_options = ['sqrt', 'log2', 0.5, 1.0]\n",
    "descriptions = ['sqrt(p)', 'log2(p)', '50% of features', 'All features']\n",
    "diversity = [0.9, 0.85, 0.7, 0.3]  # Higher = more diverse trees\n",
    "individual_accuracy = [0.75, 0.78, 0.82, 0.88]  # Individual tree accuracy\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=descriptions,\n",
    "    y=diversity,\n",
    "    name='Tree Diversity',\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=descriptions,\n",
    "    y=individual_accuracy,\n",
    "    name='Individual Tree Accuracy',\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='max_features Trade-off: Diversity vs. Individual Accuracy',\n",
    "    xaxis_title='max_features Setting',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Lower `max_features` creates more diverse (less correlated) trees but each tree is less accurate\n",
    "- Higher `max_features` creates more accurate individual trees but they're more correlated\n",
    "- The optimal is usually somewhere in between (sqrt or log2 for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Groupings\n",
    "\n",
    "Although Random Forests don't require scaling, we still need to:\n",
    "1. Handle categorical variables (one-hot encoding)\n",
    "2. Define which features to use\n",
    "\n",
    "We'll use the same feature groupings from our logistic regression models for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "\n",
    "# Numeric features (no scaling needed for Random Forests, but included for consistency)\n",
    "numeric_columns = [\n",
    "    'HS_GPA',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "# Categorical columns for one-hot encoding\n",
    "categorical_columns = [\n",
    "    'GENDER',\n",
    "    'RACE_ETHNICITY',\n",
    "    'FIRST_GEN_STATUS'\n",
    "]\n",
    "\n",
    "# All features\n",
    "all_features = numeric_columns + categorical_columns\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print(f\"Numeric: {len(numeric_columns)}\")\n",
    "print(f\"Categorical: {len(categorical_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocessor Configuration\n",
    "\n",
    "For Random Forests, we have two options:\n",
    "\n",
    "**Option A**: Minimal preprocessing (just handle categoricals)\n",
    "```python\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('passthrough', 'passthrough', numeric_columns),\n",
    "    ('onehot', OneHotEncoder(...), categorical_columns)\n",
    "])\n",
    "```\n",
    "\n",
    "**Option B**: Use the same preprocessing as logistic regression (for fair comparison)\n",
    "```python\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('minmax', MinMaxScaler(), minmax_columns),\n",
    "    ('standard', StandardScaler(), standard_columns),\n",
    "    ('onehot', OneHotEncoder(...), categorical_columns)\n",
    "])\n",
    "```\n",
    "\n",
    "We'll use Option B for consistency, but note that **scaling doesn't affect Random Forest performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fair comparison with logistic regression, use same preprocessing\n",
    "# (But know that scaling is optional for Random Forests)\n",
    "\n",
    "minmax_columns = [\n",
    "    'HS_GPA',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2'\n",
    "]\n",
    "\n",
    "standard_columns = [\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'GENDER',\n",
    "    'RACE_ETHNICITY',\n",
    "    'FIRST_GEN_STATUS'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), minmax_columns),\n",
    "        ('standard', StandardScaler(), standard_columns),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                  drop=['Female', 'Other', 'Unknown'], \n",
    "                                  sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Random Forest Model\n",
    "\n",
    "Our baseline Random Forest will use sensible default parameters:\n",
    "- 100 trees (standard starting point)\n",
    "- `max_features='sqrt'` (default for classification)\n",
    "- `class_weight='balanced'` (handle class imbalance)\n",
    "- `oob_score=True` (free validation metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the baseline Random Forest pipeline\n",
    "rf_baseline_model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,           # Number of trees\n",
    "        max_depth=None,             # No depth limit (trees grow fully)\n",
    "        max_features='sqrt',        # sqrt(n_features) at each split\n",
    "        min_samples_split=2,        # Minimum samples to split\n",
    "        min_samples_leaf=1,         # Minimum samples in leaf\n",
    "        bootstrap=True,             # Use bootstrap sampling\n",
    "        oob_score=True,             # Calculate out-of-bag score\n",
    "        class_weight='balanced',    # Handle class imbalance\n",
    "        random_state=42,            # For reproducibility\n",
    "        n_jobs=-1                   # Use all CPU cores\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Baseline Random Forest Model:\")\n",
    "rf_baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model configuration\n",
    "classifier = rf_baseline_model.named_steps['classifier']\n",
    "print(\"Model Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"n_estimators: {classifier.n_estimators}\")\n",
    "print(f\"max_depth: {classifier.max_depth}\")\n",
    "print(f\"max_features: {classifier.max_features}\")\n",
    "print(f\"min_samples_split: {classifier.min_samples_split}\")\n",
    "print(f\"min_samples_leaf: {classifier.min_samples_leaf}\")\n",
    "print(f\"bootstrap: {classifier.bootstrap}\")\n",
    "print(f\"oob_score: {classifier.oob_score}\")\n",
    "print(f\"class_weight: {classifier.class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Alternative Configurations\n",
    "\n",
    "Let's create a few alternative Random Forest configurations for comparison:\n",
    "\n",
    "1. **More Trees**: 500 trees for increased stability\n",
    "2. **Constrained Depth**: Limited tree depth to prevent overfitting\n",
    "3. **Different max_features**: Using log2 instead of sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: More trees (500)\n",
    "rf_large_model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=500,           # More trees for stability\n",
    "        max_depth=None,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Large Random Forest Model (500 trees):\")\n",
    "rf_large_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Constrained depth (prevent overfitting)\n",
    "rf_constrained_model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,               # Limit tree depth\n",
    "        max_features='sqrt',\n",
    "        min_samples_split=5,        # Require more samples to split\n",
    "        min_samples_leaf=2,         # Require at least 2 samples in leaves\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Constrained Random Forest Model (max_depth=10):\")\n",
    "rf_constrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Different max_features (log2)\n",
    "rf_log2_model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        max_features='log2',        # log2 of features at each split\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Random Forest Model with log2 features:\")\n",
    "rf_log2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Pipeline Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all Random Forest models\n",
    "models = {\n",
    "    'RF Baseline': rf_baseline_model,\n",
    "    'RF Large (500)': rf_large_model,\n",
    "    'RF Constrained': rf_constrained_model,\n",
    "    'RF Log2': rf_log2_model\n",
    "}\n",
    "\n",
    "print(\"Random Forest Models Configured:\")\n",
    "print(\"=\"*70)\n",
    "for name, model in models.items():\n",
    "    clf = model.named_steps['classifier']\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  n_estimators: {clf.n_estimators}\")\n",
    "    print(f\"  max_depth: {clf.max_depth}\")\n",
    "    print(f\"  max_features: {clf.max_features}\")\n",
    "    print(f\"  min_samples_split: {clf.min_samples_split}\")\n",
    "    print(f\"  min_samples_leaf: {clf.min_samples_leaf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "comparison_data = []\n",
    "for name, model in models.items():\n",
    "    clf = model.named_steps['classifier']\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'n_estimators': clf.n_estimators,\n",
    "        'max_depth': str(clf.max_depth),\n",
    "        'max_features': clf.max_features,\n",
    "        'min_samples_split': clf.min_samples_split,\n",
    "        'min_samples_leaf': clf.min_samples_leaf\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model configurations\n",
    "fig = go.Figure()\n",
    "\n",
    "model_names = list(models.keys())\n",
    "n_trees = [models[m].named_steps['classifier'].n_estimators for m in model_names]\n",
    "max_depths = [models[m].named_steps['classifier'].max_depth for m in model_names]\n",
    "max_depths_display = [d if d is not None else 50 for d in max_depths]  # Display None as high value\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='n_estimators',\n",
    "    x=model_names,\n",
    "    y=n_trees,\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='max_depth (None=50)',\n",
    "    x=model_names,\n",
    "    y=max_depths_display,\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest Model Configurations',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Value',\n",
    "    barmode='group',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory for Course 3 Module 3 if it doesn't exist\n",
    "import os\n",
    "models_path = f'{course3_filepath}models/'\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save each model pipeline (untrained)\n",
    "for name, model in models.items():\n",
    "    # Create a clean filename\n",
    "    filename = name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filepath = f'{models_path}{filename}_model.pkl'\n",
    "    pickle.dump(model, open(filepath, 'wb'))\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved models\n",
    "print(\"\\nVerifying saved models:\")\n",
    "for name, model in models.items():\n",
    "    filename = name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filepath = f'{models_path}{filename}_model.pkl'\n",
    "    loaded_model = pickle.load(open(filepath, 'rb'))\n",
    "    print(f\"{name}: {type(loaded_model.named_steps['classifier']).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we built four Random Forest classification models for predicting student departure:\n",
    "\n",
    "### Models Built\n",
    "\n",
    "| Model | Description | Key Settings |\n",
    "|:------|:------------|:-------------|\n",
    "| **RF Baseline** | Standard configuration | 100 trees, sqrt features, no depth limit |\n",
    "| **RF Large** | More trees for stability | 500 trees, sqrt features |\n",
    "| **RF Constrained** | Limited depth to prevent overfitting | 100 trees, max_depth=10 |\n",
    "| **RF Log2** | Alternative feature sampling | 100 trees, log2 features |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **No scaling required**: Random Forests work with raw feature values\n",
    "2. **class_weight='balanced'**: Automatically handles class imbalance\n",
    "3. **oob_score=True**: Provides free validation metric using out-of-bag samples\n",
    "4. **n_jobs=-1**: Uses all CPU cores for faster training\n",
    "\n",
    "### Random Forest vs. Logistic Regression Pipelines\n",
    "\n",
    "| Aspect | Logistic Regression | Random Forest |\n",
    "|:-------|:--------------------|:--------------|\n",
    "| Scaling | Required | Optional |\n",
    "| Training speed | Fast | Slower (many trees) |\n",
    "| Key hyperparameters | C, penalty | n_estimators, max_depth, max_features |\n",
    "| Built-in validation | No | Yes (OOB score) |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Train these Random Forest models on our data\n",
    "2. Evaluate using Out-of-Bag (OOB) scores and cross-validation\n",
    "3. Examine feature importance to understand what drives predictions\n",
    "\n",
    "**Proceed to:** `3.3 Train and Evaluate Random Forests`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}