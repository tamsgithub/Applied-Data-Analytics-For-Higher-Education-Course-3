{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 **Tune** Random Forest Hyperparameters - Optimize Student Departure Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the Random Forest pipeline.  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### **5. Improve the Model : Tune hyperparameters for optimal performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebooks, we built and evaluated Random Forest models with default or manually selected hyperparameters. Now we systematically search for the **optimal hyperparameters** to maximize model performance.\n",
    "\n",
    "Hyperparameter tuning is critical because:\n",
    "1. Default values are not always optimal for your specific dataset\n",
    "2. Small changes in hyperparameters can significantly impact performance\n",
    "3. Different hyperparameter combinations may work better together\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Identify the key hyperparameters that affect Random Forest performance\n",
    "2. Use Grid Search to systematically explore hyperparameter combinations\n",
    "3. Use Randomized Search for efficient exploration of large search spaces\n",
    "4. Interpret hyperparameter tuning results and select the best model\n",
    "5. Compare tuned models to baseline models and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score, \n",
    "    StratifiedKFold, validation_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "models_path = f'{course3_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "print(f\"Training data: {df_training.shape}\")\n",
    "print(f\"Testing data: {df_testing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "X_test = df_testing\n",
    "y_test = df_testing['SEM_3_STATUS']\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the preprocessing pipeline\n",
    "minmax_columns = ['HS_GPA', 'GPA_1', 'GPA_2', 'DFW_RATE_1', 'DFW_RATE_2']\n",
    "standard_columns = ['UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2']\n",
    "categorical_columns = ['GENDER', 'RACE_ETHNICITY', 'FIRST_GEN_STATUS']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), minmax_columns),\n",
    "        ('standard', StandardScaler(), standard_columns),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                  drop=['Female', 'Other', 'Unknown'], \n",
    "                                  sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Key Hyperparameters to Tune\n",
    "\n",
    "Random Forests have many hyperparameters, but these are the most impactful:\n",
    "\n",
    "| Hyperparameter | Description | Typical Range | Effect on Model |\n",
    "|:---------------|:------------|:--------------|:----------------|\n",
    "| `n_estimators` | Number of trees | 100 - 1000 | More trees = more stable, slower |\n",
    "| `max_depth` | Maximum tree depth | 5 - 50 or None | Limits complexity, prevents overfitting |\n",
    "| `max_features` | Features per split | 'sqrt', 'log2', float | Controls randomness and diversity |\n",
    "| `min_samples_split` | Min samples to split | 2 - 20 | Prevents overfitting |\n",
    "| `min_samples_leaf` | Min samples in leaf | 1 - 10 | Controls tree size |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'n_estimators: More Trees = More Stable',\n",
    "    'max_depth: Deeper = More Complex',\n",
    "    'max_features: Lower = More Diverse',\n",
    "    'min_samples_split: Higher = Less Overfit'\n",
    "))\n",
    "\n",
    "# Simulated effects\n",
    "x1 = [10, 50, 100, 200, 500, 1000]\n",
    "y1_train = [0.98, 0.97, 0.96, 0.955, 0.95, 0.95]\n",
    "y1_test = [0.70, 0.78, 0.82, 0.83, 0.835, 0.84]\n",
    "\n",
    "x2 = [3, 5, 10, 15, 20, None]\n",
    "x2_display = [3, 5, 10, 15, 20, 25]\n",
    "y2_train = [0.75, 0.85, 0.92, 0.95, 0.98, 0.99]\n",
    "y2_test = [0.73, 0.82, 0.84, 0.83, 0.80, 0.78]\n",
    "\n",
    "x3 = [0.1, 0.2, 0.3, 0.5, 0.7, 1.0]\n",
    "y3_diversity = [0.95, 0.85, 0.75, 0.55, 0.35, 0.15]\n",
    "y3_accuracy = [0.72, 0.78, 0.82, 0.84, 0.85, 0.83]\n",
    "\n",
    "x4 = [2, 5, 10, 15, 20, 30]\n",
    "y4_train = [0.99, 0.96, 0.92, 0.88, 0.85, 0.80]\n",
    "y4_test = [0.78, 0.82, 0.84, 0.83, 0.82, 0.78]\n",
    "\n",
    "# Plot 1: n_estimators\n",
    "fig.add_trace(go.Scatter(x=x1, y=y1_train, name='Train', line=dict(color='lightblue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x1, y=y1_test, name='Test', line=dict(color='darkblue')), row=1, col=1)\n",
    "\n",
    "# Plot 2: max_depth\n",
    "fig.add_trace(go.Scatter(x=x2_display, y=y2_train, name='Train', line=dict(color='lightblue'), showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x2_display, y=y2_test, name='Test', line=dict(color='darkblue'), showlegend=False), row=1, col=2)\n",
    "\n",
    "# Plot 3: max_features\n",
    "fig.add_trace(go.Scatter(x=x3, y=y3_diversity, name='Diversity', line=dict(color='orange')), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x3, y=y3_accuracy, name='Accuracy', line=dict(color='green')), row=2, col=1)\n",
    "\n",
    "# Plot 4: min_samples_split\n",
    "fig.add_trace(go.Scatter(x=x4, y=y4_train, name='Train', line=dict(color='lightblue'), showlegend=False), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=x4, y=y4_test, name='Test', line=dict(color='darkblue'), showlegend=False), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, title_text='Hyperparameter Effects on Model Performance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter Interactions\n",
    "\n",
    "Hyperparameters don't work in isolation—they interact:\n",
    "\n",
    "- **n_estimators + max_features**: More trees can compensate for aggressive feature subsampling\n",
    "- **max_depth + min_samples_split**: Both control tree complexity; may be redundant to tune both aggressively\n",
    "- **Class imbalance**: `class_weight='balanced'` interacts with tree structure parameters\n",
    "\n",
    "This is why we search **combinations** of hyperparameters, not just individual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up Grid Search\n",
    "\n",
    "**Grid Search** exhaustively tries all combinations of specified hyperparameter values.\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 3 values\n",
    "    'max_depth': [5, 10, 15],          # 3 values\n",
    "    'max_features': ['sqrt', 'log2']   # 2 values\n",
    "}\n",
    "\n",
    "Total combinations: 3 × 3 × 2 = 18\n",
    "With 5-fold CV: 18 × 5 = 90 model fits\n",
    "```\n",
    "\n",
    "**Pros**: Thorough, guaranteed to find best combination in grid\n",
    "\n",
    "**Cons**: Computationally expensive, grows exponentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base pipeline for tuning\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Base pipeline created for hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "# Note: Use 'classifier__' prefix for pipeline parameters\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.3],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for key, values in param_grid.items():\n",
    "    total_combinations *= len(values)\n",
    "    print(f\"{key}: {len(values)} values\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(f\"With 5-fold CV: {total_combinations * 5} model fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: With 405 combinations and 5-fold CV, this would require 2025 model fits. We'll use a smaller grid for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced parameter grid for faster execution\n",
    "param_grid_reduced = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 15, None],\n",
    "    'classifier__max_features': ['sqrt', 'log2'],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "total_reduced = 2 * 3 * 2 * 2\n",
    "print(f\"Reduced combinations: {total_reduced}\")\n",
    "print(f\"With 5-fold CV: {total_reduced * 5} model fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Running Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid_reduced,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',  # Optimize for ROC-AUC\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Grid Search configured.\")\n",
    "print(f\"Optimizing for: ROC-AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Grid Search\n",
    "print(\"Running Grid Search...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGrid Search completed in {grid_time:.2f} seconds\")\n",
    "print(f\"Best ROC-AUC score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    param_name = param.replace('classifier__', '')\n",
    "    print(f\"{param_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analyzing Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns\n",
    "display_cols = [\n",
    "    'param_classifier__n_estimators',\n",
    "    'param_classifier__max_depth',\n",
    "    'param_classifier__max_features',\n",
    "    'param_classifier__min_samples_split',\n",
    "    'mean_train_score',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'rank_test_score'\n",
    "]\n",
    "\n",
    "# Display top 10 configurations\n",
    "top_results = results_df[display_cols].sort_values('rank_test_score').head(10)\n",
    "top_results.columns = ['n_estimators', 'max_depth', 'max_features', 'min_samples_split', \n",
    "                       'Train Score', 'Test Score', 'Std', 'Rank']\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grid Search results\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add all results as scatter\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df['mean_train_score'],\n",
    "    y=results_df['mean_test_score'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=results_df['rank_test_score'],\n",
    "        colorscale='Viridis_r',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Rank')\n",
    "    ),\n",
    "    text=[f\"n_est={row['param_classifier__n_estimators']}, depth={row['param_classifier__max_depth']}\" \n",
    "          for _, row in results_df.iterrows()],\n",
    "    hovertemplate='Train: %{x:.4f}<br>Test: %{y:.4f}<br>%{text}<extra></extra>',\n",
    "    name='Configurations'\n",
    "))\n",
    "\n",
    "# Add diagonal line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0.7, 1.0], y=[0.7, 1.0],\n",
    "    mode='lines',\n",
    "    line=dict(color='gray', dash='dash'),\n",
    "    name='Perfect Generalization'\n",
    "))\n",
    "\n",
    "# Mark best configuration\n",
    "best_idx = results_df['rank_test_score'].idxmin()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[results_df.loc[best_idx, 'mean_train_score']],\n",
    "    y=[results_df.loc[best_idx, 'mean_test_score']],\n",
    "    mode='markers',\n",
    "    marker=dict(size=20, color='red', symbol='star'),\n",
    "    name='Best Configuration'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Grid Search Results: Train vs Test Scores',\n",
    "    xaxis_title='Mean Train Score (ROC-AUC)',\n",
    "    yaxis_title='Mean Test Score (ROC-AUC)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hyperparameter effects\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Effect of n_estimators',\n",
    "    'Effect of max_depth',\n",
    "    'Effect of max_features',\n",
    "    'Effect of min_samples_split'\n",
    "))\n",
    "\n",
    "# n_estimators effect\n",
    "n_est_means = results_df.groupby('param_classifier__n_estimators')['mean_test_score'].mean()\n",
    "fig.add_trace(go.Bar(x=[str(x) for x in n_est_means.index], y=n_est_means.values, \n",
    "                     marker_color='darkblue'), row=1, col=1)\n",
    "\n",
    "# max_depth effect\n",
    "depth_means = results_df.groupby('param_classifier__max_depth')['mean_test_score'].mean()\n",
    "fig.add_trace(go.Bar(x=[str(x) for x in depth_means.index], y=depth_means.values,\n",
    "                     marker_color='darkgreen'), row=1, col=2)\n",
    "\n",
    "# max_features effect\n",
    "feat_means = results_df.groupby('param_classifier__max_features')['mean_test_score'].mean()\n",
    "fig.add_trace(go.Bar(x=[str(x) for x in feat_means.index], y=feat_means.values,\n",
    "                     marker_color='darkorange'), row=2, col=1)\n",
    "\n",
    "# min_samples_split effect\n",
    "split_means = results_df.groupby('param_classifier__min_samples_split')['mean_test_score'].mean()\n",
    "fig.add_trace(go.Bar(x=[str(x) for x in split_means.index], y=split_means.values,\n",
    "                     marker_color='darkred'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Test Score by Hyperparameter Value',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_yaxes(title_text='Mean ROC-AUC')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 When to Use Randomized Search\n",
    "\n",
    "**Randomized Search** samples a fixed number of random combinations from specified distributions.\n",
    "\n",
    "**Advantages over Grid Search**:\n",
    "- More efficient for large search spaces\n",
    "- Can explore continuous distributions\n",
    "- Often finds good solutions faster\n",
    "- Doesn't require specifying exact values\n",
    "\n",
    "**Rule of thumb**: Use Randomized Search when:\n",
    "- Grid has > 100-200 combinations\n",
    "- Hyperparameters are continuous\n",
    "- You want to explore broadly first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for randomized search\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': randint(50, 500),  # Random integer 50-500\n",
    "    'classifier__max_depth': [5, 10, 15, 20, 25, 30, None],\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.2, 0.3, 0.4, 0.5],\n",
    "    'classifier__min_samples_split': randint(2, 20),\n",
    "    'classifier__min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "print(\"Parameter distributions defined for Randomized Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementing Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Randomized Search configured.\")\n",
    "print(f\"Sampling {50} random combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Randomized Search\n",
    "print(\"Running Randomized Search...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRandomized Search completed in {random_time:.2f} seconds\")\n",
    "print(f\"Best ROC-AUC score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters from Randomized Search\n",
    "print(\"Best Hyperparameters (Randomized Search):\")\n",
    "print(\"=\"*50)\n",
    "for param, value in random_search.best_params_.items():\n",
    "    param_name = param.replace('classifier__', '')\n",
    "    print(f\"{param_name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Grid Search vs Randomized Search\n",
    "print(\"\\nComparison: Grid Search vs Randomized Search\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Grid Search:\")\n",
    "print(f\"  Best Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"  Time: {grid_time:.2f} seconds\")\n",
    "print(f\"  Combinations tested: {len(grid_search.cv_results_['mean_test_score'])}\")\n",
    "print(f\"\\nRandomized Search:\")\n",
    "print(f\"  Best Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"  Time: {random_time:.2f} seconds\")\n",
    "print(f\"  Combinations tested: {len(random_search.cv_results_['mean_test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning Individual Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tuning n_estimators\n",
    "\n",
    "Let's analyze how performance changes with the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze n_estimators using validation curve\n",
    "n_estimators_range = [10, 25, 50, 100, 150, 200, 300, 500]\n",
    "\n",
    "# Create a simple RF for validation curve (no pipeline for speed)\n",
    "# First, preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "rf_simple = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Computing validation curve for n_estimators...\")\n",
    "train_scores, test_scores = validation_curve(\n",
    "    rf_simple, X_train_preprocessed, y_train,\n",
    "    param_name='n_estimators',\n",
    "    param_range=n_estimators_range,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Validation curve computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve for n_estimators\n",
    "fig = go.Figure()\n",
    "\n",
    "# Training scores\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_range,\n",
    "    y=train_scores.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Training Score',\n",
    "    line=dict(color='lightblue', width=2)\n",
    "))\n",
    "\n",
    "# Fill area for training std\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_range + n_estimators_range[::-1],\n",
    "    y=list(train_scores.mean(axis=1) + train_scores.std(axis=1)) + \n",
    "      list(train_scores.mean(axis=1) - train_scores.std(axis=1))[::-1],\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(173,216,230,0.3)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    name='Training Std',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Test scores\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_range,\n",
    "    y=test_scores.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Cross-Validation Score',\n",
    "    line=dict(color='darkblue', width=2)\n",
    "))\n",
    "\n",
    "# Fill area for test std\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_estimators_range + n_estimators_range[::-1],\n",
    "    y=list(test_scores.mean(axis=1) + test_scores.std(axis=1)) + \n",
    "      list(test_scores.mean(axis=1) - test_scores.std(axis=1))[::-1],\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(0,0,139,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    name='CV Std',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation Curve: n_estimators',\n",
    "    xaxis_title='Number of Trees (n_estimators)',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for max_depth\n",
    "max_depth_range = [3, 5, 7, 10, 12, 15, 20, 25, 30]\n",
    "\n",
    "rf_depth = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Computing validation curve for max_depth...\")\n",
    "train_scores_depth, test_scores_depth = validation_curve(\n",
    "    rf_depth, X_train_preprocessed, y_train,\n",
    "    param_name='max_depth',\n",
    "    param_range=max_depth_range,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Validation curve computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve for max_depth\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=max_depth_range,\n",
    "    y=train_scores_depth.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Training Score',\n",
    "    line=dict(color='lightgreen', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=max_depth_range,\n",
    "    y=test_scores_depth.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Cross-Validation Score',\n",
    "    line=dict(color='darkgreen', width=2)\n",
    "))\n",
    "\n",
    "# Find optimal depth\n",
    "optimal_idx = test_scores_depth.mean(axis=1).argmax()\n",
    "optimal_depth = max_depth_range[optimal_idx]\n",
    "\n",
    "fig.add_vline(x=optimal_depth, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=f\"Optimal: {optimal_depth}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation Curve: max_depth',\n",
    "    xaxis_title='Maximum Tree Depth (max_depth)',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Tuning max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_features values\n",
    "max_features_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "rf_features = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Computing validation curve for max_features...\")\n",
    "train_scores_feat, test_scores_feat = validation_curve(\n",
    "    rf_features, X_train_preprocessed, y_train,\n",
    "    param_name='max_features',\n",
    "    param_range=max_features_values,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Validation curve computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve for max_features\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=max_features_values,\n",
    "    y=train_scores_feat.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Training Score',\n",
    "    line=dict(color='lightsalmon', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=max_features_values,\n",
    "    y=test_scores_feat.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Cross-Validation Score',\n",
    "    line=dict(color='darkorange', width=2)\n",
    "))\n",
    "\n",
    "# Mark sqrt and log2 equivalents (approximately)\n",
    "n_features = X_train_preprocessed.shape[1]\n",
    "sqrt_equiv = np.sqrt(n_features) / n_features\n",
    "log2_equiv = np.log2(n_features) / n_features\n",
    "\n",
    "fig.add_vline(x=sqrt_equiv, line_dash=\"dot\", line_color=\"purple\",\n",
    "              annotation_text=f\"sqrt ({sqrt_equiv:.2f})\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation Curve: max_features (fraction of features)',\n",
    "    xaxis_title='Fraction of Features (max_features)',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tuning min_samples_split and min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for min_samples_split\n",
    "min_split_range = [2, 5, 10, 15, 20, 30, 50]\n",
    "\n",
    "rf_split = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Computing validation curve for min_samples_split...\")\n",
    "train_scores_split, test_scores_split = validation_curve(\n",
    "    rf_split, X_train_preprocessed, y_train,\n",
    "    param_name='min_samples_split',\n",
    "    param_range=min_split_range,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Validation curve computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curves for min_samples_split\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=min_split_range,\n",
    "    y=train_scores_split.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Training Score',\n",
    "    line=dict(color='lightcoral', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=min_split_range,\n",
    "    y=test_scores_split.mean(axis=1),\n",
    "    mode='lines+markers',\n",
    "    name='Cross-Validation Score',\n",
    "    line=dict(color='darkred', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation Curve: min_samples_split',\n",
    "    xaxis_title='Minimum Samples to Split (min_samples_split)',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Best Hyperparameters\n",
    "\n",
    "Based on our tuning experiments, let's select the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best parameters from different searches\n",
    "print(\"Best Hyperparameters Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGrid Search Best:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "print(f\"  Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nRandomized Search Best:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "print(f\"  Score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model\n",
    "if random_search.best_score_ >= grid_search.best_score_:\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    best_source = 'Randomized Search'\n",
    "else:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_source = 'Grid Search'\n",
    "\n",
    "print(f\"\\nSelected Best Model from: {best_source}\")\n",
    "print(f\"Best CV Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob)\n",
    "}\n",
    "\n",
    "print(\"Final Model Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Departed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = go.Figure(go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Retained', 'Predicted Departed'],\n",
    "    y=['Actual Retained', 'Actual Departed'],\n",
    "    colorscale='Blues',\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont=dict(size=16)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix - Tuned Random Forest',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr,\n",
    "    mode='lines',\n",
    "    name=f'Tuned RF (AUC={auc:.4f})',\n",
    "    line=dict(color='darkblue', width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random (AUC=0.5)',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curve - Tuned Random Forest',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing to Baseline and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline RF model\n",
    "baseline_rf = pickle.load(open(f'{models_path}rf_baseline_model.pkl', 'rb'))\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_baseline = baseline_rf.predict(X_test)\n",
    "y_prob_baseline = baseline_rf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load logistic regression model if available\n",
    "try:\n",
    "    lr_model = pickle.load(open(f'{models_path}l2_ridge_logistic_model.pkl', 'rb'))\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "    y_prob_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "    lr_available = True\n",
    "    print(\"Logistic Regression model loaded.\")\n",
    "except:\n",
    "    lr_available = False\n",
    "    print(\"Logistic Regression model not available for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'RF Baseline',\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "        'Precision': precision_score(y_test, y_pred_baseline),\n",
    "        'Recall': recall_score(y_test, y_pred_baseline),\n",
    "        'F1-Score': f1_score(y_test, y_pred_baseline),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob_baseline)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'RF Tuned',\n",
    "        'Accuracy': test_metrics['Accuracy'],\n",
    "        'Precision': test_metrics['Precision'],\n",
    "        'Recall': test_metrics['Recall'],\n",
    "        'F1-Score': test_metrics['F1-Score'],\n",
    "        'ROC-AUC': test_metrics['ROC-AUC']\n",
    "    }\n",
    "]\n",
    "\n",
    "if lr_available:\n",
    "    comparison_data.append({\n",
    "        'Model': 'Logistic Regression (L2)',\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "        'Precision': precision_score(y_test, y_pred_lr),\n",
    "        'Recall': recall_score(y_test, y_pred_lr),\n",
    "        'F1-Score': f1_score(y_test, y_pred_lr),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob_lr)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors = ['lightblue', 'darkblue', 'green']\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    values = [row[m] for m in metrics]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=row['Model'],\n",
    "        x=metrics,\n",
    "        y=values,\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Baseline vs Tuned RF vs Logistic Regression',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "# Baseline RF\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_prob_baseline)\n",
    "auc_base = roc_auc_score(y_test, y_prob_baseline)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr_base, y=tpr_base,\n",
    "    mode='lines',\n",
    "    name=f'RF Baseline (AUC={auc_base:.4f})',\n",
    "    line=dict(color='lightblue', width=2)\n",
    "))\n",
    "\n",
    "# Tuned RF\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr,\n",
    "    mode='lines',\n",
    "    name=f'RF Tuned (AUC={auc:.4f})',\n",
    "    line=dict(color='darkblue', width=2)\n",
    "))\n",
    "\n",
    "# Logistic Regression if available\n",
    "if lr_available:\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "    auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr_lr, y=tpr_lr,\n",
    "        mode='lines',\n",
    "        name=f'Logistic Regression (AUC={auc_lr:.4f})',\n",
    "        line=dict(color='green', width=2)\n",
    "    ))\n",
    "\n",
    "# Random baseline\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random (AUC=0.5)',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curve Comparison',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "tuned_model_path = f'{models_path}rf_tuned_best_model.pkl'\n",
    "pickle.dump(best_model, open(tuned_model_path, 'wb'))\n",
    "print(f\"Tuned model saved to: {tuned_model_path}\")\n",
    "\n",
    "# Save best parameters\n",
    "params_path = f'{models_path}rf_best_params.pkl'\n",
    "pickle.dump(best_params, open(params_path, 'wb'))\n",
    "print(f\"Best parameters saved to: {params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we systematically tuned Random Forest hyperparameters for student departure prediction.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Hyperparameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param.replace('classifier__', '')}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Methods Comparison\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|:-------|:-----|:-----|:---------|\n",
    "| **Grid Search** | Thorough, exhaustive | Slow, exponential growth | Small search spaces |\n",
    "| **Randomized Search** | Efficient, explores broadly | May miss optimal | Large search spaces |\n",
    "| **Validation Curves** | Visual understanding | Single parameter at a time | Initial exploration |\n",
    "\n",
    "### Hyperparameter Insights\n",
    "\n",
    "| Hyperparameter | Finding |\n",
    "|:---------------|:--------|\n",
    "| `n_estimators` | More trees help, diminishing returns after ~200 |\n",
    "| `max_depth` | Moderate depth prevents overfitting |\n",
    "| `max_features` | 'sqrt' or similar works well for classification |\n",
    "| `min_samples_split` | Small values (2-5) often work best |\n",
    "\n",
    "### Performance Improvement\n",
    "\n",
    "Tuning typically improves model performance by:\n",
    "- Reducing overfitting through depth/sample constraints\n",
    "- Finding the optimal trade-off between bias and variance\n",
    "- Balancing tree diversity and individual accuracy\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With our tuned Random Forest model, you can:\n",
    "1. Deploy the model for student departure predictions\n",
    "2. Compare with other ensemble methods (Gradient Boosting)\n",
    "3. Explore more advanced feature engineering\n",
    "\n",
    "**Module Complete!** You have successfully learned to build, train, evaluate, and tune Random Forest models for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}