{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Introduction to Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Module 3, we explored **Random Forests**—an ensemble method that combines many decision trees trained in parallel on bootstrap samples. Random Forests use **bagging** to reduce variance. In this module, we introduce **Gradient Boosting**—a fundamentally different ensemble approach that builds trees **sequentially**, with each new tree correcting the errors of the previous ones.\n",
    "\n",
    "Gradient Boosting has become one of the most successful machine learning algorithms, consistently winning Kaggle competitions and powering production systems across industries. Libraries like **XGBoost**, **LightGBM**, and **CatBoost** have made gradient boosting accessible, fast, and highly effective.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the difference between bagging and boosting ensemble strategies\n",
    "2. Describe how AdaBoost uses sample weights to focus on difficult examples\n",
    "3. Understand how gradient boosting fits trees to residual errors\n",
    "4. Compare XGBoost, LightGBM, and CatBoost libraries\n",
    "5. Articulate why gradient boosting is effective for student departure prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Bagging to Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Recap: How Bagging Works\n",
    "\n",
    "In **Bagging** (Bootstrap Aggregating), we:\n",
    "\n",
    "1. Create multiple bootstrap samples from the training data\n",
    "2. Train one model (tree) on each sample **independently**\n",
    "3. Combine predictions by voting or averaging\n",
    "\n",
    "Key characteristics:\n",
    "- Trees are trained in **parallel** (independent of each other)\n",
    "- Each tree sees a different random subset of data\n",
    "- Primary goal: **Reduce variance** by averaging\n",
    "\n",
    "```\n",
    "Bagging: Parallel Training\n",
    "                 Data\n",
    "                  |\n",
    "    +-------------+-------------+\n",
    "    |             |             |\n",
    "  Tree 1       Tree 2       Tree 3  ...  (Independent)\n",
    "    |             |             |\n",
    "    +-------------+-------------+\n",
    "                  |\n",
    "              Average/Vote\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Boosting Philosophy\n",
    "\n",
    "**Boosting** takes a fundamentally different approach:\n",
    "\n",
    "1. Train models **sequentially** (one after another)\n",
    "2. Each new model focuses on the **mistakes** of previous models\n",
    "3. Combine models with weighted voting\n",
    "\n",
    "Key characteristics:\n",
    "- Trees are trained **sequentially** (each depends on previous)\n",
    "- Later trees specifically target hard-to-predict samples\n",
    "- Primary goal: **Reduce bias** by iteratively improving\n",
    "\n",
    "```\n",
    "Boosting: Sequential Training\n",
    "\n",
    "Data --> Tree 1 --> Errors --> Tree 2 --> Errors --> Tree 3 --> ...\n",
    "           |                     |                     |\n",
    "           v                     v                     v\n",
    "        Focus on             Focus on              Focus on\n",
    "        all samples     Tree 1's mistakes    Tree 1+2's mistakes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Visualize the conceptual difference between Bagging and Boosting\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Bagging: Parallel, Independent Trees',\n",
    "    'Boosting: Sequential, Dependent Trees'\n",
    "))\n",
    "\n",
    "# Bagging visualization (left)\n",
    "# Data source\n",
    "fig.add_shape(type=\"rect\", x0=2, y0=4.5, x1=4, y1=5.5,\n",
    "              fillcolor=\"lightblue\", line=dict(color=\"darkblue\", width=2),\n",
    "              row=1, col=1)\n",
    "fig.add_annotation(x=3, y=5, text=\"Training Data\", showarrow=False, row=1, col=1)\n",
    "\n",
    "# Parallel trees for bagging\n",
    "for i, x_pos in enumerate([1, 3, 5]):\n",
    "    fig.add_shape(type=\"rect\", x0=x_pos-0.6, y0=2.5, x1=x_pos+0.6, y1=3.5,\n",
    "                  fillcolor=\"lightgreen\", line=dict(color=\"green\", width=2),\n",
    "                  row=1, col=1)\n",
    "    fig.add_annotation(x=x_pos, y=3, text=f\"Tree {i+1}\", showarrow=False, row=1, col=1)\n",
    "    # Arrow from data to tree\n",
    "    fig.add_annotation(x=x_pos, y=3.5, ax=3, ay=4.5, xref=\"x\", yref=\"y\",\n",
    "                      axref=\"x\", ayref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\",\n",
    "                      row=1, col=1)\n",
    "\n",
    "# Aggregation for bagging\n",
    "fig.add_shape(type=\"rect\", x0=2, y0=0.5, x1=4, y1=1.5,\n",
    "              fillcolor=\"lightyellow\", line=dict(color=\"orange\", width=2),\n",
    "              row=1, col=1)\n",
    "fig.add_annotation(x=3, y=1, text=\"Vote/Average\", showarrow=False, row=1, col=1)\n",
    "\n",
    "for x_pos in [1, 3, 5]:\n",
    "    fig.add_annotation(x=3, y=1.5, ax=x_pos, ay=2.5, xref=\"x\", yref=\"y\",\n",
    "                      axref=\"x\", ayref=\"y\", showarrow=True, arrowhead=2, arrowcolor=\"gray\",\n",
    "                      row=1, col=1)\n",
    "\n",
    "# Boosting visualization (right)\n",
    "# Sequential trees\n",
    "positions = [(1, 4), (3, 4), (5, 4)]\n",
    "for i, (x, y) in enumerate(positions):\n",
    "    fig.add_shape(type=\"rect\", x0=x-0.6, y0=y-0.5, x1=x+0.6, y1=y+0.5,\n",
    "                  fillcolor=\"lightcoral\" if i == 0 else \"lightyellow\" if i == 1 else \"lightgreen\",\n",
    "                  line=dict(color=\"darkred\" if i == 0 else \"orange\" if i == 1 else \"green\", width=2),\n",
    "                  row=1, col=2)\n",
    "    fig.add_annotation(x=x, y=y, text=f\"Tree {i+1}\", showarrow=False, row=1, col=2)\n",
    "\n",
    "# Arrows showing sequential dependency\n",
    "fig.add_annotation(x=2.4, y=4, ax=1.6, ay=4, xref=\"x2\", yref=\"y2\",\n",
    "                  axref=\"x2\", ayref=\"y2\", showarrow=True, arrowhead=2, arrowcolor=\"darkblue\",\n",
    "                  row=1, col=2)\n",
    "fig.add_annotation(x=4.4, y=4, ax=3.6, ay=4, xref=\"x2\", yref=\"y2\",\n",
    "                  axref=\"x2\", ayref=\"y2\", showarrow=True, arrowhead=2, arrowcolor=\"darkblue\",\n",
    "                  row=1, col=2)\n",
    "\n",
    "# Labels for errors\n",
    "fig.add_annotation(x=2, y=4.7, text=\"errors\", showarrow=False, font=dict(size=10, color=\"darkblue\"),\n",
    "                  row=1, col=2)\n",
    "fig.add_annotation(x=4, y=4.7, text=\"errors\", showarrow=False, font=dict(size=10, color=\"darkblue\"),\n",
    "                  row=1, col=2)\n",
    "\n",
    "# Aggregation for boosting\n",
    "fig.add_shape(type=\"rect\", x0=2, y0=1.5, x1=4, y1=2.5,\n",
    "              fillcolor=\"lavender\", line=dict(color=\"purple\", width=2),\n",
    "              row=1, col=2)\n",
    "fig.add_annotation(x=3, y=2, text=\"Weighted Sum\", showarrow=False, row=1, col=2)\n",
    "\n",
    "for x in [1, 3, 5]:\n",
    "    fig.add_annotation(x=3, y=2.5, ax=x, ay=3.5, xref=\"x2\", yref=\"y2\",\n",
    "                      axref=\"x2\", ayref=\"y2\", showarrow=True, arrowhead=2, arrowcolor=\"gray\",\n",
    "                      row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(visible=False, range=[0, 6])\n",
    "fig.update_yaxes(visible=False, range=[0, 6])\n",
    "fig.update_layout(height=450, title_text=\"Bagging vs. Boosting: Two Ensemble Strategies\",\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: \n",
    "- **Bagging** reduces variance by averaging independent predictions\n",
    "- **Boosting** reduces bias by sequentially correcting errors\n",
    "\n",
    "Both approaches can reduce overall prediction error, but they do so in fundamentally different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AdaBoost: The Original Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 How AdaBoost Works\n",
    "\n",
    "**AdaBoost** (Adaptive Boosting), introduced by Freund and Schapire in 1996, was the first practical boosting algorithm. It works by:\n",
    "\n",
    "1. **Initialize**: Give all training samples equal weight\n",
    "2. **For each iteration**:\n",
    "   - Train a weak learner (typically a decision stump—a tree with one split)\n",
    "   - Calculate the weighted error rate\n",
    "   - Compute the learner's weight (higher accuracy = higher weight)\n",
    "   - **Increase weights** of misclassified samples\n",
    "   - **Decrease weights** of correctly classified samples\n",
    "3. **Final prediction**: Weighted vote of all weak learners\n",
    "\n",
    "**The key idea**: Samples that are hard to classify get higher weights, forcing subsequent learners to focus on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how AdaBoost reweights samples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample data: 10 students\n",
    "n_samples = 10\n",
    "students = [f\"S{i+1}\" for i in range(n_samples)]\n",
    "\n",
    "# Initial weights (uniform)\n",
    "initial_weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Simulate classifications\n",
    "# Round 1: Tree 1 misclassifies S3, S7, S8\n",
    "misclassified_r1 = [2, 6, 7]  # indices\n",
    "correct_r1 = [i for i in range(n_samples) if i not in misclassified_r1]\n",
    "\n",
    "# Calculate weights after round 1 (simplified demonstration)\n",
    "weights_r1 = initial_weights.copy()\n",
    "weights_r1[misclassified_r1] *= 2.5  # Increase weight of misclassified\n",
    "weights_r1[correct_r1] *= 0.7  # Decrease weight of correct\n",
    "weights_r1 = weights_r1 / weights_r1.sum()  # Normalize\n",
    "\n",
    "# Round 2: Tree 2 focuses on hard samples, misclassifies S3, S5\n",
    "misclassified_r2 = [2, 4]\n",
    "correct_r2 = [i for i in range(n_samples) if i not in misclassified_r2]\n",
    "\n",
    "weights_r2 = weights_r1.copy()\n",
    "weights_r2[misclassified_r2] *= 2.5\n",
    "weights_r2[correct_r2] *= 0.7\n",
    "weights_r2 = weights_r2 / weights_r2.sum()\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Initial Weights',\n",
    "    x=students,\n",
    "    y=initial_weights,\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='After Round 1',\n",
    "    x=students,\n",
    "    y=weights_r1,\n",
    "    marker_color='orange'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='After Round 2',\n",
    "    x=students,\n",
    "    y=weights_r2,\n",
    "    marker_color='red'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='AdaBoost: Sample Weights Evolve to Focus on Hard Examples',\n",
    "    xaxis_title='Student',\n",
    "    yaxis_title='Sample Weight',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    annotations=[\n",
    "        dict(x='S3', y=weights_r2[2]+0.02, text='Hard sample',\n",
    "             showarrow=True, arrowhead=2, ax=0, ay=-30)\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Misclassified in Round 1: S3, S7, S8 (weights increased)\")\n",
    "print(\"Misclassified in Round 2: S3, S5 (S3 weight increased again)\")\n",
    "print(f\"\\nS3's weight evolution: {initial_weights[2]:.2f} -> {weights_r1[2]:.2f} -> {weights_r2[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing AdaBoost\n",
    "\n",
    "AdaBoost can be understood as **fitting weak learners** (simple decision stumps) that, when combined, create a strong classifier. Each stump makes a simple decision, but together they capture complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how AdaBoost combines weak learners\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 2D data with a non-linear boundary\n",
    "n_points = 200\n",
    "X1 = np.random.uniform(0, 10, n_points)\n",
    "X2 = np.random.uniform(0, 10, n_points)\n",
    "# XOR-like pattern\n",
    "y = ((X1 > 5) ^ (X2 > 5)).astype(int)\n",
    "# Add some noise\n",
    "noise_idx = np.random.choice(n_points, 20, replace=False)\n",
    "y[noise_idx] = 1 - y[noise_idx]\n",
    "\n",
    "# Simulate decision stumps\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Stump 1: X1 < 5?',\n",
    "    'Stump 2: X2 < 5?',\n",
    "    'Stump 3: X1 < 5? (weighted)',\n",
    "    'Combined: AdaBoost'\n",
    "))\n",
    "\n",
    "colors = ['blue' if yi == 0 else 'red' for yi in y]\n",
    "\n",
    "# Data points on all subplots\n",
    "for row in [1, 2]:\n",
    "    for col in [1, 2]:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X1, y=X2, mode='markers',\n",
    "            marker=dict(color=colors, size=6, opacity=0.6),\n",
    "            showlegend=False\n",
    "        ), row=row, col=col)\n",
    "\n",
    "# Stump 1: vertical line at X1=5\n",
    "fig.add_vline(x=5, line_dash=\"dash\", line_color=\"green\", line_width=3, row=1, col=1)\n",
    "\n",
    "# Stump 2: horizontal line at X2=5\n",
    "fig.add_hline(y=5, line_dash=\"dash\", line_color=\"green\", line_width=3, row=1, col=2)\n",
    "\n",
    "# Stump 3: another vertical line (weighted differently)\n",
    "fig.add_vline(x=5, line_dash=\"dash\", line_color=\"purple\", line_width=2, row=2, col=1)\n",
    "\n",
    "# Combined: show the XOR-like decision boundary\n",
    "fig.add_vline(x=5, line_dash=\"solid\", line_color=\"green\", line_width=2, row=2, col=2)\n",
    "fig.add_hline(y=5, line_dash=\"solid\", line_color=\"green\", line_width=2, row=2, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='GPA (scaled)', range=[0, 10])\n",
    "fig.update_yaxes(title_text='DFW Rate (scaled)', range=[0, 10])\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text='AdaBoost: Combining Weak Learners (Decision Stumps)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Each decision stump makes a simple split. Individually, they perform only slightly better than random. But when combined with appropriate weights, they can capture complex non-linear patterns like the XOR boundary shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting: Learning from Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Gradient Descent Connection\n",
    "\n",
    "**Gradient Boosting** takes a different approach than AdaBoost. Instead of reweighting samples, it:\n",
    "\n",
    "1. Fits a model to the data\n",
    "2. Calculates the **residuals** (errors) for each sample\n",
    "3. Fits the next model to predict these residuals\n",
    "4. Updates predictions by adding the new model's predictions\n",
    "5. Repeats\n",
    "\n",
    "The name comes from the connection to **gradient descent** optimization:\n",
    "\n",
    "- In gradient descent, we update parameters by moving in the direction of the negative gradient\n",
    "- In gradient boosting, we update predictions by adding a model that predicts the negative gradient of the loss function\n",
    "\n",
    "For squared error loss, the negative gradient is simply the residual: $y - \\hat{y}$\n",
    "\n",
    "**Key equation**:\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "Where:\n",
    "- $F_m(x)$ is the prediction after $m$ iterations\n",
    "- $\\eta$ is the learning rate\n",
    "- $h_m(x)$ is the new tree fitted to residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient boosting with residuals\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple 1D regression example\n",
    "X = np.linspace(0, 10, 50)\n",
    "y_true = np.sin(X) + 0.5 * X  # True function\n",
    "y = y_true + np.random.normal(0, 0.3, len(X))  # Noisy observations\n",
    "\n",
    "# Iteration 0: Start with the mean\n",
    "pred_0 = np.full_like(y, y.mean())\n",
    "residuals_0 = y - pred_0\n",
    "\n",
    "# Iteration 1: Fit to residuals (simulate with partial fit)\n",
    "# Simple step function for demonstration\n",
    "tree_1 = np.where(X < 5, residuals_0[X < 5].mean(), residuals_0[X >= 5].mean())\n",
    "pred_1 = pred_0 + 0.5 * tree_1  # Learning rate = 0.5\n",
    "residuals_1 = y - pred_1\n",
    "\n",
    "# Iteration 2: Fit to new residuals\n",
    "tree_2 = np.where(X < 2.5, residuals_1[X < 2.5].mean(),\n",
    "                  np.where(X < 7.5, residuals_1[(X >= 2.5) & (X < 7.5)].mean(),\n",
    "                           residuals_1[X >= 7.5].mean()))\n",
    "pred_2 = pred_1 + 0.5 * tree_2\n",
    "residuals_2 = y - pred_2\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=(\n",
    "    'Iteration 0: Mean', 'Iteration 1: +Tree 1', 'Iteration 2: +Tree 2',\n",
    "    'Residuals after 0', 'Residuals after 1', 'Residuals after 2'\n",
    "))\n",
    "\n",
    "# Top row: Predictions\n",
    "for col, (pred, title) in enumerate([(pred_0, 'Iter 0'), (pred_1, 'Iter 1'), (pred_2, 'Iter 2')], 1):\n",
    "    fig.add_trace(go.Scatter(x=X, y=y, mode='markers', marker=dict(color='blue', size=6),\n",
    "                             name='Data', showlegend=(col==1)), row=1, col=col)\n",
    "    fig.add_trace(go.Scatter(x=X, y=pred, mode='lines', line=dict(color='red', width=2),\n",
    "                             name='Prediction', showlegend=(col==1)), row=1, col=col)\n",
    "\n",
    "# Bottom row: Residuals\n",
    "for col, resid in enumerate([residuals_0, residuals_1, residuals_2], 1):\n",
    "    fig.add_trace(go.Bar(x=list(range(len(resid))), y=resid, marker_color='lightblue',\n",
    "                         showlegend=False), row=2, col=col)\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=2, col=col)\n",
    "\n",
    "fig.update_layout(height=500, title_text='Gradient Boosting: Iteratively Fitting Residuals')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Mean Squared Error progression:\")\n",
    "print(f\"  After iteration 0: {np.mean(residuals_0**2):.3f}\")\n",
    "print(f\"  After iteration 1: {np.mean(residuals_1**2):.3f}\")\n",
    "print(f\"  After iteration 2: {np.mean(residuals_2**2):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Top row: The prediction (red line) gets closer to the data points with each iteration\n",
    "- Bottom row: The residuals (errors) get smaller and more random\n",
    "- Each tree learns to predict what the previous ensemble got wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 How Gradient Boosting Works\n",
    "\n",
    "The gradient boosting algorithm for classification:\n",
    "\n",
    "**Algorithm: Gradient Boosting for Classification**\n",
    "\n",
    "1. **Initialize** with a constant prediction (e.g., log-odds of the positive class)\n",
    "2. **For m = 1 to M** (number of trees):\n",
    "   - Compute **pseudo-residuals**: the gradient of the loss with respect to predictions\n",
    "   - Fit a decision tree $h_m$ to the pseudo-residuals\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$\n",
    "3. **Final prediction**: Convert $F_M(x)$ to probabilities using sigmoid function\n",
    "\n",
    "For **log loss** (binary classification):\n",
    "- Pseudo-residual = $y - p$ where $p$ is the current predicted probability\n",
    "- This is the same as fitting trees to the difference between actual and predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the gradient boosting process for classification\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated student departure data\n",
    "n_students = 100\n",
    "gpa = np.random.uniform(1.5, 4.0, n_students)\n",
    "true_prob = sigmoid(-3 + 1.5 * (4 - gpa))  # Lower GPA = higher departure prob\n",
    "departed = np.random.binomial(1, true_prob)\n",
    "\n",
    "# Gradient Boosting iterations\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Iteration 0: Initialize with log-odds\n",
    "p_mean = departed.mean()\n",
    "F_0 = np.full(n_students, np.log(p_mean / (1 - p_mean)))\n",
    "prob_0 = sigmoid(F_0)\n",
    "pseudo_resid_0 = departed - prob_0\n",
    "\n",
    "# Iteration 1: Fit tree to pseudo-residuals\n",
    "# Simple split at GPA = 2.5\n",
    "tree_1_pred = np.where(gpa < 2.5, pseudo_resid_0[gpa < 2.5].mean(), pseudo_resid_0[gpa >= 2.5].mean())\n",
    "F_1 = F_0 + learning_rate * tree_1_pred\n",
    "prob_1 = sigmoid(F_1)\n",
    "pseudo_resid_1 = departed - prob_1\n",
    "\n",
    "# Iteration 2\n",
    "tree_2_pred = np.where(gpa < 2.0, pseudo_resid_1[gpa < 2.0].mean(),\n",
    "                       np.where(gpa < 3.0, pseudo_resid_1[(gpa >= 2.0) & (gpa < 3.0)].mean(),\n",
    "                                pseudo_resid_1[gpa >= 3.0].mean()))\n",
    "F_2 = F_1 + learning_rate * tree_2_pred\n",
    "prob_2 = sigmoid(F_2)\n",
    "\n",
    "# Visualize\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "    'Iteration 0: Constant', 'Iteration 1: +Tree 1', 'Iteration 2: +Tree 2'\n",
    "))\n",
    "\n",
    "for col, prob in enumerate([prob_0, prob_1, prob_2], 1):\n",
    "    # Actual outcomes\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=gpa[departed==0], y=np.zeros(sum(departed==0)),\n",
    "        mode='markers', marker=dict(color='blue', size=8, symbol='circle'),\n",
    "        name='Retained', showlegend=(col==1)\n",
    "    ), row=1, col=col)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=gpa[departed==1], y=np.ones(sum(departed==1)),\n",
    "        mode='markers', marker=dict(color='red', size=8, symbol='circle'),\n",
    "        name='Departed', showlegend=(col==1)\n",
    "    ), row=1, col=col)\n",
    "    \n",
    "    # Predicted probabilities\n",
    "    sort_idx = np.argsort(gpa)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=gpa[sort_idx], y=prob[sort_idx],\n",
    "        mode='lines', line=dict(color='green', width=3),\n",
    "        name='P(Departed)', showlegend=(col==1)\n",
    "    ), row=1, col=col)\n",
    "\n",
    "fig.update_xaxes(title_text='GPA')\n",
    "fig.update_yaxes(title_text='P(Departed)', range=[-0.1, 1.1])\n",
    "fig.update_layout(height=400, title_text='Gradient Boosting for Classification: Probability Refinement')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: The predicted probability curve becomes more refined with each iteration, better separating students who departed from those who were retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 AdaBoost vs. Gradient Boosting\n",
    "\n",
    "| Aspect | AdaBoost | Gradient Boosting |\n",
    "|:-------|:---------|:------------------|\n",
    "| **Focus mechanism** | Reweights samples | Fits residuals |\n",
    "| **Base learners** | Typically decision stumps | Deeper trees allowed |\n",
    "| **Loss function** | Exponential loss | Any differentiable loss |\n",
    "| **Sensitivity to outliers** | Very sensitive | Less sensitive (depending on loss) |\n",
    "| **Noise handling** | Can overfit to noisy samples | More robust |\n",
    "| **Flexibility** | Limited | Highly flexible (custom losses) |\n",
    "| **Modern usage** | Less common | Very common (XGBoost, LightGBM, CatBoost) |\n",
    "\n",
    "**Key insight**: Gradient boosting is more general and flexible. By choosing different loss functions, we can adapt it to various problems:\n",
    "- Log loss for classification\n",
    "- MSE or MAE for regression\n",
    "- Ranking losses for search engines\n",
    "- Custom losses for specific business problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modern Gradient Boosting Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic gradient boosting algorithm has been enhanced significantly by modern libraries. The three most popular implementations are **XGBoost**, **LightGBM**, and **CatBoost**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 XGBoost\n",
    "\n",
    "**XGBoost** (eXtreme Gradient Boosting) was released in 2014 by Tianqi Chen and became famous for winning numerous Kaggle competitions.\n",
    "\n",
    "**Key innovations**:\n",
    "- **Regularization**: L1 and L2 regularization on leaf weights to prevent overfitting\n",
    "- **Sparsity awareness**: Efficient handling of missing values\n",
    "- **Parallel tree construction**: Faster training\n",
    "- **Cache optimization**: Efficient memory usage\n",
    "- **Out-of-core computing**: Can handle datasets larger than memory\n",
    "\n",
    "**Best for**:\n",
    "- General-purpose gradient boosting\n",
    "- When you need a reliable, well-tested implementation\n",
    "- Competitions and benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LightGBM\n",
    "\n",
    "**LightGBM** (Light Gradient Boosting Machine) was released by Microsoft in 2017, focusing on speed and memory efficiency.\n",
    "\n",
    "**Key innovations**:\n",
    "- **Gradient-based One-Side Sampling (GOSS)**: Keeps samples with large gradients, samples from small gradients\n",
    "- **Exclusive Feature Bundling (EFB)**: Bundles mutually exclusive features to reduce dimensionality\n",
    "- **Histogram-based splitting**: Bins continuous features for faster splits\n",
    "- **Leaf-wise tree growth**: Grows trees by splitting the leaf with maximum delta loss (vs. level-wise)\n",
    "\n",
    "**Best for**:\n",
    "- Large datasets (millions of rows)\n",
    "- High-dimensional data\n",
    "- When training speed is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 CatBoost\n",
    "\n",
    "**CatBoost** (Categorical Boosting) was released by Yandex in 2017, with a focus on handling categorical features.\n",
    "\n",
    "**Key innovations**:\n",
    "- **Native categorical feature support**: No manual encoding required\n",
    "- **Ordered boosting**: Prevents target leakage by using \"future\" data ordering\n",
    "- **Symmetric trees**: Faster prediction and less overfitting\n",
    "- **GPU acceleration**: Efficient GPU training\n",
    "\n",
    "**Best for**:\n",
    "- Data with many categorical features\n",
    "- When you want minimal preprocessing\n",
    "- Preventing overfitting on small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Feature': [\n",
    "        'Release Year',\n",
    "        'Developer',\n",
    "        'Tree Growth Strategy',\n",
    "        'Categorical Handling',\n",
    "        'Missing Value Handling',\n",
    "        'Training Speed',\n",
    "        'Memory Usage',\n",
    "        'GPU Support',\n",
    "        'Best Use Case',\n",
    "        'Default Regularization'\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        '2014',\n",
    "        'DMLC',\n",
    "        'Level-wise (default)',\n",
    "        'Requires encoding',\n",
    "        'Learns optimal direction',\n",
    "        'Fast',\n",
    "        'Moderate',\n",
    "        'Yes',\n",
    "        'General purpose, competitions',\n",
    "        'L1 + L2 on weights'\n",
    "    ],\n",
    "    'LightGBM': [\n",
    "        '2017',\n",
    "        'Microsoft',\n",
    "        'Leaf-wise',\n",
    "        'Native (integer encoded)',\n",
    "        'Learns optimal direction',\n",
    "        'Very Fast',\n",
    "        'Low',\n",
    "        'Yes',\n",
    "        'Large datasets, speed critical',\n",
    "        'L1 + L2 on weights'\n",
    "    ],\n",
    "    'CatBoost': [\n",
    "        '2017',\n",
    "        'Yandex',\n",
    "        'Symmetric trees',\n",
    "        'Native (string/object)',\n",
    "        'Native support',\n",
    "        'Fast',\n",
    "        'Moderate',\n",
    "        'Yes (excellent)',\n",
    "        'Categorical features, easy setup',\n",
    "        'L2 + ordered boosting'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Style the dataframe\n",
    "styled_df = comparison_df.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('text-align', 'left'), ('font-weight', 'bold')]}\n",
    "])\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relative strengths\n",
    "categories = ['Training Speed', 'Memory Efficiency', 'Categorical Handling', \n",
    "              'Ease of Use', 'Accuracy Potential']\n",
    "\n",
    "xgboost_scores = [4, 3, 2, 4, 5]\n",
    "lightgbm_scores = [5, 5, 3, 3, 5]\n",
    "catboost_scores = [4, 3, 5, 5, 5]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=xgboost_scores + [xgboost_scores[0]],\n",
    "    theta=categories + [categories[0]],\n",
    "    fill='toself',\n",
    "    name='XGBoost',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=lightgbm_scores + [lightgbm_scores[0]],\n",
    "    theta=categories + [categories[0]],\n",
    "    fill='toself',\n",
    "    name='LightGBM',\n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=catboost_scores + [catboost_scores[0]],\n",
    "    theta=categories + [categories[0]],\n",
    "    fill='toself',\n",
    "    name='CatBoost',\n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(radialaxis=dict(visible=True, range=[0, 5])),\n",
    "    showlegend=True,\n",
    "    title='Gradient Boosting Libraries: Relative Strengths',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Gradient Boosting for Student Departure Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is particularly well-suited for our student departure prediction problem:\n",
    "\n",
    "### Advantages for Higher Education Analytics\n",
    "\n",
    "| Advantage | Explanation |\n",
    "|:----------|:------------|\n",
    "| **Non-linear relationships** | GPA's effect on departure may not be linear (e.g., threshold effects) |\n",
    "| **Feature interactions** | Captures interactions like \"low GPA + high DFW rate\" without explicit engineering |\n",
    "| **Handles mixed data** | Works with both continuous (GPA) and categorical (major, ethnicity) features |\n",
    "| **Missing data** | Modern implementations handle missing values natively |\n",
    "| **Imbalanced classes** | Can handle departure rates of 10-30% with proper settings |\n",
    "| **Feature importance** | Provides interpretable feature importance scores |\n",
    "| **Probability calibration** | Outputs well-calibrated probabilities for ranking students |\n",
    "\n",
    "### Potential Concerns\n",
    "\n",
    "| Concern | Mitigation |\n",
    "|:--------|:-----------|\n",
    "| **Overfitting** | Use early stopping, cross-validation, regularization |\n",
    "| **Interpretability** | Use SHAP values for local explanations |\n",
    "| **Training time** | LightGBM is fast even on large datasets |\n",
    "| **Hyperparameter tuning** | Default parameters often work well; tune incrementally |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate why non-linear modeling matters\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate GPA effect on departure probability\n",
    "gpa_range = np.linspace(1.5, 4.0, 100)\n",
    "\n",
    "# Linear model assumption\n",
    "linear_prob = 1 - (gpa_range - 1.5) / 2.5 * 0.7  # Linear decrease\n",
    "\n",
    "# Reality: Threshold effect\n",
    "# Students below 2.0 GPA have very high risk, above 3.0 have low risk\n",
    "real_prob = 1 / (1 + np.exp(3 * (gpa_range - 2.3)))  # Sigmoid with threshold at 2.3\n",
    "real_prob = 0.8 * real_prob + 0.1  # Scale to 0.1-0.9 range\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=gpa_range, y=linear_prob,\n",
    "    mode='lines', name='Linear Model Assumption',\n",
    "    line=dict(color='blue', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=gpa_range, y=real_prob,\n",
    "    mode='lines', name='Reality: Threshold Effect',\n",
    "    line=dict(color='red', width=3)\n",
    "))\n",
    "\n",
    "fig.add_vline(x=2.0, line_dash=\"dot\", line_color=\"gray\",\n",
    "              annotation_text=\"Academic Probation\")\n",
    "fig.add_vline(x=3.0, line_dash=\"dot\", line_color=\"gray\",\n",
    "              annotation_text=\"Good Standing\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Why Non-linear Models Matter: GPA and Departure Risk',\n",
    "    xaxis_title='Cumulative GPA',\n",
    "    yaxis_title='Probability of Departure',\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: The relationship between GPA and departure probability likely has threshold effects. Students near academic probation (GPA < 2.0) face dramatically higher departure risk. Gradient boosting can capture this non-linearity without us having to specify it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Bagging vs. Boosting**:\n",
    "   - Bagging: Parallel training, reduces variance\n",
    "   - Boosting: Sequential training, reduces bias\n",
    "\n",
    "2. **AdaBoost**:\n",
    "   - Reweights samples to focus on hard examples\n",
    "   - Uses weak learners (decision stumps)\n",
    "   - Sensitive to outliers and noise\n",
    "\n",
    "3. **Gradient Boosting**:\n",
    "   - Fits trees to residuals (negative gradients)\n",
    "   - More flexible (any differentiable loss)\n",
    "   - Foundation for modern implementations\n",
    "\n",
    "4. **Modern Libraries**:\n",
    "   - **XGBoost**: Reliable, well-tested, regularized\n",
    "   - **LightGBM**: Fast, memory-efficient, leaf-wise growth\n",
    "   - **CatBoost**: Native categorical handling, easy setup\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Concept | Key Point |\n",
    "|:--------|:----------|\n",
    "| Boosting | Sequential ensemble that corrects errors |\n",
    "| AdaBoost | Reweights samples, uses weak learners |\n",
    "| Gradient Boosting | Fits residuals, gradient descent in function space |\n",
    "| Learning Rate | Controls contribution of each tree (0.01-0.3 typical) |\n",
    "| XGBoost | General-purpose, regularized, competition winner |\n",
    "| LightGBM | Fast, efficient, good for large data |\n",
    "| CatBoost | Best for categorical features |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will build XGBoost models for student departure prediction using scikit-learn compatible pipelines.\n",
    "\n",
    "**Proceed to:** `4.2 Build XGBoost Models`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}