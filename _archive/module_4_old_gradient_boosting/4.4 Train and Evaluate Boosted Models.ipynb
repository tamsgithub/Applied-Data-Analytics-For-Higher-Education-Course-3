{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Train and Evaluate Boosted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebooks, we built gradient boosting models using XGBoost, LightGBM, and CatBoost. Now we focus on **training best practices** and **comprehensive evaluation**. Proper training techniques like early stopping prevent overfitting, while thorough evaluation ensures our models will perform well on new student data.\n",
    "\n",
    "We also introduce **SHAP (SHapley Additive exPlanations)**, a powerful method for interpreting model predictions that provides both global feature importance and local explanations for individual students.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Implement early stopping to prevent overfitting in gradient boosting models\n",
    "2. Perform cross-validation with early stopping for robust model evaluation\n",
    "3. Evaluate models using multiple classification metrics\n",
    "4. Interpret model predictions using SHAP values\n",
    "5. Explain individual predictions to stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# SHAP for model interpretation\n",
    "import shap\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic student data\n",
    "np.random.seed(42)\n",
    "n_students = 3000\n",
    "\n",
    "data = {\n",
    "    'STUDENT_ID': range(1, n_students + 1),\n",
    "    'HS_GPA': np.random.normal(3.2, 0.5, n_students).clip(2.0, 4.0),\n",
    "    'MATH_PLACEMENT': np.random.choice(['Remedial', 'College-Ready', 'Advanced'], n_students, p=[0.2, 0.5, 0.3]),\n",
    "    'FIRST_GEN': np.random.choice(['Yes', 'No'], n_students, p=[0.35, 0.65]),\n",
    "    'PELL_ELIGIBLE': np.random.choice(['Yes', 'No'], n_students, p=[0.40, 0.60]),\n",
    "    'RESIDENCY': np.random.choice(['In-State', 'Out-of-State', 'International'], n_students, p=[0.7, 0.2, 0.1]),\n",
    "    'UNITS_ATTEMPT_1': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_1': np.random.normal(2.8, 0.7, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_1': np.random.beta(2, 8, n_students),\n",
    "    'UNITS_ATTEMPT_2': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_2': np.random.normal(2.9, 0.6, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_2': np.random.beta(2, 8, n_students),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Derived features\n",
    "df['CUM_GPA'] = (df['GPA_1'] + df['GPA_2']) / 2\n",
    "df['CUM_UNITS'] = df['UNITS_ATTEMPT_1'] + df['UNITS_ATTEMPT_2']\n",
    "df['AVG_DFW'] = (df['DFW_RATE_1'] + df['DFW_RATE_2']) / 2\n",
    "df['GPA_TREND'] = df['GPA_2'] - df['GPA_1']  # Improvement or decline\n",
    "\n",
    "# Generate target\n",
    "departure_prob = (\n",
    "    0.3 - 0.15 * (df['CUM_GPA'] - 2.5) + 0.3 * df['AVG_DFW']\n",
    "    + 0.05 * (df['FIRST_GEN'] == 'Yes') - 0.02 * (df['HS_GPA'] - 3.0)\n",
    "    + 0.05 * (df['MATH_PLACEMENT'] == 'Remedial')\n",
    "    - 0.05 * df['GPA_TREND']  # Improvement reduces risk\n",
    ")\n",
    "departure_prob = departure_prob.clip(0.05, 0.95)\n",
    "df['DEPARTED'] = np.random.binomial(1, departure_prob)\n",
    "\n",
    "# Define columns\n",
    "categorical_cols = ['MATH_PLACEMENT', 'FIRST_GEN', 'PELL_ELIGIBLE', 'RESIDENCY']\n",
    "numerical_cols = ['HS_GPA', 'UNITS_ATTEMPT_1', 'GPA_1', 'DFW_RATE_1', \n",
    "                  'UNITS_ATTEMPT_2', 'GPA_2', 'DFW_RATE_2', \n",
    "                  'CUM_GPA', 'CUM_UNITS', 'AVG_DFW', 'GPA_TREND']\n",
    "\n",
    "feature_cols = categorical_cols + numerical_cols\n",
    "\n",
    "print(f\"Dataset: {n_students} students\")\n",
    "print(f\"Departure rate: {df['DEPARTED'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Validation Set\n",
    "\n",
    "For early stopping, we need a separate validation set to monitor performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 60% train, 20% validation, 20% test\n",
    "X = df[feature_cols]\n",
    "y = df['DEPARTED']\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% val (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {len(X_train)} students ({len(X_train)/len(X):.0%})\")\n",
    "print(f\"Validation set: {len(X_val)} students ({len(X_val)/len(X):.0%})\")\n",
    "print(f\"Test set:       {len(X_test)} students ({len(X_test)/len(X):.0%})\")\n",
    "print(f\"\\nDeparture rates:\")\n",
    "print(f\"  Train: {y_train.mean():.1%}\")\n",
    "print(f\"  Val:   {y_val.mean():.1%}\")\n",
    "print(f\"  Test:  {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare encoded versions for XGBoost and LightGBM\n",
    "\n",
    "# One-hot encode for XGBoost\n",
    "X_train_xgb = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_val_xgb = pd.get_dummies(X_val, columns=categorical_cols, drop_first=True)\n",
    "X_test_xgb = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure consistent columns\n",
    "X_val_xgb = X_val_xgb.reindex(columns=X_train_xgb.columns, fill_value=0)\n",
    "X_test_xgb = X_test_xgb.reindex(columns=X_train_xgb.columns, fill_value=0)\n",
    "\n",
    "# Label encode for LightGBM\n",
    "X_train_lgb = X_train.copy()\n",
    "X_val_lgb = X_val.copy()\n",
    "X_test_lgb = X_test.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_lgb[col] = le.fit_transform(X_train_lgb[col])\n",
    "    X_val_lgb[col] = le.transform(X_val_lgb[col])\n",
    "    X_test_lgb[col] = le.transform(X_test_lgb[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Data prepared for all three libraries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Why Early Stopping?\n",
    "\n",
    "**Early stopping** monitors performance on a validation set during training and stops when performance stops improving. This prevents overfitting and saves training time.\n",
    "\n",
    "Without early stopping:\n",
    "- Model may train for more iterations than necessary\n",
    "- Training performance keeps improving\n",
    "- Validation performance peaks then degrades (overfitting)\n",
    "\n",
    "With early stopping:\n",
    "- Training stops when validation performance stops improving\n",
    "- Automatically finds optimal number of iterations\n",
    "- Acts as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting without early stopping\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate training and validation loss over iterations\n",
    "n_iters = 200\n",
    "iterations = np.arange(1, n_iters + 1)\n",
    "\n",
    "# Training loss decreases continuously\n",
    "train_loss = 0.7 * np.exp(-0.02 * iterations) + 0.05 + np.random.normal(0, 0.01, n_iters)\n",
    "\n",
    "# Validation loss decreases then increases (overfitting)\n",
    "val_loss = 0.7 * np.exp(-0.02 * iterations) + 0.1 + 0.0005 * (iterations - 80) ** 2 / 100\n",
    "val_loss[:80] = 0.7 * np.exp(-0.02 * iterations[:80]) + 0.1\n",
    "val_loss = val_loss + np.random.normal(0, 0.015, n_iters)\n",
    "\n",
    "# Find best iteration\n",
    "best_iter = np.argmin(val_loss) + 1\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=iterations, y=train_loss,\n",
    "    mode='lines', name='Training Loss',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=iterations, y=val_loss,\n",
    "    mode='lines', name='Validation Loss',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.add_vline(x=best_iter, line_dash=\"dash\", line_color=\"green\",\n",
    "              annotation_text=f\"Best: {best_iter}\")\n",
    "\n",
    "# Highlight overfitting region\n",
    "fig.add_vrect(x0=best_iter, x1=n_iters, fillcolor=\"red\", opacity=0.1,\n",
    "              annotation_text=\"Overfitting\", annotation_position=\"top right\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Early Stopping: Preventing Overfitting',\n",
    "    xaxis_title='Boosting Iterations',\n",
    "    yaxis_title='Loss',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Optimal stopping point: {best_iter} iterations\")\n",
    "print(f\"Without early stopping: {n_iters} iterations (wasted {n_iters - best_iter} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Early Stopping in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with early stopping\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,          # Set high, early stopping will find optimal\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=20,  # Stop if no improvement for 20 rounds\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train with validation set\n",
    "xgb_model.fit(\n",
    "    X_train_xgb, y_train,\n",
    "    eval_set=[(X_train_xgb, y_train), (X_val_xgb, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"XGBoost Training with Early Stopping\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Requested iterations: 500\")\n",
    "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"Best validation score: {xgb_model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "results = xgb_model.evals_result()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=results['validation_0']['logloss'],\n",
    "    mode='lines', name='Training',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=results['validation_1']['logloss'],\n",
    "    mode='lines', name='Validation',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.add_vline(x=xgb_model.best_iteration, line_dash=\"dash\", line_color=\"green\",\n",
    "              annotation_text=f\"Best: {xgb_model.best_iteration}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='XGBoost Training History with Early Stopping',\n",
    "    xaxis_title='Iteration',\n",
    "    yaxis_title='Log Loss',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Early Stopping in LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with early stopping\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train with callbacks for early stopping\n",
    "lgb_model.fit(\n",
    "    X_train_lgb, y_train,\n",
    "    eval_set=[(X_val_lgb, y_val)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20, verbose=False),\n",
    "        lgb.log_evaluation(period=0)  # Suppress logging\n",
    "    ],\n",
    "    categorical_feature=categorical_cols\n",
    ")\n",
    "\n",
    "print(\"LightGBM Training with Early Stopping\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Requested iterations: 500\")\n",
    "print(f\"Best iteration: {lgb_model.best_iteration_}\")\n",
    "print(f\"Best validation score: {lgb_model.best_score_['valid_0']['binary_logloss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Early Stopping in CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost with early stopping (built-in)\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    cat_features=categorical_cols,\n",
    "    early_stopping_rounds=20,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train with validation set\n",
    "cat_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "print(\"CatBoost Training with Early Stopping\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Requested iterations: 500\")\n",
    "print(f\"Best iteration: {cat_model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best iterations across models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Requested': [500, 500, 500],\n",
    "    'Best Iteration': [xgb_model.best_iteration, lgb_model.best_iteration_, cat_model.best_iteration_],\n",
    "})\n",
    "comparison['Savings'] = comparison['Requested'] - comparison['Best Iteration']\n",
    "comparison['Savings %'] = (comparison['Savings'] / comparison['Requested'] * 100).round(1)\n",
    "\n",
    "print(\"Early Stopping Comparison\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stratified K-Fold Cross-Validation\n",
    "\n",
    "For imbalanced classes like student departure, **stratified** K-fold ensures each fold has the same class distribution as the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified 5-Fold CV on the full training+validation set\n",
    "X_full = pd.concat([X_train, X_val])\n",
    "y_full = pd.concat([y_train, y_val])\n",
    "\n",
    "# Prepare for CatBoost (easiest to use)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate CatBoost\n",
    "cv_scores = cross_validate(\n",
    "    CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        cat_features=categorical_cols,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "    X_full, y_full,\n",
    "    cv=cv,\n",
    "    scoring=['accuracy', 'roc_auc', 'f1', 'precision', 'recall'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"5-Fold Stratified Cross-Validation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<15} {'Train Mean':>12} {'Test Mean':>12} {'Test Std':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['accuracy', 'roc_auc', 'f1', 'precision', 'recall']:\n",
    "    train_scores = cv_scores[f'train_{metric}']\n",
    "    test_scores = cv_scores[f'test_{metric}']\n",
    "    print(f\"{metric:<15} {train_scores.mean():>12.3f} {test_scores.mean():>12.3f} {test_scores.std():>12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Validation with Early Stopping\n",
    "\n",
    "For production models, combine cross-validation with early stopping to get both robust estimates and optimal iteration counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual CV with early stopping\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "best_iterations = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_full, y_full)):\n",
    "    X_cv_train = X_full.iloc[train_idx]\n",
    "    X_cv_val = X_full.iloc[val_idx]\n",
    "    y_cv_train = y_full.iloc[train_idx]\n",
    "    y_cv_val = y_full.iloc[val_idx]\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        cat_features=categorical_cols,\n",
    "        early_stopping_rounds=20,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_cv_train, y_cv_train,\n",
    "        eval_set=(X_cv_val, y_cv_val),\n",
    "        use_best_model=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_proba = model.predict_proba(X_cv_val)[:, 1]\n",
    "    y_pred = model.predict(X_cv_val)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Best Iteration': model.best_iteration_,\n",
    "        'ROC-AUC': roc_auc_score(y_cv_val, y_pred_proba),\n",
    "        'Accuracy': accuracy_score(y_cv_val, y_pred),\n",
    "        'F1': f1_score(y_cv_val, y_pred)\n",
    "    })\n",
    "    best_iterations.append(model.best_iteration_)\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "print(\"Cross-Validation with Early Stopping\")\n",
    "print(\"=\" * 60)\n",
    "print(cv_results_df.round(4))\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Mean Best Iteration: {np.mean(best_iterations):.0f} (std: {np.std(best_iterations):.0f})\")\n",
    "print(f\"  Mean ROC-AUC: {cv_results_df['ROC-AUC'].mean():.4f} (std: {cv_results_df['ROC-AUC'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set using CatBoost\n",
    "# Train on full train+val with optimal iterations\n",
    "optimal_iters = int(np.mean(best_iterations))\n",
    "\n",
    "final_model = CatBoostClassifier(\n",
    "    iterations=optimal_iters,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    cat_features=categorical_cols,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Final Model Performance on Test Set\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training iterations: {optimal_iters}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Departed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Retained', 'Predicted Departed'],\n",
    "    y=['Actual Retained', 'Actual Departed'],\n",
    "    text=[[f'{cm[i,j]}<br>({cm[i,j]/cm.sum()*100:.1f}%)' for j in range(2)] for i in range(2)],\n",
    "    texttemplate='%{text}',\n",
    "    textfont=dict(size=14),\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix on Test Set',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate curves\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    f'ROC Curve (AUC = {roc_auc:.3f})',\n",
    "    f'Precision-Recall Curve (AP = {avg_precision:.3f})'\n",
    "))\n",
    "\n",
    "# ROC Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr, mode='lines',\n",
    "    name='ROC', line=dict(color='blue', width=2)\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1], mode='lines',\n",
    "    name='Random', line=dict(color='gray', dash='dash')\n",
    "), row=1, col=1)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=recall, y=precision, mode='lines',\n",
    "    name='PR', line=dict(color='green', width=2)\n",
    "), row=1, col=2)\n",
    "\n",
    "# Baseline (random classifier)\n",
    "baseline = y_test.mean()\n",
    "fig.add_hline(y=baseline, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='False Positive Rate', row=1, col=1)\n",
    "fig.update_yaxes(title_text='True Positive Rate', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Recall', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Precision', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False,\n",
    "                  title_text='Model Discrimination Performance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calibration Curves\n",
    "\n",
    "A **calibration curve** shows whether predicted probabilities match actual outcomes. Well-calibrated models are essential for risk scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "# Calculate Brier score (lower is better)\n",
    "brier = brier_score_loss(y_test, y_pred_proba)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Perfect calibration line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1], mode='lines',\n",
    "    name='Perfect Calibration',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "# Model calibration\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_pred, y=prob_true, mode='lines+markers',\n",
    "    name='Model Calibration',\n",
    "    line=dict(color='blue', width=2),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Calibration Curve (Brier Score = {brier:.4f})',\n",
    "    xaxis_title='Mean Predicted Probability',\n",
    "    yaxis_title='Fraction of Positives',\n",
    "    height=450,\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Calibration Interpretation:\")\n",
    "print(\"- Perfect calibration: points on diagonal\")\n",
    "print(\"- Above diagonal: model underestimates probability\")\n",
    "print(\"- Below diagonal: model overestimates probability\")\n",
    "print(f\"\\nBrier Score: {brier:.4f} (0 = perfect, 0.25 = random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP for Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Introduction to SHAP\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** uses game theory to explain individual predictions:\n",
    "\n",
    "- Each feature's contribution to a prediction is its \"Shapley value\"\n",
    "- Shapley values sum to the difference between the prediction and the baseline\n",
    "- Provides both **global** (feature importance) and **local** (individual) explanations\n",
    "\n",
    "SHAP is particularly valuable in higher education because it helps explain **why** a student is flagged as at-risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "# CatBoost has native SHAP support\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "\n",
    "# Calculate SHAP values for test set\n",
    "# Use a sample for faster computation\n",
    "sample_size = min(500, len(X_test))\n",
    "X_sample = X_test.head(sample_size)\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values calculated for {sample_size} students\")\n",
    "print(f\"Shape of SHAP values: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SHAP Summary Plot\n",
    "\n",
    "The summary plot shows global feature importance and how features affect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute SHAP values for global importance\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Mean |SHAP|': mean_abs_shap\n",
    "}).sort_values('Mean |SHAP|', ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig = px.bar(\n",
    "    feature_importance.tail(15),\n",
    "    x='Mean |SHAP|', y='Feature',\n",
    "    orientation='h',\n",
    "    title='SHAP Feature Importance (Mean Absolute SHAP Value)',\n",
    "    color='Mean |SHAP|',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, yaxis_title='', xaxis_title='Mean |SHAP Value|')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beeswarm-style SHAP plot using Plotly\n",
    "# Get top 10 features\n",
    "top_features = feature_importance.tail(10)['Feature'].tolist()\n",
    "\n",
    "# Prepare data for visualization\n",
    "plot_data = []\n",
    "for i, feat in enumerate(top_features):\n",
    "    feat_idx = feature_cols.index(feat)\n",
    "    feat_shap = shap_values[:, feat_idx]\n",
    "    feat_values = X_sample[feat].values\n",
    "    \n",
    "    # Normalize feature values for coloring\n",
    "    if X_sample[feat].dtype in ['object', 'category']:\n",
    "        feat_norm = pd.factorize(feat_values)[0]\n",
    "    else:\n",
    "        feat_norm = (feat_values - feat_values.min()) / (feat_values.max() - feat_values.min() + 1e-10)\n",
    "    \n",
    "    for j in range(len(feat_shap)):\n",
    "        plot_data.append({\n",
    "            'Feature': feat,\n",
    "            'SHAP Value': feat_shap[j],\n",
    "            'Feature Value (normalized)': feat_norm[j],\n",
    "            'Feature Index': i + np.random.normal(0, 0.1)  # Add jitter\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df, x='SHAP Value', y='Feature',\n",
    "    color='Feature Value (normalized)',\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    title='SHAP Summary Plot: Feature Impact on Departure Prediction',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(height=500, xaxis_title='SHAP Value (impact on departure probability)')\n",
    "fig.update_coloraxes(colorbar_title='Feature<br>Value')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Each dot is one student\")\n",
    "print(\"- Red = high feature value, Blue = low feature value\")\n",
    "print(\"- Right of center = increases departure probability\")\n",
    "print(\"- Left of center = decreases departure probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 SHAP Dependence Plots\n",
    "\n",
    "Dependence plots show how a feature's value affects its SHAP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for CUM_GPA\n",
    "feat = 'CUM_GPA'\n",
    "feat_idx = feature_cols.index(feat)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_sample[feat],\n",
    "    y=shap_values[:, feat_idx],\n",
    "    color=X_sample['AVG_DFW'],  # Color by interacting feature\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    title=f'SHAP Dependence Plot: {feat}',\n",
    "    labels={'x': feat, 'y': 'SHAP Value', 'color': 'AVG_DFW'},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(height=450)\n",
    "fig.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Higher GPA generally has negative SHAP values (reduces departure risk)\")\n",
    "print(\"- Color shows interaction: students with high DFW rates (red) have different patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple dependence plots\n",
    "features_to_plot = ['CUM_GPA', 'AVG_DFW', 'HS_GPA', 'GPA_TREND']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=features_to_plot)\n",
    "\n",
    "for i, feat in enumerate(features_to_plot):\n",
    "    row = i // 2 + 1\n",
    "    col = i % 2 + 1\n",
    "    feat_idx = feature_cols.index(feat)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_sample[feat],\n",
    "        y=shap_values[:, feat_idx],\n",
    "        mode='markers',\n",
    "        marker=dict(color='blue', size=5, opacity=0.5),\n",
    "        name=feat,\n",
    "        showlegend=False\n",
    "    ), row=row, col=col)\n",
    "    \n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=600, title_text='SHAP Dependence Plots for Key Features')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Individual Prediction Explanations\n",
    "\n",
    "SHAP allows us to explain why a specific student was predicted to be at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a high-risk student to explain\n",
    "high_risk_idx = y_pred_proba.argmax()\n",
    "high_risk_student = X_test.iloc[[high_risk_idx]]\n",
    "high_risk_prob = y_pred_proba[high_risk_idx]\n",
    "\n",
    "# Calculate SHAP values for this student\n",
    "individual_shap = explainer.shap_values(high_risk_student)[0]\n",
    "\n",
    "print(f\"High-Risk Student Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Predicted Departure Probability: {high_risk_prob:.1%}\")\n",
    "print(f\"Actual Outcome: {'Departed' if y_test.iloc[high_risk_idx] == 1 else 'Retained'}\")\n",
    "print(\"\\nStudent Profile:\")\n",
    "for col in feature_cols:\n",
    "    print(f\"  {col}: {high_risk_student[col].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for individual explanation\n",
    "base_value = explainer.expected_value\n",
    "\n",
    "# Sort features by absolute SHAP value\n",
    "shap_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Value': [high_risk_student[f].values[0] for f in feature_cols],\n",
    "    'SHAP': individual_shap\n",
    "}).sort_values('SHAP', key=abs, ascending=True)\n",
    "\n",
    "# Take top 10 features\n",
    "top_shap = shap_df.tail(10)\n",
    "\n",
    "# Create waterfall-style plot\n",
    "colors = ['red' if x > 0 else 'blue' for x in top_shap['SHAP']]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=[f\"{row['Feature']} = {row['Value']}\" for _, row in top_shap.iterrows()],\n",
    "    x=top_shap['SHAP'],\n",
    "    orientation='h',\n",
    "    marker_color=colors\n",
    "))\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"solid\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'SHAP Explanation for High-Risk Student (P(Departed) = {high_risk_prob:.1%})',\n",
    "    xaxis_title='SHAP Value (impact on log-odds)',\n",
    "    yaxis_title='',\n",
    "    height=450,\n",
    "    annotations=[\n",
    "        dict(x=0.95, y=0.05, xref='paper', yref='paper',\n",
    "             text=f'Base rate: {base_value:.2f}', showarrow=False,\n",
    "             font=dict(size=10))\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Red bars push prediction toward 'Departed'\")\n",
    "print(\"- Blue bars push prediction toward 'Retained'\")\n",
    "print(\"- Length indicates strength of impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare high-risk vs low-risk student\n",
    "low_risk_idx = y_pred_proba.argmin()\n",
    "low_risk_student = X_test.iloc[[low_risk_idx]]\n",
    "low_risk_prob = y_pred_proba[low_risk_idx]\n",
    "low_risk_shap = explainer.shap_values(low_risk_student)[0]\n",
    "\n",
    "# Create comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'High-Risk Value': [high_risk_student[f].values[0] for f in feature_cols],\n",
    "    'High-Risk SHAP': individual_shap,\n",
    "    'Low-Risk Value': [low_risk_student[f].values[0] for f in feature_cols],\n",
    "    'Low-Risk SHAP': low_risk_shap\n",
    "})\n",
    "\n",
    "print(f\"Student Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"High-Risk Student: P(Departed) = {high_risk_prob:.1%}\")\n",
    "print(f\"Low-Risk Student:  P(Departed) = {low_risk_prob:.1%}\")\n",
    "print(\"\\nKey Feature Differences:\")\n",
    "comparison[['Feature', 'High-Risk Value', 'Low-Risk Value']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Early Stopping**:\n",
    "   - Prevents overfitting by monitoring validation performance\n",
    "   - Automatically finds optimal number of iterations\n",
    "   - Supported natively in XGBoost, LightGBM, and CatBoost\n",
    "\n",
    "2. **Cross-Validation**:\n",
    "   - Use stratified K-fold for imbalanced classes\n",
    "   - Combine with early stopping for robust estimates\n",
    "   - Average best iterations across folds for final model\n",
    "\n",
    "3. **Comprehensive Evaluation**:\n",
    "   - Multiple metrics: accuracy, precision, recall, F1, ROC-AUC\n",
    "   - ROC and Precision-Recall curves for discrimination\n",
    "   - Calibration curves for probability reliability\n",
    "\n",
    "4. **SHAP Interpretation**:\n",
    "   - Global importance via mean absolute SHAP values\n",
    "   - Dependence plots for feature effects\n",
    "   - Individual explanations for stakeholder communication\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|:------|:-------------|\n",
    "| Early Stopping | Set high n_estimators, let early stopping find optimal |\n",
    "| Validation Split | Use 20-25% for early stopping |\n",
    "| Cross-Validation | 5-fold stratified is standard |\n",
    "| Calibration | Check Brier score and calibration curve |\n",
    "| SHAP | Best method for explaining individual predictions |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will focus on **hyperparameter tuning** to optimize model performance.\n",
    "\n",
    "**Proceed to:** `4.5 Tune Boosted Model Hyperparameters`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}