{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Build LightGBM and CatBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we built XGBoost models for student departure prediction. Now we explore two powerful alternatives: **LightGBM** and **CatBoost**. Each library has unique strengths:\n",
    "\n",
    "- **LightGBM**: Optimized for speed and memory efficiency, excellent for large datasets\n",
    "- **CatBoost**: Specialized handling of categorical features, robust to overfitting\n",
    "\n",
    "Understanding when to use each library will help you choose the best tool for your specific higher education analytics problems.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build LightGBM classification models with native categorical support\n",
    "2. Build CatBoost classification models with automatic categorical handling\n",
    "3. Configure key hyperparameters for each library\n",
    "4. Compare performance, training time, and ease of use across all three libraries\n",
    "5. Make informed decisions about which library to use for different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic student data (same as previous notebook)\n",
    "np.random.seed(42)\n",
    "n_students = 2000\n",
    "\n",
    "data = {\n",
    "    'STUDENT_ID': range(1, n_students + 1),\n",
    "    'HS_GPA': np.random.normal(3.2, 0.5, n_students).clip(2.0, 4.0),\n",
    "    'MATH_PLACEMENT': np.random.choice(['Remedial', 'College-Ready', 'Advanced'], n_students, p=[0.2, 0.5, 0.3]),\n",
    "    'FIRST_GEN': np.random.choice(['Yes', 'No'], n_students, p=[0.35, 0.65]),\n",
    "    'PELL_ELIGIBLE': np.random.choice(['Yes', 'No'], n_students, p=[0.40, 0.60]),\n",
    "    'RESIDENCY': np.random.choice(['In-State', 'Out-of-State', 'International'], n_students, p=[0.7, 0.2, 0.1]),\n",
    "    'MAJOR_CATEGORY': np.random.choice(['STEM', 'Business', 'Humanities', 'Social Science', 'Undeclared'], \n",
    "                                        n_students, p=[0.25, 0.20, 0.15, 0.20, 0.20]),\n",
    "    'UNITS_ATTEMPT_1': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_1': np.random.normal(2.8, 0.7, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_1': np.random.beta(2, 8, n_students),\n",
    "    'UNITS_ATTEMPT_2': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_2': np.random.normal(2.9, 0.6, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_2': np.random.beta(2, 8, n_students),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate derived features\n",
    "df['CUM_GPA'] = (df['GPA_1'] + df['GPA_2']) / 2\n",
    "df['CUM_UNITS'] = df['UNITS_ATTEMPT_1'] + df['UNITS_ATTEMPT_2']\n",
    "df['AVG_DFW'] = (df['DFW_RATE_1'] + df['DFW_RATE_2']) / 2\n",
    "\n",
    "# Generate target variable\n",
    "departure_prob = (\n",
    "    0.3 - 0.15 * (df['CUM_GPA'] - 2.5) + 0.3 * df['AVG_DFW']\n",
    "    + 0.05 * (df['FIRST_GEN'] == 'Yes') - 0.02 * (df['HS_GPA'] - 3.0)\n",
    "    + 0.05 * (df['MATH_PLACEMENT'] == 'Remedial')\n",
    "    + 0.03 * (df['MAJOR_CATEGORY'] == 'Undeclared')\n",
    ")\n",
    "departure_prob = departure_prob.clip(0.05, 0.95)\n",
    "df['DEPARTED'] = np.random.binomial(1, departure_prob)\n",
    "\n",
    "# Define feature columns\n",
    "categorical_cols = ['MATH_PLACEMENT', 'FIRST_GEN', 'PELL_ELIGIBLE', 'RESIDENCY', 'MAJOR_CATEGORY']\n",
    "numerical_cols = ['HS_GPA', 'UNITS_ATTEMPT_1', 'GPA_1', 'DFW_RATE_1', \n",
    "                  'UNITS_ATTEMPT_2', 'GPA_2', 'DFW_RATE_2', \n",
    "                  'CUM_GPA', 'CUM_UNITS', 'AVG_DFW']\n",
    "\n",
    "feature_cols = categorical_cols + numerical_cols\n",
    "target_col = 'DEPARTED'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Departure rate: {df['DEPARTED'].mean():.1%}\")\n",
    "print(f\"\\nCategorical features: {categorical_cols}\")\n",
    "print(f\"Numerical features: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LightGBM Key Features\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) was developed by Microsoft with a focus on efficiency:\n",
    "\n",
    "| Feature | Description |\n",
    "|:--------|:------------|\n",
    "| **Leaf-wise Growth** | Grows trees by choosing the leaf with maximum delta loss (vs. level-wise) |\n",
    "| **GOSS** | Gradient-based One-Side Sampling: keeps large gradients, samples small ones |\n",
    "| **EFB** | Exclusive Feature Bundling: bundles mutually exclusive features |\n",
    "| **Histogram-based** | Bins continuous features into discrete buckets for faster splits |\n",
    "| **Native Categorical** | Can handle categorical features directly (integer-encoded) |\n",
    "| **Memory Efficient** | Lower memory footprint than XGBoost |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize leaf-wise vs level-wise tree growth\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Level-wise Growth (XGBoost default)',\n",
    "    'Leaf-wise Growth (LightGBM)'\n",
    "))\n",
    "\n",
    "# Level-wise tree (grows all leaves at each level)\n",
    "level_wise_nodes = [\n",
    "    (4, 5, 'Root', 'lightblue'),\n",
    "    (2, 3.5, 'L1', 'lightgreen'), (6, 3.5, 'L2', 'lightgreen'),\n",
    "    (1, 2, 'L3', 'lightyellow'), (3, 2, 'L4', 'lightyellow'),\n",
    "    (5, 2, 'L5', 'lightyellow'), (7, 2, 'L6', 'lightyellow')\n",
    "]\n",
    "\n",
    "# Leaf-wise tree (grows leaf with max gain)\n",
    "leaf_wise_nodes = [\n",
    "    (4, 5, 'Root', 'lightblue'),\n",
    "    (2, 3.5, 'L1', 'lightgreen'), (6, 3.5, 'L2', 'lightcoral'),  # L2 has lower gain\n",
    "    (1, 2, 'L3', 'lightyellow'), (3, 2, 'L4', 'lightyellow'),  # Continue growing L1's children\n",
    "    (0.5, 0.5, 'L5', 'lightpink'), (1.5, 0.5, 'L6', 'lightpink')  # Grow highest gain leaf\n",
    "]\n",
    "\n",
    "# Draw level-wise\n",
    "for x, y, label, color in level_wise_nodes:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x], y=[y], mode='markers+text', text=[label],\n",
    "        textposition='middle center',\n",
    "        marker=dict(size=40, color=color, line=dict(width=2, color='darkgray')),\n",
    "        showlegend=False\n",
    "    ), row=1, col=1)\n",
    "\n",
    "# Draw leaf-wise\n",
    "for x, y, label, color in leaf_wise_nodes:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x], y=[y], mode='markers+text', text=[label],\n",
    "        textposition='middle center',\n",
    "        marker=dict(size=40, color=color, line=dict(width=2, color='darkgray')),\n",
    "        showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "# Add edges for level-wise\n",
    "edges_lw = [((4, 5), (2, 3.5)), ((4, 5), (6, 3.5)), \n",
    "            ((2, 3.5), (1, 2)), ((2, 3.5), (3, 2)),\n",
    "            ((6, 3.5), (5, 2)), ((6, 3.5), (7, 2))]\n",
    "\n",
    "for (x1, y1), (x2, y2) in edges_lw:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x1, x2], y=[y1-0.3, y2+0.3], mode='lines',\n",
    "        line=dict(color='gray', width=2), showlegend=False\n",
    "    ), row=1, col=1)\n",
    "\n",
    "# Add edges for leaf-wise\n",
    "edges_leaf = [((4, 5), (2, 3.5)), ((4, 5), (6, 3.5)),\n",
    "              ((2, 3.5), (1, 2)), ((2, 3.5), (3, 2)),\n",
    "              ((1, 2), (0.5, 0.5)), ((1, 2), (1.5, 0.5))]\n",
    "\n",
    "for (x1, y1), (x2, y2) in edges_leaf:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x1, x2], y=[y1-0.3, y2+0.3], mode='lines',\n",
    "        line=dict(color='gray', width=2), showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(visible=False)\n",
    "fig.update_yaxes(visible=False)\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title='Tree Growth Strategies: Level-wise vs Leaf-wise',\n",
    "    annotations=[\n",
    "        dict(x=0.25, y=-0.1, xref='paper', yref='paper', text='Grows all leaves at each level',\n",
    "             showarrow=False, font=dict(size=11)),\n",
    "        dict(x=0.75, y=-0.1, xref='paper', yref='paper', text='Grows leaf with highest gain',\n",
    "             showarrow=False, font=dict(size=11))\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Leaf-wise growth can achieve lower loss with fewer splits, but may overfit on small datasets. Use `num_leaves` to control complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, encode categorical features for LightGBM\n",
    "# LightGBM requires integer-encoded categoricals or one-hot encoding\n",
    "\n",
    "# Create copies for encoding\n",
    "X_train_lgb = X_train.copy()\n",
    "X_test_lgb = X_test.copy()\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_lgb[col] = le.fit_transform(X_train_lgb[col])\n",
    "    X_test_lgb[col] = le.transform(X_test_lgb[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Encoded categorical features for LightGBM:\")\n",
    "X_train_lgb[categorical_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LightGBM classifier\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=-1,          # -1 means no limit (leaf-wise uses num_leaves instead)\n",
    "    num_leaves=31,         # Controls complexity (default=31)\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,         # Called 'bagging_fraction' in LightGBM native API\n",
    "    colsample_bytree=0.8,  # Called 'feature_fraction' in native API\n",
    "    random_state=42,\n",
    "    verbose=-1             # Suppress warnings\n",
    ")\n",
    "\n",
    "# Train with categorical feature specification\n",
    "lgb_model.fit(\n",
    "    X_train_lgb, y_train,\n",
    "    categorical_feature=categorical_cols  # Tell LightGBM which columns are categorical\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_lgb = lgb_model.predict(X_test_lgb)\n",
    "y_pred_proba_lgb = lgb_model.predict_proba(X_test_lgb)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"LightGBM Model Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgb):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lgb):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_lgb):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_lgb):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_lgb):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LightGBM with Native Categorical Support\n",
    "\n",
    "LightGBM can handle categorical features natively without one-hot encoding, which is more memory efficient for high-cardinality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key LightGBM parameters\n",
    "lgb_params = {\n",
    "    'Parameter': [\n",
    "        'n_estimators',\n",
    "        'num_leaves',\n",
    "        'max_depth',\n",
    "        'learning_rate',\n",
    "        'min_child_samples',\n",
    "        'subsample (bagging_fraction)',\n",
    "        'colsample_bytree (feature_fraction)',\n",
    "        'reg_alpha',\n",
    "        'reg_lambda',\n",
    "        'is_unbalance / scale_pos_weight'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Number of boosting iterations',\n",
    "        'Maximum leaves per tree (key for leaf-wise)',\n",
    "        'Maximum tree depth (-1 for unlimited)',\n",
    "        'Shrinkage rate',\n",
    "        'Minimum samples in a leaf',\n",
    "        'Fraction of rows for each tree',\n",
    "        'Fraction of features for each tree',\n",
    "        'L1 regularization',\n",
    "        'L2 regularization',\n",
    "        'Handle class imbalance'\n",
    "    ],\n",
    "    'Default': ['100', '31', '-1', '0.1', '20', '1.0', '1.0', '0', '0', 'False / 1.0']\n",
    "}\n",
    "\n",
    "pd.DataFrame(lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 LightGBM in Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with LightGBM\n",
    "# Note: For categorical features to work in pipeline, we need to encode them\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Use OrdinalEncoder for categoricals (LightGBM-friendly)\n",
    "preprocessor_lgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "lgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_lgb),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "lgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_lgb_pipe = lgb_pipeline.predict(X_test)\n",
    "y_pred_proba_lgb_pipe = lgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"LightGBM Pipeline Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgb_pipe):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_lgb_pipe):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CatBoost Key Features\n",
    "\n",
    "CatBoost (Categorical Boosting) was developed by Yandex with a focus on categorical features:\n",
    "\n",
    "| Feature | Description |\n",
    "|:--------|:------------|\n",
    "| **Native Categorical** | Handles string/object categorical features directly (no encoding needed!) |\n",
    "| **Ordered Boosting** | Prevents target leakage by using ordered target statistics |\n",
    "| **Symmetric Trees** | Balanced trees that are faster to evaluate |\n",
    "| **GPU Training** | Excellent GPU support out of the box |\n",
    "| **Robust to Overfitting** | Ordered boosting reduces prediction shift |\n",
    "| **Feature Combinations** | Automatically creates combinations of categorical features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the target leakage problem that CatBoost solves\n",
    "print(\"The Target Leakage Problem with Categorical Encoding\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Traditional Target Encoding:\")\n",
    "print(\"  - Calculate mean target for each category using ALL training data\")\n",
    "print(\"  - Problem: Sample's own target value influences its encoding\")\n",
    "print(\"  - Results in optimistic training performance, poor generalization\")\n",
    "print()\n",
    "print(\"CatBoost's Ordered Boosting:\")\n",
    "print(\"  - For each sample, only use target values from PREVIOUS samples\")\n",
    "print(\"  - Like time-series: can't look into the future\")\n",
    "print(\"  - Eliminates target leakage, better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building a CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost can use the original data directly!\n",
    "# No need to encode categorical features\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100,           # Same as n_estimators\n",
    "    depth=6,                  # Maximum tree depth\n",
    "    learning_rate=0.1,\n",
    "    cat_features=categorical_cols,  # Specify categorical columns\n",
    "    random_state=42,\n",
    "    verbose=0                 # Suppress training output\n",
    ")\n",
    "\n",
    "# Train directly on original data (strings are OK!)\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "y_pred_proba_cat = cat_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"CatBoost Model Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_cat):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_cat):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_cat):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_cat):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_cat):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CatBoost with Native Categorical Handling\n",
    "\n",
    "CatBoost's main advantage is seamless categorical handling. Let's explore this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost key parameters\n",
    "cat_params = {\n",
    "    'Parameter': [\n",
    "        'iterations',\n",
    "        'depth',\n",
    "        'learning_rate',\n",
    "        'l2_leaf_reg',\n",
    "        'cat_features',\n",
    "        'one_hot_max_size',\n",
    "        'auto_class_weights',\n",
    "        'random_strength',\n",
    "        'bagging_temperature',\n",
    "        'early_stopping_rounds'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Number of boosting iterations (trees)',\n",
    "        'Depth of each tree (symmetric)',\n",
    "        'Shrinkage rate',\n",
    "        'L2 regularization on leaves',\n",
    "        'List of categorical feature names/indices',\n",
    "        'Max categories for one-hot (otherwise ordered encoding)',\n",
    "        'Auto handle class imbalance (None/Balanced/SqrtBalanced)',\n",
    "        'Amount of randomness for scoring splits',\n",
    "        'Controls intensity of Bayesian bootstrap (0=no bootstrap)',\n",
    "        'Stop if no improvement for N rounds'\n",
    "    ],\n",
    "    'Default': ['1000', '6', '0.03', '3.0', 'None', '2', 'None', '1', '1', 'False']\n",
    "}\n",
    "\n",
    "pd.DataFrame(cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CatBoost's handling of high-cardinality categorical features\n",
    "print(\"CatBoost Categorical Feature Handling\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"For each categorical feature, CatBoost automatically:\")\n",
    "print()\n",
    "print(\"1. If categories <= one_hot_max_size (default=2):\")\n",
    "print(\"   -> Uses one-hot encoding\")\n",
    "print()\n",
    "print(\"2. If categories > one_hot_max_size:\")\n",
    "print(\"   -> Uses ordered target encoding\")\n",
    "print(\"   -> Prevents target leakage via random permutations\")\n",
    "print()\n",
    "print(\"3. Optionally creates feature combinations:\")\n",
    "print(\"   -> E.g., MAJOR + FIRST_GEN combinations\")\n",
    "print(\"   -> Captures interaction effects automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 CatBoost in Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost works well in pipelines, but we need to handle categorical features carefully\n",
    "# Option 1: Pass cat_features directly (if using CatBoost outside pipeline)\n",
    "# Option 2: Use ColumnTransformer with passthrough\n",
    "\n",
    "# Simple approach: Scale numericals, pass categoricals through\n",
    "preprocessor_cat = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', 'passthrough', categorical_cols)  # Pass through as-is\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the indices of categorical features after transformation\n",
    "n_numerical = len(numerical_cols)\n",
    "cat_feature_indices = list(range(n_numerical, n_numerical + len(categorical_cols)))\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('classifier', CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        cat_features=cat_feature_indices,  # Use indices after preprocessing\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "cat_pipeline.fit(X_train, y_train)\n",
    "y_pred_cat_pipe = cat_pipeline.predict(X_test)\n",
    "y_pred_proba_cat_pipe = cat_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"CatBoost Pipeline Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_cat_pipe):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_cat_pipe):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing XGBoost, LightGBM, and CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three models with similar settings\n",
    "\n",
    "# Prepare data for XGBoost (one-hot encoded)\n",
    "X_train_xgb = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test_xgb = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "X_test_xgb = X_test_xgb.reindex(columns=X_train_xgb.columns, fill_value=0)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "    random_state=42, eval_metric='logloss', use_label_encoder=False\n",
    ")\n",
    "start = time.time()\n",
    "xgb_model.fit(X_train_xgb, y_train)\n",
    "xgb_time = time.time() - start\n",
    "xgb_proba = xgb_model.predict_proba(X_test_xgb)[:, 1]\n",
    "xgb_pred = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=100, num_leaves=31, learning_rate=0.1,\n",
    "    random_state=42, verbose=-1\n",
    ")\n",
    "start = time.time()\n",
    "lgb_model.fit(X_train_lgb, y_train, categorical_feature=categorical_cols)\n",
    "lgb_time = time.time() - start\n",
    "lgb_proba = lgb_model.predict_proba(X_test_lgb)[:, 1]\n",
    "lgb_pred = lgb_model.predict(X_test_lgb)\n",
    "\n",
    "# CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100, depth=6, learning_rate=0.1,\n",
    "    cat_features=categorical_cols, random_state=42, verbose=0\n",
    ")\n",
    "start = time.time()\n",
    "cat_model.fit(X_train, y_train)\n",
    "cat_time = time.time() - start\n",
    "cat_proba = cat_model.predict_proba(X_test)[:, 1]\n",
    "cat_pred = cat_model.predict(X_test)\n",
    "\n",
    "print(\"Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile performance metrics\n",
    "results = {\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, xgb_pred),\n",
    "        accuracy_score(y_test, lgb_pred),\n",
    "        accuracy_score(y_test, cat_pred)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, xgb_pred),\n",
    "        precision_score(y_test, lgb_pred),\n",
    "        precision_score(y_test, cat_pred)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, xgb_pred),\n",
    "        recall_score(y_test, lgb_pred),\n",
    "        recall_score(y_test, cat_pred)\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        f1_score(y_test, xgb_pred),\n",
    "        f1_score(y_test, lgb_pred),\n",
    "        f1_score(y_test, cat_pred)\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test, xgb_proba),\n",
    "        roc_auc_score(y_test, lgb_proba),\n",
    "        roc_auc_score(y_test, cat_proba)\n",
    "    ],\n",
    "    'Training Time (s)': [xgb_time, lgb_time, cat_time]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Style the dataframe\n",
    "def highlight_best(s):\n",
    "    if s.name == 'Training Time (s)':\n",
    "        is_best = s == s.min()\n",
    "    else:\n",
    "        is_best = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_best]\n",
    "\n",
    "styled_results = results_df.style.apply(highlight_best).format('{:.4f}')\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {'XGBoost': 'blue', 'LightGBM': 'green', 'CatBoost': 'orange'}\n",
    "\n",
    "for model in ['XGBoost', 'LightGBM', 'CatBoost']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=model,\n",
    "        x=metrics,\n",
    "        y=[results_df.loc[model, m] for m in metrics],\n",
    "        marker_color=colors[model]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: XGBoost vs LightGBM vs CatBoost',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training times across different dataset sizes\n",
    "# (Simulated for demonstration)\n",
    "\n",
    "dataset_sizes = [500, 1000, 2000, 5000, 10000]\n",
    "training_times = {\n",
    "    'XGBoost': [],\n",
    "    'LightGBM': [],\n",
    "    'CatBoost': []\n",
    "}\n",
    "\n",
    "# Simulate timing for different sizes\n",
    "np.random.seed(42)\n",
    "for size in dataset_sizes:\n",
    "    # XGBoost: roughly linear with size\n",
    "    training_times['XGBoost'].append(0.05 + size * 0.00008 + np.random.normal(0, 0.01))\n",
    "    # LightGBM: faster, especially for larger sizes\n",
    "    training_times['LightGBM'].append(0.03 + size * 0.00004 + np.random.normal(0, 0.005))\n",
    "    # CatBoost: more overhead but efficient\n",
    "    training_times['CatBoost'].append(0.08 + size * 0.00006 + np.random.normal(0, 0.01))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for model, color in colors.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataset_sizes,\n",
    "        y=training_times[model],\n",
    "        mode='lines+markers',\n",
    "        name=model,\n",
    "        line=dict(color=color, width=2),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training Time Scaling by Dataset Size',\n",
    "    xaxis_title='Number of Samples',\n",
    "    yaxis_title='Training Time (seconds)',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Note: LightGBM typically shows the biggest speed advantage on larger datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 When to Use Each Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision guide\n",
    "decision_guide = {\n",
    "    'Scenario': [\n",
    "        'Large dataset (>100k rows)',\n",
    "        'Many categorical features',\n",
    "        'Quick prototyping needed',\n",
    "        'Production deployment',\n",
    "        'GPU training required',\n",
    "        'Minimal preprocessing desired',\n",
    "        'Best documentation/community',\n",
    "        'Small dataset (risk of overfit)',\n",
    "        'High-cardinality categoricals',\n",
    "        'Need feature selection'\n",
    "    ],\n",
    "    'Best Choice': [\n",
    "        'LightGBM',\n",
    "        'CatBoost',\n",
    "        'CatBoost',\n",
    "        'XGBoost or LightGBM',\n",
    "        'CatBoost or XGBoost',\n",
    "        'CatBoost',\n",
    "        'XGBoost',\n",
    "        'CatBoost',\n",
    "        'CatBoost',\n",
    "        'Any (all support)\n",
    "    ],\n",
    "    'Reason': [\n",
    "        'Fastest training, lowest memory',\n",
    "        'Native handling without encoding',\n",
    "        'Works out of the box with minimal tuning',\n",
    "        'Mature, well-tested, great tooling',\n",
    "        'Best GPU implementation',\n",
    "        'Accepts string categoricals directly',\n",
    "        'Largest community, most tutorials',\n",
    "        'Ordered boosting prevents overfit',\n",
    "        'Efficient ordered target encoding',\n",
    "        'Built-in importance metrics'\n",
    "    ]\n",
    "}\n",
    "\n",
    "guide_df = pd.DataFrame(decision_guide)\n",
    "guide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "summary_comparison = {\n",
    "    'Aspect': [\n",
    "        'Speed',\n",
    "        'Memory Usage',\n",
    "        'Categorical Handling',\n",
    "        'Missing Values',\n",
    "        'Default Performance',\n",
    "        'Overfitting Risk',\n",
    "        'GPU Support',\n",
    "        'scikit-learn Compatible',\n",
    "        'Ease of Use'\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        'Fast',\n",
    "        'Moderate',\n",
    "        'Requires encoding',\n",
    "        'Native (learns direction)',\n",
    "        'Good',\n",
    "        'Moderate',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'Good'\n",
    "    ],\n",
    "    'LightGBM': [\n",
    "        'Very Fast',\n",
    "        'Low',\n",
    "        'Native (integer-encoded)',\n",
    "        'Native',\n",
    "        'Good',\n",
    "        'Higher (leaf-wise)',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'Good'\n",
    "    ],\n",
    "    'CatBoost': [\n",
    "        'Fast',\n",
    "        'Moderate',\n",
    "        'Native (strings OK!)',\n",
    "        'Native',\n",
    "        'Excellent',\n",
    "        'Lower (ordered)',\n",
    "        'Excellent',\n",
    "        'Yes',\n",
    "        'Excellent'\n",
    "    ]\n",
    "}\n",
    "\n",
    "pd.DataFrame(summary_comparison).set_index('Aspect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for Higher Education Analytics\n",
    "\n",
    "For **student departure prediction** and similar higher education problems:\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|:----------|:---------------|\n",
    "| **Starting out** | CatBoost - easiest setup, handles categoricals automatically |\n",
    "| **Large institution (100k+ students)** | LightGBM - fastest training |\n",
    "| **Production system** | XGBoost - most mature, best tooling |\n",
    "| **Many demographic categoricals** | CatBoost - best categorical handling |\n",
    "| **Ensemble all three** | Often gives best results! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **LightGBM**:\n",
    "   - Leaf-wise tree growth for efficiency\n",
    "   - Native categorical support (integer-encoded)\n",
    "   - Best for large datasets and speed-critical applications\n",
    "   - Use `num_leaves` to control complexity\n",
    "\n",
    "2. **CatBoost**:\n",
    "   - Native string categorical support (no encoding needed)\n",
    "   - Ordered boosting prevents target leakage\n",
    "   - Best for ease of use and categorical-heavy data\n",
    "   - Robust to overfitting on small datasets\n",
    "\n",
    "3. **Comparison**:\n",
    "   - All three perform similarly on many tasks\n",
    "   - Choose based on data characteristics and requirements\n",
    "   - Consider ensembling for best results\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Library | Key Strength | Best For |\n",
    "|:--------|:-------------|:---------|\n",
    "| XGBoost | Maturity, tooling | Production, benchmarking |\n",
    "| LightGBM | Speed, efficiency | Large datasets |\n",
    "| CatBoost | Categorical handling | Ease of use, categoricals |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will explore **training and evaluation** techniques including early stopping, cross-validation, and SHAP-based feature importance.\n",
    "\n",
    "**Proceed to:** `4.4 Train and Evaluate Boosted Models`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}