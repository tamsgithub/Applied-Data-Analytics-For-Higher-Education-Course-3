{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Tune Boosted Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebooks, we built and evaluated gradient boosting models with default or manually chosen hyperparameters. Now we focus on **systematic hyperparameter tuning** to optimize model performance.\n",
    "\n",
    "Hyperparameter tuning is crucial because:\n",
    "- Default parameters are general-purpose, not optimal for your specific data\n",
    "- The right hyperparameters can significantly improve performance\n",
    "- Proper tuning prevents overfitting while maximizing predictive power\n",
    "\n",
    "We'll explore three tuning strategies:\n",
    "1. **Grid Search**: Exhaustive search over specified parameter values\n",
    "2. **Randomized Search**: Random sampling from parameter distributions\n",
    "3. **Bayesian Optimization**: Intelligent search using prior results (Optuna)\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Identify the most important hyperparameters for gradient boosting\n",
    "2. Implement grid search and randomized search for hyperparameter tuning\n",
    "3. Use Optuna for efficient Bayesian optimization\n",
    "4. Analyze and visualize hyperparameter search results\n",
    "5. Build a final optimized model for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, make_scorer\n",
    ")\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Optuna for Bayesian optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic student data\n",
    "np.random.seed(42)\n",
    "n_students = 3000\n",
    "\n",
    "data = {\n",
    "    'STUDENT_ID': range(1, n_students + 1),\n",
    "    'HS_GPA': np.random.normal(3.2, 0.5, n_students).clip(2.0, 4.0),\n",
    "    'MATH_PLACEMENT': np.random.choice(['Remedial', 'College-Ready', 'Advanced'], n_students, p=[0.2, 0.5, 0.3]),\n",
    "    'FIRST_GEN': np.random.choice(['Yes', 'No'], n_students, p=[0.35, 0.65]),\n",
    "    'PELL_ELIGIBLE': np.random.choice(['Yes', 'No'], n_students, p=[0.40, 0.60]),\n",
    "    'RESIDENCY': np.random.choice(['In-State', 'Out-of-State', 'International'], n_students, p=[0.7, 0.2, 0.1]),\n",
    "    'UNITS_ATTEMPT_1': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_1': np.random.normal(2.8, 0.7, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_1': np.random.beta(2, 8, n_students),\n",
    "    'UNITS_ATTEMPT_2': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_2': np.random.normal(2.9, 0.6, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_2': np.random.beta(2, 8, n_students),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Derived features\n",
    "df['CUM_GPA'] = (df['GPA_1'] + df['GPA_2']) / 2\n",
    "df['CUM_UNITS'] = df['UNITS_ATTEMPT_1'] + df['UNITS_ATTEMPT_2']\n",
    "df['AVG_DFW'] = (df['DFW_RATE_1'] + df['DFW_RATE_2']) / 2\n",
    "df['GPA_TREND'] = df['GPA_2'] - df['GPA_1']\n",
    "\n",
    "# Generate target\n",
    "departure_prob = (\n",
    "    0.3 - 0.15 * (df['CUM_GPA'] - 2.5) + 0.3 * df['AVG_DFW']\n",
    "    + 0.05 * (df['FIRST_GEN'] == 'Yes') - 0.02 * (df['HS_GPA'] - 3.0)\n",
    "    + 0.05 * (df['MATH_PLACEMENT'] == 'Remedial') - 0.05 * df['GPA_TREND']\n",
    ")\n",
    "departure_prob = departure_prob.clip(0.05, 0.95)\n",
    "df['DEPARTED'] = np.random.binomial(1, departure_prob)\n",
    "\n",
    "# Define columns\n",
    "categorical_cols = ['MATH_PLACEMENT', 'FIRST_GEN', 'PELL_ELIGIBLE', 'RESIDENCY']\n",
    "numerical_cols = ['HS_GPA', 'UNITS_ATTEMPT_1', 'GPA_1', 'DFW_RATE_1', \n",
    "                  'UNITS_ATTEMPT_2', 'GPA_2', 'DFW_RATE_2', \n",
    "                  'CUM_GPA', 'CUM_UNITS', 'AVG_DFW', 'GPA_TREND']\n",
    "feature_cols = categorical_cols + numerical_cols\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['DEPARTED']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Prepare encoded versions for different libraries\n",
    "# One-hot for XGBoost\n",
    "X_train_xgb = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test_xgb = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "X_test_xgb = X_test_xgb.reindex(columns=X_train_xgb.columns, fill_value=0)\n",
    "\n",
    "# Label encode for LightGBM\n",
    "X_train_lgb = X_train.copy()\n",
    "X_test_lgb = X_test.copy()\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_lgb[col] = le.fit_transform(X_train_lgb[col])\n",
    "    X_test_lgb[col] = le.transform(X_test_lgb[col])\n",
    "\n",
    "print(f\"Training set: {len(X_train)} students\")\n",
    "print(f\"Test set: {len(X_test)} students\")\n",
    "print(f\"Departure rate: {y_train.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Hyperparameters for Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Learning Rate and Number of Estimators\n",
    "\n",
    "The **learning rate** (`learning_rate` or `eta`) and **number of estimators** (`n_estimators`) work together:\n",
    "\n",
    "- Lower learning rate = more trees needed, but often better generalization\n",
    "- Higher learning rate = fewer trees, faster training, risk of overfitting\n",
    "\n",
    "**Rule of thumb**: Start with `learning_rate=0.1`, then try lower values with more trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter categories\n",
    "param_categories = {\n",
    "    'Category': ['Boosting', 'Boosting', 'Tree', 'Tree', 'Tree', \n",
    "                 'Regularization', 'Regularization', 'Regularization',\n",
    "                 'Sampling', 'Sampling', 'Class Balance'],\n",
    "    'Parameter': ['n_estimators', 'learning_rate', 'max_depth', 'num_leaves', 'min_child_weight',\n",
    "                  'gamma', 'reg_alpha', 'reg_lambda',\n",
    "                  'subsample', 'colsample_bytree', 'scale_pos_weight'],\n",
    "    'XGBoost Name': ['n_estimators', 'learning_rate', 'max_depth', 'N/A', 'min_child_weight',\n",
    "                     'gamma', 'reg_alpha', 'reg_lambda',\n",
    "                     'subsample', 'colsample_bytree', 'scale_pos_weight'],\n",
    "    'LightGBM Name': ['n_estimators', 'learning_rate', 'max_depth', 'num_leaves', 'min_child_samples',\n",
    "                      'min_split_gain', 'reg_alpha', 'reg_lambda',\n",
    "                      'subsample', 'colsample_bytree', 'scale_pos_weight'],\n",
    "    'CatBoost Name': ['iterations', 'learning_rate', 'depth', 'N/A', 'min_data_in_leaf',\n",
    "                      'N/A', 'N/A', 'l2_leaf_reg',\n",
    "                      'subsample', 'rsm', 'auto_class_weights'],\n",
    "    'Typical Range': ['100-1000', '0.01-0.3', '3-10', '15-255', '1-10',\n",
    "                      '0-5', '0-1', '0-1',\n",
    "                      '0.5-1.0', '0.5-1.0', '1 or class ratio']\n",
    "}\n",
    "\n",
    "pd.DataFrame(param_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tree Complexity Parameters\n",
    "\n",
    "These control how complex each individual tree can be:\n",
    "\n",
    "| Parameter | Effect |\n",
    "|:----------|:-------|\n",
    "| `max_depth` | Maximum tree depth (deeper = more complex) |\n",
    "| `num_leaves` | Maximum leaves per tree (LightGBM) |\n",
    "| `min_child_weight` | Minimum sum of instance weight in a leaf |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Regularization Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|:----------|:------------|\n",
    "| `gamma` | Minimum loss reduction for a split |\n",
    "| `reg_alpha` | L1 regularization on weights |\n",
    "| `reg_lambda` | L2 regularization on weights |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sampling Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|:----------|:------------|\n",
    "| `subsample` | Fraction of samples used per tree |\n",
    "| `colsample_bytree` | Fraction of features used per tree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Learning Rate: Lower = More Conservative',\n",
    "    'Max Depth: Higher = More Complex',\n",
    "    'Regularization: Higher = Simpler Model',\n",
    "    'Subsample: Lower = More Stochastic'\n",
    "))\n",
    "\n",
    "# Learning rate effect\n",
    "x = np.linspace(0.01, 0.3, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=1 - np.exp(-10*x),\n",
    "    mode='lines', name='Faster Learning',\n",
    "    line=dict(color='red')\n",
    "), row=1, col=1)\n",
    "\n",
    "# Max depth effect\n",
    "x = np.arange(1, 11)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=1 - 1/x,\n",
    "    mode='lines+markers', name='Complexity',\n",
    "    line=dict(color='blue')\n",
    "), row=1, col=2)\n",
    "\n",
    "# Regularization effect\n",
    "x = np.linspace(0, 1, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=1 - x,\n",
    "    mode='lines', name='Model Freedom',\n",
    "    line=dict(color='green')\n",
    "), row=2, col=1)\n",
    "\n",
    "# Subsample effect\n",
    "x = np.linspace(0.5, 1.0, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=x,\n",
    "    mode='lines', name='Stability',\n",
    "    line=dict(color='orange')\n",
    "), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=500, showlegend=False,\n",
    "                  title_text='Hyperparameter Effects on Model Behavior')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Hyperparameter Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Learning Rate vs. n_estimators Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore learning rate and n_estimators combinations\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "n_estimators_list = [50, 100, 200, 500]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_est in n_estimators_list:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_est,\n",
    "            learning_rate=lr,\n",
    "            max_depth=6,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        \n",
    "        scores = cross_val_score(model, X_train_xgb, y_train, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'n_estimators': n_est,\n",
    "            'mean_auc': scores.mean(),\n",
    "            'std_auc': scores.std()\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create heatmap\n",
    "pivot = results_df.pivot(index='learning_rate', columns='n_estimators', values='mean_auc')\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot.values,\n",
    "    x=[str(x) for x in pivot.columns],\n",
    "    y=[str(x) for x in pivot.index],\n",
    "    text=pivot.values.round(4),\n",
    "    texttemplate='%{text}',\n",
    "    colorscale='Blues',\n",
    "    colorbar=dict(title='ROC-AUC')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Learning Rate vs n_estimators: Cross-Validation ROC-AUC',\n",
    "    xaxis_title='n_estimators',\n",
    "    yaxis_title='learning_rate',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "best_idx = results_df['mean_auc'].idxmax()\n",
    "print(f\"\\nBest combination:\")\n",
    "print(f\"  learning_rate: {results_df.loc[best_idx, 'learning_rate']}\")\n",
    "print(f\"  n_estimators: {results_df.loc[best_idx, 'n_estimators']}\")\n",
    "print(f\"  ROC-AUC: {results_df.loc[best_idx, 'mean_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Max Depth Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore max_depth\n",
    "max_depths = [2, 3, 4, 5, 6, 8, 10]\n",
    "\n",
    "depth_results = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=depth,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    # Get train and validation scores\n",
    "    model.fit(X_train_xgb, y_train)\n",
    "    train_score = roc_auc_score(y_train, model.predict_proba(X_train_xgb)[:, 1])\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train_xgb, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': depth,\n",
    "        'train_auc': train_score,\n",
    "        'cv_auc': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=depth_df['max_depth'],\n",
    "    y=depth_df['train_auc'],\n",
    "    mode='lines+markers',\n",
    "    name='Train AUC',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=depth_df['max_depth'],\n",
    "    y=depth_df['cv_auc'],\n",
    "    mode='lines+markers',\n",
    "    name='CV AUC',\n",
    "    line=dict(color='green', width=2),\n",
    "    error_y=dict(type='data', array=depth_df['cv_std'])\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Effect of max_depth on Model Performance',\n",
    "    xaxis_title='max_depth',\n",
    "    yaxis_title='ROC-AUC',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Observation: As max_depth increases:\")\n",
    "print(\"- Training AUC increases (more complex model)\")\n",
    "print(\"- CV AUC may plateau or decrease (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for param, values in param_grid.items():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"Grid Search Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total parameter combinations: {total_combinations}\")\n",
    "print(f\"With 3-fold CV: {total_combinations * 3} model fits\")\n",
    "print(f\"\\nParameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Grid Search\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all cores\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_xgb, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGrid Search completed in {grid_time:.1f} seconds\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyzing Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort by mean test score\n",
    "grid_results_sorted = grid_results.sort_values('mean_test_score', ascending=False)\n",
    "\n",
    "# Display top 10 combinations\n",
    "display_cols = ['param_n_estimators', 'param_max_depth', 'param_learning_rate',\n",
    "                'param_subsample', 'param_colsample_bytree',\n",
    "                'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "\n",
    "print(\"Top 10 Parameter Combinations:\")\n",
    "grid_results_sorted[display_cols].head(10).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter importance\n",
    "# Calculate mean score for each parameter value\n",
    "param_analysis = {}\n",
    "\n",
    "for param in param_grid.keys():\n",
    "    param_col = f'param_{param}'\n",
    "    param_means = grid_results.groupby(param_col)['mean_test_score'].mean()\n",
    "    param_analysis[param] = param_means\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=list(param_grid.keys()))\n",
    "\n",
    "positions = [(1,1), (1,2), (1,3), (2,1), (2,2)]\n",
    "\n",
    "for idx, (param, means) in enumerate(param_analysis.items()):\n",
    "    row, col = positions[idx]\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[str(x) for x in means.index],\n",
    "        y=means.values,\n",
    "        marker_color='steelblue'\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=500, showlegend=False,\n",
    "                  title_text='Mean CV Score by Parameter Value')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Randomized Search Setup\n",
    "\n",
    "Randomized search samples from continuous distributions, exploring more of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(3, 12),\n",
    "    'learning_rate': loguniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'reg_alpha': loguniform(1e-5, 1),\n",
    "    'reg_lambda': loguniform(1e-5, 1)\n",
    "}\n",
    "\n",
    "# Number of random samples\n",
    "n_iter = 50\n",
    "\n",
    "print(f\"Randomized Search Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Random samples: {n_iter}\")\n",
    "print(f\"With 3-fold CV: {n_iter * 3} model fits\")\n",
    "print(f\"\\nParameter Distributions:\")\n",
    "for param, dist in param_distributions.items():\n",
    "    print(f\"  {param}: {type(dist).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_xgb, y_train)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRandomized Search completed in {random_time:.1f} seconds\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparing Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Grid vs Random Search\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Best CV Score', 'Time (seconds)', 'Combinations Tried', 'Parameters Tuned'],\n",
    "    'Grid Search': [f\"{grid_search.best_score_:.4f}\", f\"{grid_time:.1f}\", \n",
    "                    total_combinations, len(param_grid)],\n",
    "    'Random Search': [f\"{random_search.best_score_:.4f}\", f\"{random_time:.1f}\",\n",
    "                      n_iter, len(param_distributions)]\n",
    "})\n",
    "\n",
    "print(\"Grid Search vs Randomized Search\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the search trajectories\n",
    "random_results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Grid Search: Score Distribution',\n",
    "    'Random Search: Score Distribution'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=grid_results['mean_test_score'],\n",
    "    nbinsx=20,\n",
    "    name='Grid Search',\n",
    "    marker_color='blue'\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=random_results['mean_test_score'],\n",
    "    nbinsx=20,\n",
    "    name='Random Search',\n",
    "    marker_color='green'\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False,\n",
    "                  title_text='Distribution of CV Scores Across Search')\n",
    "fig.update_xaxes(title_text='Mean CV Score')\n",
    "fig.update_yaxes(title_text='Count')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Optimization with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Introduction to Optuna\n",
    "\n",
    "**Optuna** uses Bayesian optimization to intelligently search the parameter space:\n",
    "\n",
    "- Uses previous results to guide the search\n",
    "- Balances exploration (trying new areas) and exploitation (refining good areas)\n",
    "- More efficient than grid or random search\n",
    "- Supports pruning (stopping unpromising trials early)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1, log=True),\n",
    "    }\n",
    "    \n",
    "    # Create model with suggested parameters\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train_xgb, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "print(\"Optuna objective function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the study\n",
    "sampler = TPESampler(seed=42)  # Tree-structured Parzen Estimator\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Maximize ROC-AUC\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "# Suppress Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "optuna_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nOptuna Study completed in {optuna_time:.1f} seconds\")\n",
    "print(f\"\\nBest Trial:\")\n",
    "print(f\"  Value (ROC-AUC): {study.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in study.best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Visualizing Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "trials_df = study.trials_dataframe()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# All trials\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=trials_df['number'],\n",
    "    y=trials_df['value'],\n",
    "    mode='markers',\n",
    "    name='Trial Score',\n",
    "    marker=dict(color='lightblue', size=8)\n",
    "))\n",
    "\n",
    "# Best so far\n",
    "best_so_far = trials_df['value'].cummax()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=trials_df['number'],\n",
    "    y=best_so_far,\n",
    "    mode='lines',\n",
    "    name='Best So Far',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Optuna Optimization History',\n",
    "    xaxis_title='Trial Number',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter importance\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Parameter': list(importance.keys()),\n",
    "    'Importance': list(importance.values())\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    importance_df,\n",
    "    x='Importance', y='Parameter',\n",
    "    orientation='h',\n",
    "    title='Hyperparameter Importance (Optuna)',\n",
    "    color='Importance',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=450, yaxis_title='')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter relationship visualization\n",
    "# Plot learning_rate vs score\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'learning_rate', 'max_depth', 'n_estimators', 'subsample'\n",
    "))\n",
    "\n",
    "params_to_plot = ['learning_rate', 'max_depth', 'n_estimators', 'subsample']\n",
    "\n",
    "for idx, param in enumerate(params_to_plot):\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=trials_df[f'params_{param}'],\n",
    "        y=trials_df['value'],\n",
    "        mode='markers',\n",
    "        marker=dict(color=trials_df['number'], colorscale='Viridis', size=8),\n",
    "        showlegend=False\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=600, title_text='Parameter-Score Relationships')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Training the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with Optuna's best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    **best_params,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_xgb, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = final_model.predict(X_test_xgb)\n",
    "y_pred_proba = final_model.predict_proba(X_test_xgb)[:, 1]\n",
    "\n",
    "print(\"Final Tuned Model Performance on Test Set\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models: Default, Grid, Random, Optuna\n",
    "\n",
    "# Default model\n",
    "default_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "default_model.fit(X_train_xgb, y_train)\n",
    "default_proba = default_model.predict_proba(X_test_xgb)[:, 1]\n",
    "default_pred = default_model.predict(X_test_xgb)\n",
    "\n",
    "# Grid Search model\n",
    "grid_model = grid_search.best_estimator_\n",
    "grid_proba = grid_model.predict_proba(X_test_xgb)[:, 1]\n",
    "grid_pred = grid_model.predict(X_test_xgb)\n",
    "\n",
    "# Random Search model\n",
    "random_model = random_search.best_estimator_\n",
    "random_proba = random_model.predict_proba(X_test_xgb)[:, 1]\n",
    "random_pred = random_model.predict(X_test_xgb)\n",
    "\n",
    "# Compile results\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Default XGBoost', 'Grid Search', 'Random Search', 'Optuna (Bayesian)'],\n",
    "    'CV Score': [\n",
    "        cross_val_score(default_model, X_train_xgb, y_train, cv=3, scoring='roc_auc').mean(),\n",
    "        grid_search.best_score_,\n",
    "        random_search.best_score_,\n",
    "        study.best_value\n",
    "    ],\n",
    "    'Test ROC-AUC': [\n",
    "        roc_auc_score(y_test, default_proba),\n",
    "        roc_auc_score(y_test, grid_proba),\n",
    "        roc_auc_score(y_test, random_proba),\n",
    "        roc_auc_score(y_test, y_pred_proba)\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_score(y_test, default_pred),\n",
    "        accuracy_score(y_test, grid_pred),\n",
    "        accuracy_score(y_test, random_pred),\n",
    "        accuracy_score(y_test, y_pred)\n",
    "    ],\n",
    "    'Test F1': [\n",
    "        f1_score(y_test, default_pred),\n",
    "        f1_score(y_test, grid_pred),\n",
    "        f1_score(y_test, random_pred),\n",
    "        f1_score(y_test, y_pred)\n",
    "    ],\n",
    "    'Tuning Time (s)': [0, grid_time, random_time, optuna_time]\n",
    "})\n",
    "\n",
    "# Highlight best values\n",
    "def highlight_best(s):\n",
    "    if s.name == 'Tuning Time (s)':\n",
    "        return [''] * len(s)\n",
    "    is_best = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_best]\n",
    "\n",
    "model_comparison.style.apply(highlight_best).format({\n",
    "    'CV Score': '{:.4f}',\n",
    "    'Test ROC-AUC': '{:.4f}',\n",
    "    'Test Accuracy': '{:.4f}',\n",
    "    'Test F1': '{:.4f}',\n",
    "    'Tuning Time (s)': '{:.1f}'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics = ['CV Score', 'Test ROC-AUC', 'Test Accuracy', 'Test F1']\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "for model, color in zip(model_comparison['Model'], colors):\n",
    "    model_data = model_comparison[model_comparison['Model'] == model]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=model,\n",
    "        x=metrics,\n",
    "        y=[model_data[m].values[0] for m in metrics],\n",
    "        marker_color=color\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Default vs Tuned',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0.6, 0.85])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of tuning methods\n",
    "tuning_summary = pd.DataFrame({\n",
    "    'Method': ['Grid Search', 'Random Search', 'Optuna (Bayesian)'],\n",
    "    'Pros': [\n",
    "        'Exhaustive, reproducible, simple',\n",
    "        'Explores continuous space, faster than grid',\n",
    "        'Intelligent search, most efficient, handles complex spaces'\n",
    "    ],\n",
    "    'Cons': [\n",
    "        'Computationally expensive, misses between grid points',\n",
    "        'Random, no learning from previous trials',\n",
    "        'More complex setup, requires more dependencies'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Small parameter spaces, quick validation',\n",
    "        'Medium-sized searches, continuous parameters',\n",
    "        'Large searches, production tuning'\n",
    "    ]\n",
    "})\n",
    "\n",
    "tuning_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Important Hyperparameters**:\n",
    "   - Boosting: `learning_rate`, `n_estimators`\n",
    "   - Tree: `max_depth`, `min_child_weight`\n",
    "   - Regularization: `gamma`, `reg_alpha`, `reg_lambda`\n",
    "   - Sampling: `subsample`, `colsample_bytree`\n",
    "\n",
    "2. **Tuning Strategies**:\n",
    "   - **Grid Search**: Exhaustive but expensive\n",
    "   - **Random Search**: Efficient for continuous parameters\n",
    "   - **Bayesian (Optuna)**: Most efficient, learns from trials\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Start with sensible defaults\n",
    "   - Tune learning rate and n_estimators together\n",
    "   - Use early stopping during tuning\n",
    "   - Validate final model on held-out test set\n",
    "\n",
    "### Hyperparameter Tuning Workflow\n",
    "\n",
    "```\n",
    "1. Start with defaults\n",
    "   |\n",
    "2. Manual exploration of key parameters\n",
    "   |\n",
    "3. Coarse search (Grid or Random)\n",
    "   |\n",
    "4. Fine-tuning (Optuna/Bayesian)\n",
    "   |\n",
    "5. Final validation on test set\n",
    "```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|:------|:-------------|\n",
    "| Learning Rate | Lower = more trees, often better generalization |\n",
    "| Max Depth | 4-8 usually sufficient; higher may overfit |\n",
    "| Regularization | Use when overfitting observed |\n",
    "| Grid Search | Good for small, discrete spaces |\n",
    "| Random Search | Good for continuous parameters |\n",
    "| Optuna | Best for production tuning |\n",
    "\n",
    "### Module 4 Complete!\n",
    "\n",
    "Congratulations! You have completed Module 4 on Gradient Boosting. You now know how to:\n",
    "\n",
    "1. Understand boosting theory and gradient descent connection\n",
    "2. Build models with XGBoost, LightGBM, and CatBoost\n",
    "3. Implement early stopping and cross-validation\n",
    "4. Interpret models using SHAP values\n",
    "5. Tune hyperparameters using multiple strategies\n",
    "\n",
    "**Continue to Module 5** to learn about model deployment and productionization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}