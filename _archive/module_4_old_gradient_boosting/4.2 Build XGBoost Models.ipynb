{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Build XGBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we explored the theory behind gradient boosting. Now we put that knowledge into practice by building **XGBoost** (eXtreme Gradient Boosting) classifiers for our student departure prediction problem.\n",
    "\n",
    "XGBoost has become one of the most popular machine learning algorithms due to its:\n",
    "- Excellent predictive performance\n",
    "- Built-in regularization to prevent overfitting\n",
    "- Efficient handling of missing values\n",
    "- Scikit-learn compatible API\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build XGBoost classification models using the scikit-learn API\n",
    "2. Understand and configure key XGBoost hyperparameters\n",
    "3. Integrate XGBoost into scikit-learn preprocessing pipelines\n",
    "4. Handle class imbalance in gradient boosting models\n",
    "5. Extract and visualize feature importance from XGBoost models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Student Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the student departure dataset\n",
    "# This dataset contains student information from semesters 1-2 to predict departure in semester 3\n",
    "\n",
    "# For this notebook, we'll create a synthetic dataset that mirrors real institutional data\n",
    "np.random.seed(42)\n",
    "n_students = 2000\n",
    "\n",
    "# Generate synthetic student data\n",
    "data = {\n",
    "    'STUDENT_ID': range(1, n_students + 1),\n",
    "    'HS_GPA': np.random.normal(3.2, 0.5, n_students).clip(2.0, 4.0),\n",
    "    'MATH_PLACEMENT': np.random.choice(['Remedial', 'College-Ready', 'Advanced'], n_students, p=[0.2, 0.5, 0.3]),\n",
    "    'FIRST_GEN': np.random.choice(['Yes', 'No'], n_students, p=[0.35, 0.65]),\n",
    "    'PELL_ELIGIBLE': np.random.choice(['Yes', 'No'], n_students, p=[0.40, 0.60]),\n",
    "    'RESIDENCY': np.random.choice(['In-State', 'Out-of-State', 'International'], n_students, p=[0.7, 0.2, 0.1]),\n",
    "    'UNITS_ATTEMPT_1': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_1': np.random.normal(2.8, 0.7, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_1': np.random.beta(2, 8, n_students),\n",
    "    'UNITS_ATTEMPT_2': np.random.normal(14, 2, n_students).clip(6, 18).astype(int),\n",
    "    'GPA_2': np.random.normal(2.9, 0.6, n_students).clip(0.0, 4.0),\n",
    "    'DFW_RATE_2': np.random.beta(2, 8, n_students),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate cumulative metrics\n",
    "df['CUM_GPA'] = (df['GPA_1'] + df['GPA_2']) / 2\n",
    "df['CUM_UNITS'] = df['UNITS_ATTEMPT_1'] + df['UNITS_ATTEMPT_2']\n",
    "df['AVG_DFW'] = (df['DFW_RATE_1'] + df['DFW_RATE_2']) / 2\n",
    "\n",
    "# Generate target variable (DEPARTED) based on realistic factors\n",
    "departure_prob = (\n",
    "    0.3  # Base rate\n",
    "    - 0.15 * (df['CUM_GPA'] - 2.5)  # Lower GPA = higher departure\n",
    "    + 0.3 * df['AVG_DFW']  # Higher DFW = higher departure\n",
    "    + 0.05 * (df['FIRST_GEN'] == 'Yes')  # First-gen slightly higher\n",
    "    - 0.02 * (df['HS_GPA'] - 3.0)  # Lower HS GPA = higher departure\n",
    "    + 0.05 * (df['MATH_PLACEMENT'] == 'Remedial')  # Remedial math higher\n",
    ")\n",
    "departure_prob = departure_prob.clip(0.05, 0.95)\n",
    "df['DEPARTED'] = np.random.binomial(1, departure_prob)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDeparture rate: {df['DEPARTED'].mean():.1%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data types and distributions\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nNumerical Features Summary:\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['MATH_PLACEMENT', 'FIRST_GEN', 'PELL_ELIGIBLE', 'RESIDENCY']\n",
    "numerical_cols = ['HS_GPA', 'UNITS_ATTEMPT_1', 'GPA_1', 'DFW_RATE_1', \n",
    "                  'UNITS_ATTEMPT_2', 'GPA_2', 'DFW_RATE_2', \n",
    "                  'CUM_GPA', 'CUM_UNITS', 'AVG_DFW']\n",
    "\n",
    "feature_cols = categorical_cols + numerical_cols\n",
    "target_col = 'DEPARTED'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} students\")\n",
    "print(f\"Test set: {X_test.shape[0]} students\")\n",
    "print(f\"\\nTraining departure rate: {y_train.mean():.1%}\")\n",
    "print(f\"Test departure rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 XGBoost Key Features\n",
    "\n",
    "XGBoost extends the basic gradient boosting algorithm with several innovations:\n",
    "\n",
    "| Feature | Description |\n",
    "|:--------|:------------|\n",
    "| **Regularization** | L1 and L2 penalties on leaf weights prevent overfitting |\n",
    "| **Sparsity Awareness** | Efficiently handles missing values by learning optimal direction |\n",
    "| **Parallel Processing** | Tree construction parallelized at the feature level |\n",
    "| **Cache Optimization** | Optimized data structures for CPU cache efficiency |\n",
    "| **Out-of-Core** | Can train on datasets larger than memory |\n",
    "| **Tree Pruning** | Uses \"max_depth\" and prunes trees using cost-complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 XGBoost vs scikit-learn GradientBoosting\n",
    "\n",
    "While scikit-learn has its own `GradientBoostingClassifier`, XGBoost offers several advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Speed',\n",
    "        'Regularization',\n",
    "        'Missing Values',\n",
    "        'Parallel Training',\n",
    "        'GPU Support',\n",
    "        'Early Stopping',\n",
    "        'Feature Importance',\n",
    "        'scikit-learn Compatible'\n",
    "    ],\n",
    "    'sklearn GradientBoosting': [\n",
    "        'Slower',\n",
    "        'Limited (via tree params)',\n",
    "        'Requires imputation',\n",
    "        'No',\n",
    "        'No',\n",
    "        'No (manual)',\n",
    "        'Basic',\n",
    "        'Yes'\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        '5-10x faster',\n",
    "        'Built-in L1/L2',\n",
    "        'Native support',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'Built-in',\n",
    "        'Multiple types',\n",
    "        'Yes (wrapper)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building XGBoost Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic XGBoost Model\n",
    "\n",
    "Let's start with a simple XGBoost model using default parameters. Note that XGBoost requires numerical features, so we first need to encode our categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's manually encode categorical features for the basic example\n",
    "# We'll use a pipeline approach later\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure same columns in train and test\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"Encoded training features: {X_train_encoded.shape[1]} columns\")\n",
    "print(f\"\\nEncoded columns:\")\n",
    "print(X_train_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a basic XGBoost classifier\n",
    "xgb_basic = XGBClassifier(\n",
    "    n_estimators=100,       # Number of boosting rounds (trees)\n",
    "    max_depth=6,            # Maximum tree depth\n",
    "    learning_rate=0.1,      # Step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',  # Evaluation metric\n",
    "    use_label_encoder=False # Suppress warning\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_basic.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_basic.predict(X_test_encoded)\n",
    "y_pred_proba = xgb_basic.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Basic XGBoost Model Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Retained', 'Predicted Departed'],\n",
    "    y=['Actual Retained', 'Actual Departed'],\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont=dict(size=20),\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix: Basic XGBoost Model',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding Key Parameters\n",
    "\n",
    "XGBoost has many hyperparameters. Here are the most important ones for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key XGBoost parameters explained\n",
    "params_explanation = {\n",
    "    'Parameter': [\n",
    "        'n_estimators',\n",
    "        'max_depth',\n",
    "        'learning_rate (eta)',\n",
    "        'subsample',\n",
    "        'colsample_bytree',\n",
    "        'min_child_weight',\n",
    "        'gamma',\n",
    "        'reg_alpha',\n",
    "        'reg_lambda',\n",
    "        'scale_pos_weight'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Number of boosting rounds (trees)',\n",
    "        'Maximum depth of each tree',\n",
    "        'Step size shrinkage (learning rate)',\n",
    "        'Fraction of samples used per tree',\n",
    "        'Fraction of features used per tree',\n",
    "        'Minimum sum of instance weight in a leaf',\n",
    "        'Minimum loss reduction for split',\n",
    "        'L1 regularization (Lasso)',\n",
    "        'L2 regularization (Ridge)',\n",
    "        'Balance of positive/negative weights'\n",
    "    ],\n",
    "    'Typical Range': [\n",
    "        '100-1000',\n",
    "        '3-10',\n",
    "        '0.01-0.3',\n",
    "        '0.5-1.0',\n",
    "        '0.5-1.0',\n",
    "        '1-10',\n",
    "        '0-5',\n",
    "        '0-1',\n",
    "        '0-1',\n",
    "        '1 or (neg/pos) ratio'\n",
    "    ],\n",
    "    'Effect': [\n",
    "        'More = more complex, may overfit',\n",
    "        'Higher = more complex, may overfit',\n",
    "        'Lower = more regularization',\n",
    "        'Lower = more regularization',\n",
    "        'Lower = more regularization',\n",
    "        'Higher = more conservative',\n",
    "        'Higher = more conservative',\n",
    "        'Higher = more regularization (sparse)',\n",
    "        'Higher = more regularization',\n",
    "        'Handles class imbalance'\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_df = pd.DataFrame(params_explanation)\n",
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of key parameters\n",
    "# Compare different learning rates\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=lr,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    train_score = roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1])\n",
    "    test_score = roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1])\n",
    "    \n",
    "    results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Train AUC': train_score,\n",
    "        'Test AUC': test_score,\n",
    "        'Gap (Overfit)': train_score - test_score\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df['Learning Rate'],\n",
    "    y=results_df['Train AUC'],\n",
    "    mode='lines+markers',\n",
    "    name='Train AUC',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df['Learning Rate'],\n",
    "    y=results_df['Test AUC'],\n",
    "    mode='lines+markers',\n",
    "    name='Test AUC',\n",
    "    line=dict(color='green', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Effect of Learning Rate on Model Performance',\n",
    "    xaxis_title='Learning Rate',\n",
    "    yaxis_title='ROC-AUC Score',\n",
    "    height=400,\n",
    "    xaxis_type='log'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nLearning Rate Comparison:\")\n",
    "results_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost with Class Weights\n",
    "\n",
    "Student departure prediction often involves imbalanced classes (e.g., 20-30% departure rate). XGBoost provides `scale_pos_weight` to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class imbalance ratio\n",
    "negative_count = (y_train == 0).sum()\n",
    "positive_count = (y_train == 1).sum()\n",
    "scale_pos_weight = negative_count / positive_count\n",
    "\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(f\"  Retained (0): {negative_count} ({negative_count/len(y_train):.1%})\")\n",
    "print(f\"  Departed (1): {positive_count} ({positive_count/len(y_train):.1%})\")\n",
    "print(f\"\\nScale positive weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with and without class weighting\n",
    "\n",
    "# Without class weighting\n",
    "xgb_unweighted = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb_unweighted.fit(X_train_encoded, y_train)\n",
    "\n",
    "# With class weighting\n",
    "xgb_weighted = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb_weighted.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Compare predictions\n",
    "y_pred_unweighted = xgb_unweighted.predict(X_test_encoded)\n",
    "y_pred_weighted = xgb_weighted.predict(X_test_encoded)\n",
    "\n",
    "print(\"Comparison: Unweighted vs Weighted XGBoost\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<15} {'Unweighted':>12} {'Weighted':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<15} {accuracy_score(y_test, y_pred_unweighted):>12.3f} {accuracy_score(y_test, y_pred_weighted):>12.3f}\")\n",
    "print(f\"{'Precision':<15} {precision_score(y_test, y_pred_unweighted):>12.3f} {precision_score(y_test, y_pred_weighted):>12.3f}\")\n",
    "print(f\"{'Recall':<15} {recall_score(y_test, y_pred_unweighted):>12.3f} {recall_score(y_test, y_pred_weighted):>12.3f}\")\n",
    "print(f\"{'F1 Score':<15} {f1_score(y_test, y_pred_unweighted):>12.3f} {f1_score(y_test, y_pred_weighted):>12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference in confusion matrices\n",
    "cm_unweighted = confusion_matrix(y_test, y_pred_unweighted)\n",
    "cm_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Unweighted', 'Weighted (scale_pos_weight)'))\n",
    "\n",
    "for col, (cm, title) in enumerate([(cm_unweighted, 'Unweighted'), (cm_weighted, 'Weighted')], 1):\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=cm,\n",
    "        x=['Pred Retained', 'Pred Departed'],\n",
    "        y=['Actual Retained', 'Actual Departed'],\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        textfont=dict(size=16),\n",
    "        colorscale='Blues',\n",
    "        showscale=False\n",
    "    ), row=1, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Effect of Class Weighting on Predictions',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Class weighting typically improves **recall** (identifying more actual departures) at the cost of **precision** (more false positives). This trade-off depends on your institutional priorities:\n",
    "- High recall: Catch more at-risk students (some false alarms)\n",
    "- High precision: Only intervene when confident (may miss some at-risk students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost in scikit-learn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocessing Pipeline\n",
    "\n",
    "Using pipelines ensures consistent preprocessing between training and prediction, and enables proper cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for different column types\n",
    "\n",
    "# Numerical preprocessing: StandardScaler (optional for tree models, but good practice)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical preprocessing: One-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created!\")\n",
    "print(f\"  Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"  Categorical columns ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Full Pipeline with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline with XGBoost\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline (using original X_train, not encoded)\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_pipeline = xgb_pipeline.predict(X_test)\n",
    "y_pred_proba_pipeline = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"XGBoost Pipeline Performance\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_pipeline):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_pipeline):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_pipeline):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_pipeline):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_pipeline):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline can now be used on new data directly\n",
    "# Simulating a new student\n",
    "new_student = pd.DataFrame({\n",
    "    'MATH_PLACEMENT': ['College-Ready'],\n",
    "    'FIRST_GEN': ['Yes'],\n",
    "    'PELL_ELIGIBLE': ['Yes'],\n",
    "    'RESIDENCY': ['In-State'],\n",
    "    'HS_GPA': [3.2],\n",
    "    'UNITS_ATTEMPT_1': [15],\n",
    "    'GPA_1': [2.4],\n",
    "    'DFW_RATE_1': [0.2],\n",
    "    'UNITS_ATTEMPT_2': [14],\n",
    "    'GPA_2': [2.6],\n",
    "    'DFW_RATE_2': [0.15],\n",
    "    'CUM_GPA': [2.5],\n",
    "    'CUM_UNITS': [29],\n",
    "    'AVG_DFW': [0.175]\n",
    "})\n",
    "\n",
    "# Predict using pipeline\n",
    "departure_prob = xgb_pipeline.predict_proba(new_student)[0, 1]\n",
    "prediction = 'Departed' if xgb_pipeline.predict(new_student)[0] == 1 else 'Retained'\n",
    "\n",
    "print(\"New Student Prediction\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Predicted outcome: {prediction}\")\n",
    "print(f\"Departure probability: {departure_prob:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-Validation with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation with the full pipeline\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate on multiple metrics\n",
    "cv_accuracy = cross_val_score(xgb_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "cv_roc_auc = cross_val_score(xgb_pipeline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "cv_f1 = cross_val_score(xgb_pipeline, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<12} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<12} {cv_accuracy.mean():>10.3f} {cv_accuracy.std():>10.3f} {cv_accuracy.min():>10.3f} {cv_accuracy.max():>10.3f}\")\n",
    "print(f\"{'ROC-AUC':<12} {cv_roc_auc.mean():>10.3f} {cv_roc_auc.std():>10.3f} {cv_roc_auc.min():>10.3f} {cv_roc_auc.max():>10.3f}\")\n",
    "print(f\"{'F1 Score':<12} {cv_f1.mean():>10.3f} {cv_f1.std():>10.3f} {cv_f1.min():>10.3f} {cv_f1.max():>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "cv_results = pd.DataFrame({\n",
    "    'Fold': list(range(1, 6)) * 3,\n",
    "    'Score': list(cv_accuracy) + list(cv_roc_auc) + list(cv_f1),\n",
    "    'Metric': ['Accuracy'] * 5 + ['ROC-AUC'] * 5 + ['F1 Score'] * 5\n",
    "})\n",
    "\n",
    "fig = px.box(cv_results, x='Metric', y='Score', color='Metric',\n",
    "             title='Cross-Validation Score Distribution by Metric',\n",
    "             points='all')\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Built-in Feature Importance\n",
    "\n",
    "XGBoost provides feature importance scores that indicate how useful each feature was in constructing the boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained XGBoost classifier from the pipeline\n",
    "xgb_model = xgb_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "# For numerical features, names stay the same\n",
    "# For categorical features, get the names from the OneHotEncoder\n",
    "cat_encoder = xgb_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(categorical_cols).tolist()\n",
    "\n",
    "all_feature_names = numerical_cols + cat_feature_names\n",
    "\n",
    "# Get feature importances (default is 'weight' - number of times feature is used)\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Plot feature importance\n",
    "fig = px.bar(importance_df.tail(15), x='Importance', y='Feature', orientation='h',\n",
    "             title='XGBoost Feature Importance (Top 15)',\n",
    "             color='Importance', color_continuous_scale='Blues')\n",
    "\n",
    "fig.update_layout(height=500, yaxis_title='', xaxis_title='Importance Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Importance Types\n",
    "\n",
    "XGBoost supports multiple importance types:\n",
    "\n",
    "- **weight**: Number of times a feature appears in all trees\n",
    "- **gain**: Average gain (improvement in accuracy) when the feature is used\n",
    "- **cover**: Average number of samples affected by splits on the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different importance types using the booster\n",
    "booster = xgb_model.get_booster()\n",
    "\n",
    "# Get importance scores for each type\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "importance_data = []\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    scores = booster.get_score(importance_type=imp_type)\n",
    "    # XGBoost uses f0, f1, f2... for feature names\n",
    "    for i, feat in enumerate(all_feature_names):\n",
    "        feat_key = f'f{i}'\n",
    "        score = scores.get(feat_key, 0)\n",
    "        importance_data.append({\n",
    "            'Feature': feat,\n",
    "            'Type': imp_type,\n",
    "            'Score': score\n",
    "        })\n",
    "\n",
    "importance_compare_df = pd.DataFrame(importance_data)\n",
    "\n",
    "# Pivot for comparison\n",
    "pivot_df = importance_compare_df.pivot(index='Feature', columns='Type', values='Score').fillna(0)\n",
    "\n",
    "# Normalize each column to [0, 1] for comparison\n",
    "pivot_normalized = pivot_df.div(pivot_df.max())\n",
    "\n",
    "# Get top 10 by gain (usually most informative)\n",
    "top_features = pivot_df.nlargest(10, 'gain').index.tolist()\n",
    "\n",
    "# Create comparison plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=imp_type.capitalize(),\n",
    "        x=top_features,\n",
    "        y=pivot_normalized.loc[top_features, imp_type]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance by Different Metrics (Top 10 by Gain)',\n",
    "    xaxis_title='Feature',\n",
    "    yaxis_title='Normalized Importance',\n",
    "    barmode='group',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance table\n",
    "print(\"Feature Importance Comparison (Top 10 by Gain)\")\n",
    "print(\"=\" * 60)\n",
    "pivot_df.loc[top_features].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Importance Types**:\n",
    "\n",
    "| Type | Interpretation | Use Case |\n",
    "|:-----|:---------------|:---------|\n",
    "| **Weight** | How often feature is used in splits | Identifies frequently used features |\n",
    "| **Gain** | Average improvement when feature is used | Best for identifying predictive power |\n",
    "| **Cover** | Number of samples affected | Shows feature reach/coverage |\n",
    "\n",
    "For understanding which features drive predictions, **gain** is typically most informative. However, for a complete picture of feature importance, we recommend using **SHAP values** (covered in notebook 4.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **XGBoost Basics**:\n",
    "   - XGBClassifier provides a scikit-learn compatible interface\n",
    "   - Built-in regularization (L1, L2) prevents overfitting\n",
    "   - Handles missing values natively\n",
    "\n",
    "2. **Key Parameters**:\n",
    "   - `n_estimators`: Number of boosting rounds\n",
    "   - `max_depth`: Controls tree complexity\n",
    "   - `learning_rate`: Shrinkage factor (lower = more regularization)\n",
    "   - `scale_pos_weight`: Handles class imbalance\n",
    "\n",
    "3. **Pipeline Integration**:\n",
    "   - XGBoost works seamlessly with sklearn pipelines\n",
    "   - Pipelines ensure consistent preprocessing\n",
    "   - Cross-validation validates the entire workflow\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - Multiple importance types: weight, gain, cover\n",
    "   - Gain is most informative for predictive power\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Aspect | Key Point |\n",
    "|:-------|:----------|\n",
    "| API | XGBClassifier is sklearn-compatible |\n",
    "| Regularization | L1 (reg_alpha) and L2 (reg_lambda) |\n",
    "| Class Imbalance | Use scale_pos_weight |\n",
    "| Pipeline | Combine with ColumnTransformer |\n",
    "| Feature Importance | Use 'gain' for predictive power |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will explore **LightGBM** and **CatBoost** as alternatives to XGBoost, each with unique strengths.\n",
    "\n",
    "**Proceed to:** `4.3 Build LightGBM and CatBoost Models`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}