{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 **Evaluate** and Tune Decision Trees - Predict Student Departure with Optimized Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the pipeline with decision tree classifier.  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### **4. Evaluate the Model : Assess performance using evaluation metrics.**  \n",
    "### **5. Improve the Model : Tune hyperparameters for optimal performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebooks, we built and trained decision tree models. Now we complete the ML cycle by evaluating model performance and tuning hyperparameters to optimize results.\n",
    "\n",
    "Decision trees have several hyperparameters that control model complexity. Finding the right balance is crucial: too simple and the model underfits, too complex and it overfits.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Evaluate decision tree performance using classification metrics\n",
    "2. Understand the role of key hyperparameters in controlling overfitting\n",
    "3. Use GridSearchCV to find optimal hyperparameter values\n",
    "4. Compare decision trees to logistic regression\n",
    "5. Select and save the best model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc, precision_recall_curve, roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "models_path = f'{course3_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(f\"Testing data shape: {df_testing.shape}\")\n",
    "print(f\"\\nTraining Target distribution:\")\n",
    "print(df_training['SEM_3_STATUS'].value_counts(normalize=True))\n",
    "print(f\"\\nTesting Target distribution:\")\n",
    "print(df_testing['SEM_3_STATUS'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrices and targets\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "X_test = df_testing\n",
    "y_test = df_testing['SEM_3_STATUS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained decision tree models\n",
    "basic_dt_model = pickle.load(open(f'{models_path}basic_decision_tree_trained.pkl', 'rb'))\n",
    "constrained_dt_model = pickle.load(open(f'{models_path}constrained_decision_tree_trained.pkl', 'rb'))\n",
    "balanced_dt_model = pickle.load(open(f'{models_path}balanced_decision_tree_trained.pkl', 'rb'))\n",
    "\n",
    "# Load feature names\n",
    "feature_names_dict = pickle.load(open(f'{models_path}decision_tree_feature_names.pkl', 'rb'))\n",
    "feature_names = feature_names_dict['feature_names']\n",
    "\n",
    "print(\"Trained models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store models for comparison\n",
    "models = {\n",
    "    'Basic (Unconstrained)': basic_dt_model,\n",
    "    'Constrained (max_depth=5)': constrained_dt_model,\n",
    "    'Balanced (class_weight)': balanced_dt_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Evaluation\n",
    "\n",
    "Let's evaluate our trained models on the test set using the metrics we learned in Course 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all models\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    probabilities[name] = model.predict_proba(X_test)[:, 1]  # Probability of 'N' class\n",
    "    \n",
    "print(\"Predictions generated for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['E', 'N'])\n",
    "    disp.plot(ax=axes[idx], colorbar=False)\n",
    "    axes[idx].set_title(f'{name}')\n",
    "\n",
    "plt.suptitle('Confusion Matrices on Test Set', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate classification metrics.\"\"\"\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision (N)': precision_score(y_true, y_pred, pos_label='N'),\n",
    "        'Recall (N)': recall_score(y_true, y_pred, pos_label='N'),\n",
    "        'F1-Score (N)': f1_score(y_true, y_pred, pos_label='N'),\n",
    "        'AUC-ROC': roc_auc_score(y_true, y_prob, labels=['E', 'N'])\n",
    "    }\n",
    "\n",
    "# Calculate for all models\n",
    "metrics_results = []\n",
    "for name, preds in predictions.items():\n",
    "    metrics = calculate_metrics(y_test, preds, probabilities[name])\n",
    "    metrics['Model'] = name\n",
    "    metrics_results.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "metrics_df = metrics_df[['Model', 'Accuracy', 'Precision (N)', 'Recall (N)', 'F1-Score (N)', 'AUC-ROC']]\n",
    "\n",
    "# Add null rate for reference\n",
    "null_rate = y_test.value_counts(normalize=True).max()\n",
    "print(f\"Null Rate (baseline accuracy): {null_rate:.1%}\")\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision (N)', 'Recall (N)', 'F1-Score (N)', 'AUC-ROC']\n",
    "colors = ['coral', 'steelblue', 'seagreen']\n",
    "\n",
    "for i, row in metrics_df.iterrows():\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=row['Model'],\n",
    "        x=metrics_to_plot,\n",
    "        y=[row[m] for m in metrics_to_plot],\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.add_hline(y=null_rate, line_dash=\"dash\", line_color=\"gray\", \n",
    "              annotation_text=f\"Null Rate: {null_rate:.1%}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decision Tree Model Performance Comparison',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('ROC Curves', 'Precision-Recall Curves'))\n",
    "\n",
    "colors = ['coral', 'steelblue', 'seagreen']\n",
    "\n",
    "for idx, (name, probs) in enumerate(probabilities.items()):\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs, pos_label='N')\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr, mode='lines',\n",
    "        name=f'{name} (AUC={roc_auc:.2f})',\n",
    "        line=dict(color=colors[idx], width=2)\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probs, pos_label='N')\n",
    "    pr_auc = auc(recall, precision)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=recall, y=precision, mode='lines',\n",
    "        name=f'{name} (AUC={pr_auc:.2f})',\n",
    "        line=dict(color=colors[idx], width=2),\n",
    "        showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "# Add diagonal for ROC\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1], mode='lines',\n",
    "    line=dict(dash='dash', color='gray'),\n",
    "    name='Random',\n",
    "    showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# Add baseline for PR\n",
    "baseline_precision = (y_test == 'N').mean()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[baseline_precision, baseline_precision], mode='lines',\n",
    "    line=dict(dash='dash', color='gray'),\n",
    "    name='Baseline',\n",
    "    showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='False Positive Rate', row=1, col=1)\n",
    "fig.update_yaxes(title_text='True Positive Rate', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Recall', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Precision', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=450, title_text='ROC and Precision-Recall Curves')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "Now let's tune the decision tree hyperparameters to find the optimal configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Understanding Key Hyperparameters\n",
    "\n",
    "The main hyperparameters that control decision tree complexity are:\n",
    "\n",
    "| Parameter | Description | Effect on Model |\n",
    "|:----------|:------------|:----------------|\n",
    "| `max_depth` | Maximum depth of tree | Lower = simpler, reduces variance |\n",
    "| `min_samples_split` | Minimum samples to split a node | Higher = simpler, reduces variance |\n",
    "| `min_samples_leaf` | Minimum samples in leaf nodes | Higher = simpler, reduces variance |\n",
    "| `max_leaf_nodes` | Maximum number of leaf nodes | Lower = simpler, reduces variance |\n",
    "| `class_weight` | Weights for classes | 'balanced' helps with imbalanced data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of max_depth on model performance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create preprocessor (same as in 2.2)\n",
    "numerical_columns = [\n",
    "    'HS_GPA', 'GPA_1', 'GPA_2', 'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "categorical_columns = ['GENDER', 'RACE_ETHNICITY', 'FIRST_GEN_STATUS']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', \n",
    "                              drop=['Female', 'Other', 'Unknown'], \n",
    "                              sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Test different max_depth values\n",
    "depths = [1, 2, 3, 4, 5, 6, 7, 8, 10, 15, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(\n",
    "            max_depth=depth,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_scores.append(f1_score(y_train, model.predict(X_train), pos_label='N'))\n",
    "    test_scores.append(f1_score(y_test, model.predict(X_test), pos_label='N'))\n",
    "\n",
    "# Convert None to string for plotting\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve for max_depth\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=depth_labels, y=train_scores,\n",
    "    mode='lines+markers',\n",
    "    name='Training F1',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=depth_labels, y=test_scores,\n",
    "    mode='lines+markers',\n",
    "    name='Test F1',\n",
    "    line=dict(color='orange', width=2)\n",
    "))\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=depth_labels[-1], y=train_scores[-1],\n",
    "    text='Overfitting Zone',\n",
    "    showarrow=True,\n",
    "    arrowhead=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation Curve: Effect of max_depth on F1 Score (Class N)',\n",
    "    xaxis_title='max_depth',\n",
    "    yaxis_title='F1 Score',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Notice the gap between training and test performance as depth increases. This is the classic sign of overfitting - the model memorizes training data but fails to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Grid Search with Cross-Validation\n",
    "\n",
    "We'll use GridSearchCV to systematically search for the best hyperparameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'classifier__min_samples_split': [10, 20, 30, 50],\n",
    "    'classifier__min_samples_leaf': [5, 10, 15, 20],\n",
    "    'classifier__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "print(\"Parameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nTotal combinations to test: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base pipeline for tuning\n",
    "base_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Running Grid Search (this may take a moment)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    base_pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='f1',  # Optimize for F1 score of positive class\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Note: scikit-learn needs binary labels for f1 scoring, let's convert\n",
    "y_train_binary = (y_train == 'N').astype(int)\n",
    "y_test_binary = (y_test == 'N').astype(int)\n",
    "\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "\n",
    "print(\"Grid Search Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation F1 Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyzing Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Get top 10 configurations\n",
    "top_10 = results_df.nsmallest(10, 'rank_test_score')[[\n",
    "    'param_classifier__max_depth',\n",
    "    'param_classifier__min_samples_split',\n",
    "    'param_classifier__min_samples_leaf',\n",
    "    'param_classifier__class_weight',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score'\n",
    "]]\n",
    "\n",
    "top_10.columns = ['max_depth', 'min_samples_split', 'min_samples_leaf', \n",
    "                  'class_weight', 'CV F1 (mean)', 'CV F1 (std)', 'Train F1']\n",
    "\n",
    "print(\"Top 10 Hyperparameter Configurations:\")\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Effect of max_depth', 'Effect of min_samples_split',\n",
    "    'Effect of min_samples_leaf', 'Effect of class_weight'\n",
    "))\n",
    "\n",
    "# Group by each parameter and calculate mean score\n",
    "for idx, param in enumerate(['max_depth', 'min_samples_split', 'min_samples_leaf', 'class_weight']):\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    grouped = results_df.groupby(f'param_classifier__{param}')['mean_test_score'].mean()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[str(x) for x in grouped.index],\n",
    "        y=grouped.values,\n",
    "        marker_color='steelblue'\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=600, title_text='Hyperparameter Effects on Cross-Validation F1 Score',\n",
    "                  showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions (need to convert back to original labels for consistency)\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "y_prob_tuned = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_tuned_labels = np.where(y_pred_tuned == 1, 'N', 'E')\n",
    "\n",
    "print(\"Tuned Model Evaluation on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_tuned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for tuned model\n",
    "tuned_metrics = {\n",
    "    'Model': 'Tuned Decision Tree',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned_labels),\n",
    "    'Precision (N)': precision_score(y_test, y_pred_tuned_labels, pos_label='N'),\n",
    "    'Recall (N)': recall_score(y_test, y_pred_tuned_labels, pos_label='N'),\n",
    "    'F1-Score (N)': f1_score(y_test, y_pred_tuned_labels, pos_label='N'),\n",
    "    'AUC-ROC': roc_auc_score(y_test_binary, y_prob_tuned)\n",
    "}\n",
    "\n",
    "# Add to comparison\n",
    "metrics_results.append(tuned_metrics)\n",
    "metrics_df_updated = pd.DataFrame(metrics_results)\n",
    "metrics_df_updated = metrics_df_updated[['Model', 'Accuracy', 'Precision (N)', 'Recall (N)', 'F1-Score (N)', 'AUC-ROC']]\n",
    "\n",
    "print(\"\\nUpdated Model Comparison:\")\n",
    "print(metrics_df_updated.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for tuned model\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tuned, display_labels=['E', 'N'])\n",
    "disp.plot(colorbar=False)\n",
    "plt.title('Confusion Matrix: Tuned Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Decision Trees vs Logistic Regression\n",
    "\n",
    "Let's compare our best decision tree to the logistic regression models from Course 2 and Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline logistic regression for comparison\n",
    "try:\n",
    "    baseline_lr = pickle.load(open(f'{root_filepath}models/baseline_logistic_model.pkl', 'rb'))\n",
    "    baseline_lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    lr_pred = baseline_lr.predict(X_test)\n",
    "    lr_prob = baseline_lr.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    lr_metrics = {\n",
    "        'Model': 'Logistic Regression (Baseline)',\n",
    "        'Accuracy': accuracy_score(y_test, lr_pred),\n",
    "        'Precision (N)': precision_score(y_test, lr_pred, pos_label='N'),\n",
    "        'Recall (N)': recall_score(y_test, lr_pred, pos_label='N'),\n",
    "        'F1-Score (N)': f1_score(y_test, lr_pred, pos_label='N'),\n",
    "        'AUC-ROC': roc_auc_score(y_test, lr_prob, labels=['E', 'N'])\n",
    "    }\n",
    "    \n",
    "    print(\"Logistic Regression (Baseline) loaded and evaluated.\")\n",
    "except:\n",
    "    print(\"Note: Could not load baseline logistic regression model.\")\n",
    "    lr_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comparison_data = [\n",
    "    {'Model': 'Basic DT', 'Accuracy': metrics_df_updated.loc[0, 'Accuracy'], \n",
    "     'F1 (N)': metrics_df_updated.loc[0, 'F1-Score (N)'], \n",
    "     'AUC-ROC': metrics_df_updated.loc[0, 'AUC-ROC'], 'Type': 'Decision Tree'},\n",
    "    {'Model': 'Constrained DT', 'Accuracy': metrics_df_updated.loc[1, 'Accuracy'], \n",
    "     'F1 (N)': metrics_df_updated.loc[1, 'F1-Score (N)'], \n",
    "     'AUC-ROC': metrics_df_updated.loc[1, 'AUC-ROC'], 'Type': 'Decision Tree'},\n",
    "    {'Model': 'Balanced DT', 'Accuracy': metrics_df_updated.loc[2, 'Accuracy'], \n",
    "     'F1 (N)': metrics_df_updated.loc[2, 'F1-Score (N)'], \n",
    "     'AUC-ROC': metrics_df_updated.loc[2, 'AUC-ROC'], 'Type': 'Decision Tree'},\n",
    "    {'Model': 'Tuned DT', 'Accuracy': metrics_df_updated.loc[3, 'Accuracy'], \n",
    "     'F1 (N)': metrics_df_updated.loc[3, 'F1-Score (N)'], \n",
    "     'AUC-ROC': metrics_df_updated.loc[3, 'AUC-ROC'], 'Type': 'Decision Tree'},\n",
    "]\n",
    "\n",
    "if lr_metrics:\n",
    "    comparison_data.append({\n",
    "        'Model': 'Logistic Reg', 'Accuracy': lr_metrics['Accuracy'],\n",
    "        'F1 (N)': lr_metrics['F1-Score (N)'], 'AUC-ROC': lr_metrics['AUC-ROC'],\n",
    "        'Type': 'Logistic Regression'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nComprehensive Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {'Decision Tree': 'steelblue', 'Logistic Regression': 'coral'}\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='F1 Score (N)',\n",
    "    x=comparison_df['Model'],\n",
    "    y=comparison_df['F1 (N)'],\n",
    "    marker_color=[colors[t] for t in comparison_df['Type']]\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    name='AUC-ROC',\n",
    "    x=comparison_df['Model'],\n",
    "    y=comparison_df['AUC-ROC'],\n",
    "    mode='markers+lines',\n",
    "    marker=dict(size=12, color='green'),\n",
    "    line=dict(color='green', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison: Decision Trees vs Logistic Regression',\n",
    "    yaxis_title='Score',\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training vs test performance for each model\n",
    "def get_train_test_gap(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate train/test performance gap.\"\"\"\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_f1 = f1_score(y_train, train_pred, pos_label='N')\n",
    "    test_f1 = f1_score(y_test, test_pred, pos_label='N')\n",
    "    \n",
    "    return train_f1, test_f1, train_f1 - test_f1\n",
    "\n",
    "overfit_analysis = []\n",
    "for name, model in models.items():\n",
    "    train_f1, test_f1, gap = get_train_test_gap(model, X_train, y_train, X_test, y_test)\n",
    "    overfit_analysis.append({\n",
    "        'Model': name,\n",
    "        'Train F1': train_f1,\n",
    "        'Test F1': test_f1,\n",
    "        'Gap (Overfitting)': gap\n",
    "    })\n",
    "\n",
    "# Add tuned model (convert labels back)\n",
    "train_pred_tuned = best_model.predict(X_train)\n",
    "train_pred_tuned_labels = np.where(train_pred_tuned == 1, 'N', 'E')\n",
    "train_f1_tuned = f1_score(y_train, train_pred_tuned_labels, pos_label='N')\n",
    "test_f1_tuned = f1_score(y_test, y_pred_tuned_labels, pos_label='N')\n",
    "\n",
    "overfit_analysis.append({\n",
    "    'Model': 'Tuned Decision Tree',\n",
    "    'Train F1': train_f1_tuned,\n",
    "    'Test F1': test_f1_tuned,\n",
    "    'Gap (Overfitting)': train_f1_tuned - test_f1_tuned\n",
    "})\n",
    "\n",
    "overfit_df = pd.DataFrame(overfit_analysis)\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "print(overfit_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Train F1',\n",
    "    x=overfit_df['Model'],\n",
    "    y=overfit_df['Train F1'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test F1',\n",
    "    x=overfit_df['Model'],\n",
    "    y=overfit_df['Test F1'],\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training vs Test Performance (Overfitting Analysis)',\n",
    "    yaxis_title='F1 Score',\n",
    "    barmode='group',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: A large gap between training and test performance indicates overfitting. The basic (unconstrained) tree has the largest gap, while the tuned model has a smaller gap, indicating better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final recommendation table\n",
    "final_comparison = metrics_df_updated.copy()\n",
    "final_comparison['Interpretability'] = ['Medium', 'High', 'High', 'High']\n",
    "final_comparison['Overfitting Risk'] = ['High', 'Low', 'Low', 'Low']\n",
    "final_comparison['Recommended For'] = [\n",
    "    'Not recommended',\n",
    "    'General use',\n",
    "    'Minority class focus',\n",
    "    'Optimal performance'\n",
    "]\n",
    "\n",
    "print(\"\\nFinal Model Comparison and Recommendations:\")\n",
    "print(final_comparison[['Model', 'F1-Score (N)', 'AUC-ROC', 'Interpretability', 'Overfitting Risk', 'Recommended For']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best model configuration\n",
    "print(\"\\nBest Model Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: Tuned Decision Tree\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "\n",
    "print(f\"\\nPerformance on Test Set:\")\n",
    "print(f\"  - F1 Score (Class N): {test_f1_tuned:.3f}\")\n",
    "print(f\"  - AUC-ROC: {tuned_metrics['AUC-ROC']:.3f}\")\n",
    "print(f\"  - Recall (Class N): {tuned_metrics['Recall (N)']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "# First, retrain on original labels for production use\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "final_model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        max_depth=best_params['classifier__max_depth'],\n",
    "        min_samples_split=best_params['classifier__min_samples_split'],\n",
    "        min_samples_leaf=best_params['classifier__min_samples_leaf'],\n",
    "        class_weight=best_params['classifier__class_weight'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "filepath = f'{models_path}tuned_decision_tree_final.pkl'\n",
    "pickle.dump(final_model, open(filepath, 'wb'))\n",
    "print(f\"Saved tuned model to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved model\n",
    "loaded_model = pickle.load(open(filepath, 'rb'))\n",
    "verify_pred = loaded_model.predict(X_test)\n",
    "verify_f1 = f1_score(y_test, verify_pred, pos_label='N')\n",
    "print(f\"Verification - Test F1 Score: {verify_f1:.3f}\")\n",
    "print(\"Model saved and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this notebook, we completed the ML cycle for decision trees by evaluating and tuning our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary_data = {\n",
    "    'Metric': ['Best F1 Score (N)', 'Best AUC-ROC', 'Best Recall (N)', \n",
    "               'Optimal max_depth', 'Optimal min_samples_split', \n",
    "               'Optimal min_samples_leaf', 'Optimal class_weight'],\n",
    "    'Value': [\n",
    "        f\"{test_f1_tuned:.3f}\",\n",
    "        f\"{tuned_metrics['AUC-ROC']:.3f}\",\n",
    "        f\"{tuned_metrics['Recall (N)']:.3f}\",\n",
    "        str(best_params['classifier__max_depth']),\n",
    "        str(best_params['classifier__min_samples_split']),\n",
    "        str(best_params['classifier__min_samples_leaf']),\n",
    "        str(best_params['classifier__class_weight'])\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "| Topic | Key Learning |\n",
    "|:------|:-------------|\n",
    "| **Hyperparameter Tuning** | max_depth, min_samples_split, min_samples_leaf control complexity |\n",
    "| **Grid Search** | Systematic search finds optimal hyperparameter combinations |\n",
    "| **Cross-Validation** | Essential for reliable hyperparameter selection |\n",
    "| **Overfitting** | Unconstrained trees overfit; constraints improve generalization |\n",
    "| **Class Imbalance** | class_weight='balanced' helps identify minority class |\n",
    "\n",
    "### ML Cycle Summary for Decision Trees\n",
    "\n",
    "| Step | What We Did |\n",
    "|:-----|:------------|\n",
    "| **1. Build** | Created pipelines with DecisionTreeClassifier |\n",
    "| **2. Train** | Fit trees using recursive partitioning |\n",
    "| **3. Predict** | Generated predictions using tree rules |\n",
    "| **4. Evaluate** | Assessed using accuracy, F1, AUC, confusion matrices |\n",
    "| **5. Improve** | Tuned hyperparameters with GridSearchCV |\n",
    "\n",
    "### When to Use Decision Trees\n",
    "\n",
    "| Use Decision Trees When... | Consider Alternatives When... |\n",
    "|:---------------------------|:------------------------------|\n",
    "| Interpretability is critical | Maximum predictive accuracy needed |\n",
    "| Stakeholders need explanations | Smooth probability estimates needed |\n",
    "| Feature interactions matter | Linear relationships dominate |\n",
    "| Building ensemble methods | Few features available |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we will explore **ensemble methods** that combine multiple decision trees:\n",
    "- **Random Forests**: Reduce variance through bagging\n",
    "- **Gradient Boosting**: Reduce bias through boosting\n",
    "\n",
    "These methods often achieve better predictive performance while maintaining some interpretability.\n",
    "\n",
    "**Proceed to:** `Module 3: Ensemble Methods`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}