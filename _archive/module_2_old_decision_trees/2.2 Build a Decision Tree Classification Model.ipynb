{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 **Build** a Decision Tree Classification Model - Predict Student Departure with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### **1. Build the Model : Create the pipeline with decision tree classifier.**  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we learned the theory behind decision trees. Now we will put that knowledge into practice by building decision tree classification models using scikit-learn. We will create pipelines that can be trained on our student departure prediction data.\n",
    "\n",
    "Unlike logistic regression, decision trees have minimal preprocessing requirements. However, we will still use a pipeline approach to maintain consistency and ensure reproducibility.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build decision tree classification pipelines in scikit-learn\n",
    "2. Understand key DecisionTreeClassifier parameters\n",
    "3. Configure trees with different complexity constraints\n",
    "4. Handle class imbalance using class weights\n",
    "5. Save models for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths - using Course 2 data\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_training['SEM_3_STATUS'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix and target\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "print(f\"Features available: {list(df_training.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review: Preprocessing for Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What's Different from Logistic Regression?\n",
    "\n",
    "One of the key advantages of decision trees is their minimal preprocessing requirements:\n",
    "\n",
    "| Preprocessing Step | Logistic Regression | Decision Trees |\n",
    "|:-------------------|:--------------------|:---------------|\n",
    "| Feature Scaling | Required | Not Required |\n",
    "| Categorical Encoding | Required | Required (in sklearn) |\n",
    "| Handling Missing Values | Required | Can handle natively* |\n",
    "| Feature Normalization | Recommended | Not Needed |\n",
    "\n",
    "*Note: scikit-learn's DecisionTreeClassifier does not handle missing values natively. We would need to impute or use other libraries like XGBoost.\n",
    "\n",
    "**Why no scaling?** Decision trees make decisions based on threshold comparisons (e.g., \"Is GPA <= 2.5?\"). The actual scale of the feature doesn't matter - only the relative ordering of values. A GPA of 2.5 on a 0-4 scale works the same as 62.5 on a 0-100 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that scaling doesn't affect decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Simple example\n",
    "np.random.seed(42)\n",
    "X_example = np.random.randn(100, 2)\n",
    "y_example = (X_example[:, 0] + X_example[:, 1] > 0).astype(int)\n",
    "\n",
    "# Unscaled tree\n",
    "tree_unscaled = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_unscaled.fit(X_example, y_example)\n",
    "\n",
    "# Scaled tree (multiply features by different amounts)\n",
    "X_scaled = X_example.copy()\n",
    "X_scaled[:, 0] *= 1000  # Scale first feature by 1000\n",
    "X_scaled[:, 1] *= 0.001  # Scale second feature by 0.001\n",
    "\n",
    "tree_scaled = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_scaled.fit(X_scaled, y_example)\n",
    "\n",
    "# Compare predictions\n",
    "print(\"Predictions match:\", np.all(tree_unscaled.predict(X_example) == tree_scaled.predict(X_scaled)))\n",
    "print(\"\\nThis demonstrates that decision trees are scale-invariant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Setting Up Feature Groups\n",
    "\n",
    "Even though decision trees don't require scaling, we still need to handle categorical variables. We'll use the same feature groupings from Course 2 but with a simpler preprocessing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature groupings from Course 2\n",
    "\n",
    "# Numerical columns (no scaling needed for decision trees)\n",
    "numerical_columns = [\n",
    "    'HS_GPA',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "# Categorical columns for one-hot encoding\n",
    "categorical_columns = [\n",
    "    'GENDER',\n",
    "    'RACE_ETHNICITY',\n",
    "    'FIRST_GEN_STATUS',\n",
    "]\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_columns)}\")\n",
    "print(f\"Categorical features: {len(categorical_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified preprocessor for decision trees\n",
    "# We pass through numerical columns and only encode categoricals\n",
    "\n",
    "preprocessor_dt = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_columns),  # No scaling!\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', \n",
    "                              drop=['Female', 'Other', 'Unknown'], \n",
    "                              sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Decision Tree Preprocessor configured:\")\n",
    "print(\"- Numerical features: passed through unchanged\")\n",
    "print(\"- Categorical features: one-hot encoded with dropped categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Decision Tree Models\n",
    "\n",
    "We will create three decision tree variants:\n",
    "\n",
    "1. **Basic Tree**: Default settings (prone to overfitting)\n",
    "2. **Constrained Tree**: Limited depth to prevent overfitting\n",
    "3. **Balanced Tree**: With class weights to handle imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic Decision Tree\n",
    "\n",
    "This is a baseline decision tree with minimal constraints. It will likely overfit, but serves as a useful comparison point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Decision Tree (default settings)\n",
    "basic_dt_model = Pipeline([\n",
    "    ('preprocessing', preprocessor_dt),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        criterion='gini',       # Impurity measure\n",
    "        max_depth=None,         # No depth limit (will overfit!)\n",
    "        min_samples_split=2,    # Default: minimum samples to split\n",
    "        min_samples_leaf=1,     # Default: minimum samples in leaf\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Basic Decision Tree Model:\")\n",
    "basic_dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree with Depth Constraint\n",
    "\n",
    "Limiting the tree depth is one of the most effective ways to prevent overfitting. A shallower tree is also more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Decision Tree (limited depth)\n",
    "constrained_dt_model = Pipeline([\n",
    "    ('preprocessing', preprocessor_dt),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,            # Limit depth to prevent overfitting\n",
    "        min_samples_split=20,   # Require more samples to split\n",
    "        min_samples_leaf=10,    # Require more samples in leaves\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Constrained Decision Tree Model:\")\n",
    "constrained_dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decision Tree with Balanced Class Weights\n",
    "\n",
    "Our dataset is imbalanced (87% Enrolled, 13% Not Enrolled). Using `class_weight='balanced'` adjusts the importance of each class inversely proportional to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what balanced weights would look like\n",
    "class_counts = y_train.value_counts()\n",
    "n_samples = len(y_train)\n",
    "n_classes = 2\n",
    "\n",
    "# Balanced weights formula: n_samples / (n_classes * n_class_samples)\n",
    "weight_E = n_samples / (n_classes * class_counts['E'])\n",
    "weight_N = n_samples / (n_classes * class_counts['N'])\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"  Enrolled (E): {class_counts['E']} samples ({class_counts['E']/n_samples:.1%})\")\n",
    "print(f\"  Not Enrolled (N): {class_counts['N']} samples ({class_counts['N']/n_samples:.1%})\")\n",
    "print(f\"\\nBalanced Class Weights:\")\n",
    "print(f\"  Weight for E: {weight_E:.3f}\")\n",
    "print(f\"  Weight for N: {weight_N:.3f}\")\n",
    "print(f\"\\nRatio (N/E): {weight_N/weight_E:.2f}x more weight on minority class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Decision Tree (handles class imbalance)\n",
    "balanced_dt_model = Pipeline([\n",
    "    ('preprocessing', preprocessor_dt),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Balanced Decision Tree Model:\")\n",
    "balanced_dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding DecisionTreeClassifier Parameters\n",
    "\n",
    "Let's review the key hyperparameters available in scikit-learn's DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key hyperparameters for DecisionTreeClassifier\n",
    "params_df = pd.DataFrame({\n",
    "    'Parameter': ['criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', \n",
    "                  'max_features', 'max_leaf_nodes', 'min_impurity_decrease', \n",
    "                  'class_weight', 'random_state'],\n",
    "    'Description': [\n",
    "        'Impurity measure (gini or entropy)',\n",
    "        'Maximum depth of tree (None = unlimited)',\n",
    "        'Minimum samples required to split a node',\n",
    "        'Minimum samples required in a leaf node',\n",
    "        'Number of features to consider for best split',\n",
    "        'Maximum number of leaf nodes',\n",
    "        'Minimum impurity decrease required for split',\n",
    "        'Weights for handling class imbalance',\n",
    "        'Random seed for reproducibility'\n",
    "    ],\n",
    "    'Default': ['gini', 'None', '2', '1', 'None', 'None', '0.0', 'None', 'None'],\n",
    "    'Effect': [\n",
    "        'Usually similar results',\n",
    "        'Lower = simpler, less overfit',\n",
    "        'Higher = simpler, less overfit',\n",
    "        'Higher = simpler, less overfit',\n",
    "        'Lower = more randomness',\n",
    "        'Lower = simpler tree',\n",
    "        'Higher = fewer splits',\n",
    "        'balanced = equal class importance',\n",
    "        'Set for reproducibility'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"DecisionTreeClassifier Hyperparameters:\")\n",
    "print(params_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between parameters and model complexity\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create a conceptual diagram of complexity control\n",
    "params = ['max_depth', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes', 'min_impurity_decrease']\n",
    "effects = [\n",
    "    'Controls tree height',\n",
    "    'Prevents splitting small nodes',\n",
    "    'Ensures leaves have enough samples',\n",
    "    'Limits total number of predictions',\n",
    "    'Requires meaningful splits'\n",
    "]\n",
    "\n",
    "# Complexity reduction potential (conceptual)\n",
    "reduction = [5, 4, 4, 5, 3]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=params,\n",
    "    x=reduction,\n",
    "    orientation='h',\n",
    "    marker=dict(color='steelblue'),\n",
    "    text=effects,\n",
    "    textposition='inside',\n",
    "    insidetextanchor='start'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decision Tree Complexity Control Parameters',\n",
    "    xaxis_title='Complexity Reduction Potential (Conceptual)',\n",
    "    yaxis_title='Parameter',\n",
    "    height=400,\n",
    "    xaxis=dict(range=[0, 6])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of models we've built\n",
    "models = {\n",
    "    'Basic (Unconstrained)': basic_dt_model,\n",
    "    'Constrained (max_depth=5)': constrained_dt_model,\n",
    "    'Balanced (class_weight)': balanced_dt_model\n",
    "}\n",
    "\n",
    "print(\"Decision Tree Models Built:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    classifier = model.named_steps['classifier']\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  - criterion: {classifier.criterion}\")\n",
    "    print(f\"  - max_depth: {classifier.max_depth}\")\n",
    "    print(f\"  - min_samples_split: {classifier.min_samples_split}\")\n",
    "    print(f\"  - min_samples_leaf: {classifier.min_samples_leaf}\")\n",
    "    print(f\"  - class_weight: {classifier.class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Basic (Unconstrained)', 'Constrained', 'Balanced'],\n",
    "    'max_depth': ['None', '5', '5'],\n",
    "    'min_samples_split': ['2', '20', '20'],\n",
    "    'min_samples_leaf': ['1', '10', '10'],\n",
    "    'class_weight': ['None', 'None', 'balanced'],\n",
    "    'Expected Behavior': [\n",
    "        'High variance, overfitting',\n",
    "        'Better generalization',\n",
    "        'Better minority class recall'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models for Future Use\n",
    "\n",
    "We save these untrained model pipelines so they can be loaded and trained in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory for Course 3 Module 2 if it doesn't exist\n",
    "models_path = f'{course3_filepath}models/'\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Define model names and pipelines\n",
    "models_to_save = {\n",
    "    'basic_decision_tree_model': basic_dt_model,\n",
    "    'constrained_decision_tree_model': constrained_dt_model,\n",
    "    'balanced_decision_tree_model': balanced_dt_model\n",
    "}\n",
    "\n",
    "# Save each model pipeline\n",
    "for name, model in models_to_save.items():\n",
    "    filepath = f'{models_path}{name}.pkl'\n",
    "    pickle.dump(model, open(filepath, 'wb'))\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved models\n",
    "print(\"\\nVerifying saved models:\")\n",
    "for name in models_to_save.keys():\n",
    "    filepath = f'{models_path}{name}.pkl'\n",
    "    loaded_model = pickle.load(open(filepath, 'rb'))\n",
    "    print(f\"  {name}: {type(loaded_model.named_steps['classifier']).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, we built three decision tree classification models:\n",
    "\n",
    "| Model | Configuration | Purpose |\n",
    "|:------|:--------------|:--------|\n",
    "| **Basic** | No constraints | Baseline (will overfit) |\n",
    "| **Constrained** | max_depth=5, min_samples_* | Prevent overfitting |\n",
    "| **Balanced** | + class_weight='balanced' | Handle class imbalance |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Minimal Preprocessing**: Decision trees don't require feature scaling\n",
    "2. **Categorical Encoding**: Still needed for scikit-learn implementation\n",
    "3. **Complexity Control**: max_depth, min_samples_split, min_samples_leaf\n",
    "4. **Class Imbalance**: Use class_weight='balanced' for imbalanced datasets\n",
    "5. **Pipeline Approach**: Maintains consistency with our logistic regression models\n",
    "\n",
    "### Connection to ML Cycle\n",
    "\n",
    "We are in **Step 1: Build the Model**. We have:\n",
    "- Created preprocessing pipeline for decision trees\n",
    "- Built three model variants with different configurations\n",
    "- Saved models for training\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Train these models on our student departure data\n",
    "2. Visualize the learned decision trees\n",
    "3. Extract and interpret decision rules\n",
    "\n",
    "**Proceed to:** `2.3 Train and Visualize Decision Trees`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}