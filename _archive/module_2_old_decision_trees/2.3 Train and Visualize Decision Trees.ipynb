{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 **Train** and Visualize Decision Trees - Predict Student Departure with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the pipeline with decision tree classifier.  \n",
    "### **2. Train the Model : Fit the model on the training data.**  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we built decision tree pipelines. Now we will train these models on our student departure data and explore one of the key advantages of decision trees: **interpretability through visualization**.\n",
    "\n",
    "Decision trees can be visualized as flowcharts, making them ideal for communicating model logic to non-technical stakeholders like academic advisors, administrators, and faculty.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Train decision tree models on student data\n",
    "2. Visualize trained trees using multiple methods\n",
    "3. Extract human-readable decision rules\n",
    "4. Interpret feature importance from trained trees\n",
    "5. Compare tree structures across different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "models_path = f'{course3_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_training['SEM_3_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix and target\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-built Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the decision tree models we built in notebook 2.2\n",
    "basic_dt_model = pickle.load(open(f'{models_path}basic_decision_tree_model.pkl', 'rb'))\n",
    "constrained_dt_model = pickle.load(open(f'{models_path}constrained_decision_tree_model.pkl', 'rb'))\n",
    "balanced_dt_model = pickle.load(open(f'{models_path}balanced_decision_tree_model.pkl', 'rb'))\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "print(f\"  - Basic Decision Tree\")\n",
    "print(f\"  - Constrained Decision Tree (max_depth=5)\")\n",
    "print(f\"  - Balanced Decision Tree (class_weight='balanced')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Decision Tree Models\n",
    "\n",
    "Training a decision tree involves the recursive partitioning algorithm we discussed in notebook 2.1. The tree grows by finding the best splits until stopping criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the basic (unconstrained) decision tree\n",
    "print(\"Training Basic Decision Tree...\")\n",
    "basic_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the trained classifier\n",
    "basic_tree = basic_dt_model.named_steps['classifier']\n",
    "\n",
    "print(f\"\\nBasic Tree Statistics:\")\n",
    "print(f\"  - Tree depth: {basic_tree.get_depth()}\")\n",
    "print(f\"  - Number of leaves: {basic_tree.get_n_leaves()}\")\n",
    "print(f\"  - Number of features used: {basic_tree.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Notice how deep the unconstrained tree grows! This is a clear sign of overfitting - the tree is memorizing the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Constrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the constrained decision tree\n",
    "print(\"Training Constrained Decision Tree...\")\n",
    "constrained_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the trained classifier\n",
    "constrained_tree = constrained_dt_model.named_steps['classifier']\n",
    "\n",
    "print(f\"\\nConstrained Tree Statistics:\")\n",
    "print(f\"  - Tree depth: {constrained_tree.get_depth()}\")\n",
    "print(f\"  - Number of leaves: {constrained_tree.get_n_leaves()}\")\n",
    "print(f\"  - Number of features used: {constrained_tree.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training the Balanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the balanced decision tree\n",
    "print(\"Training Balanced Decision Tree...\")\n",
    "balanced_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the trained classifier\n",
    "balanced_tree = balanced_dt_model.named_steps['classifier']\n",
    "\n",
    "print(f\"\\nBalanced Tree Statistics:\")\n",
    "print(f\"  - Tree depth: {balanced_tree.get_depth()}\")\n",
    "print(f\"  - Number of leaves: {balanced_tree.get_n_leaves()}\")\n",
    "print(f\"  - Number of features used: {balanced_tree.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tree statistics\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Basic (Unconstrained)', 'Constrained', 'Balanced'],\n",
    "    'Depth': [basic_tree.get_depth(), constrained_tree.get_depth(), balanced_tree.get_depth()],\n",
    "    'Leaves': [basic_tree.get_n_leaves(), constrained_tree.get_n_leaves(), balanced_tree.get_n_leaves()],\n",
    "    'Complexity': ['Very High', 'Moderate', 'Moderate']\n",
    "})\n",
    "\n",
    "print(\"\\nTree Structure Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Decision Trees\n",
    "\n",
    "One of the greatest strengths of decision trees is their interpretability. Let's explore multiple ways to visualize our trained trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Text-based Representation\n",
    "\n",
    "The simplest way to view a decision tree is as text-based rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "# We need to extract the feature names from the preprocessor\n",
    "preprocessor = constrained_dt_model.named_steps['preprocessing']\n",
    "\n",
    "# Get numerical feature names (passed through)\n",
    "numerical_columns = [\n",
    "    'HS_GPA', 'GPA_1', 'GPA_2', 'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "# Get categorical feature names (one-hot encoded)\n",
    "# We need to fit the preprocessor first to get the encoded names\n",
    "preprocessor.fit(X_train)\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(['GENDER', 'RACE_ETHNICITY', 'FIRST_GEN_STATUS']).tolist()\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = numerical_columns + cat_feature_names\n",
    "print(f\"Feature names ({len(feature_names)} total):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text representation of the constrained tree\n",
    "print(\"Text Representation of Constrained Decision Tree:\")\n",
    "print(\"=\"*60)\n",
    "tree_rules = export_text(constrained_tree, feature_names=feature_names, max_depth=4)\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Graphical Visualization\n",
    "\n",
    "Matplotlib's plot_tree provides a clear graphical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the constrained tree (more manageable size)\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(constrained_tree, \n",
    "          feature_names=feature_names,\n",
    "          class_names=['Enrolled', 'Not Enrolled'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=8,\n",
    "          max_depth=3)  # Limit display depth for readability\n",
    "plt.title('Constrained Decision Tree (First 3 Levels)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the balanced tree\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(balanced_tree, \n",
    "          feature_names=feature_names,\n",
    "          class_names=['Enrolled', 'Not Enrolled'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=8,\n",
    "          max_depth=3)\n",
    "plt.title('Balanced Decision Tree (First 3 Levels)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Tree Visualization:**\n",
    "- **First line**: The decision rule (feature <= threshold)\n",
    "- **gini**: The Gini impurity at this node\n",
    "- **samples**: Number of training samples reaching this node\n",
    "- **value**: Distribution of classes [Enrolled, Not Enrolled]\n",
    "- **class**: The majority class (prediction if this were a leaf)\n",
    "- **Color**: Blue = Enrolled majority, Orange = Not Enrolled majority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Interactive Tree Visualization\n",
    "\n",
    "Let's create an interactive visualization using Plotly to explore tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tree_structure(tree, feature_names):\n",
    "    \"\"\"\n",
    "    Extract tree structure for visualization.\n",
    "    Returns nodes and edges data.\n",
    "    \"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    n_nodes = tree_.node_count\n",
    "    \n",
    "    nodes_data = []\n",
    "    edges_data = []\n",
    "    \n",
    "    # Calculate positions using BFS\n",
    "    from collections import deque\n",
    "    \n",
    "    # Node positions\n",
    "    positions = {}\n",
    "    queue = deque([(0, 0, 0, 1)])  # (node_id, depth, left_bound, right_bound)\n",
    "    \n",
    "    while queue:\n",
    "        node_id, depth, left, right = queue.popleft()\n",
    "        x = (left + right) / 2\n",
    "        y = -depth\n",
    "        positions[node_id] = (x, y)\n",
    "        \n",
    "        # Get node info\n",
    "        is_leaf = tree_.children_left[node_id] == tree_.children_right[node_id]\n",
    "        \n",
    "        if is_leaf:\n",
    "            # Leaf node\n",
    "            values = tree_.value[node_id][0]\n",
    "            predicted_class = 'N' if np.argmax(values) == 1 else 'E'\n",
    "            label = f\"Predict: {predicted_class}\\n({int(values[0])} E, {int(values[1])} N)\"\n",
    "            node_type = 'leaf'\n",
    "        else:\n",
    "            # Internal node\n",
    "            feature = feature_names[tree_.feature[node_id]]\n",
    "            threshold = tree_.threshold[node_id]\n",
    "            label = f\"{feature}\\n<= {threshold:.2f}\"\n",
    "            node_type = 'internal'\n",
    "            \n",
    "            # Add children to queue\n",
    "            left_child = tree_.children_left[node_id]\n",
    "            right_child = tree_.children_right[node_id]\n",
    "            mid = (left + right) / 2\n",
    "            queue.append((left_child, depth + 1, left, mid))\n",
    "            queue.append((right_child, depth + 1, mid, right))\n",
    "            \n",
    "            # Add edges\n",
    "            edges_data.append((node_id, left_child, 'Yes'))\n",
    "            edges_data.append((node_id, right_child, 'No'))\n",
    "        \n",
    "        samples = tree_.n_node_samples[node_id]\n",
    "        gini = tree_.impurity[node_id]\n",
    "        \n",
    "        nodes_data.append({\n",
    "            'id': node_id,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'label': label,\n",
    "            'type': node_type,\n",
    "            'samples': samples,\n",
    "            'gini': gini\n",
    "        })\n",
    "    \n",
    "    return nodes_data, edges_data, positions\n",
    "\n",
    "# Extract structure from balanced tree (limited depth for visualization)\n",
    "nodes, edges, positions = extract_tree_structure(balanced_tree, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive tree visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add edges\n",
    "for parent_id, child_id, label in edges:\n",
    "    if parent_id in positions and child_id in positions:\n",
    "        x0, y0 = positions[parent_id]\n",
    "        x1, y1 = positions[child_id]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x0, x1], y=[y0, y1],\n",
    "            mode='lines',\n",
    "            line=dict(color='gray', width=1),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "\n",
    "# Add nodes\n",
    "internal_nodes = [n for n in nodes if n['type'] == 'internal']\n",
    "leaf_nodes = [n for n in nodes if n['type'] == 'leaf']\n",
    "\n",
    "# Internal nodes\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[n['x'] for n in internal_nodes],\n",
    "    y=[n['y'] for n in internal_nodes],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=40, color='lightblue', line=dict(color='blue', width=2)),\n",
    "    text=[n['label'].split('\\n')[0] for n in internal_nodes],\n",
    "    textposition='middle center',\n",
    "    textfont=dict(size=8),\n",
    "    hovertemplate='<b>%{text}</b><br>Samples: %{customdata[0]}<br>Gini: %{customdata[1]:.3f}<extra></extra>',\n",
    "    customdata=[[n['samples'], n['gini']] for n in internal_nodes],\n",
    "    name='Decision Nodes'\n",
    "))\n",
    "\n",
    "# Leaf nodes\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[n['x'] for n in leaf_nodes],\n",
    "    y=[n['y'] for n in leaf_nodes],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=30, color='lightgreen', symbol='square', line=dict(color='green', width=2)),\n",
    "    text=[n['label'].split('\\n')[0] for n in leaf_nodes],\n",
    "    textposition='middle center',\n",
    "    textfont=dict(size=7),\n",
    "    hovertemplate='<b>%{text}</b><br>Samples: %{customdata[0]}<br>Gini: %{customdata[1]:.3f}<extra></extra>',\n",
    "    customdata=[[n['samples'], n['gini']] for n in leaf_nodes],\n",
    "    name='Leaf Nodes (Predictions)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive Decision Tree Visualization (Balanced Model)',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting Decision Rules\n",
    "\n",
    "For stakeholder communication, we can extract human-readable decision rules from the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules(tree, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Extract decision rules from a trained decision tree.\n",
    "    \"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    rules = []\n",
    "    \n",
    "    def recurse(node, depth, path):\n",
    "        if tree_.feature[node] != -2:  # Not a leaf\n",
    "            feature = feature_names[tree_.feature[node]]\n",
    "            threshold = tree_.threshold[node]\n",
    "            \n",
    "            # Left branch (<=)\n",
    "            left_path = path + [f\"{feature} <= {threshold:.2f}\"]\n",
    "            recurse(tree_.children_left[node], depth + 1, left_path)\n",
    "            \n",
    "            # Right branch (>)\n",
    "            right_path = path + [f\"{feature} > {threshold:.2f}\"]\n",
    "            recurse(tree_.children_right[node], depth + 1, right_path)\n",
    "        else:\n",
    "            # Leaf node\n",
    "            values = tree_.value[node][0]\n",
    "            total = sum(values)\n",
    "            predicted_class = class_names[np.argmax(values)]\n",
    "            confidence = max(values) / total\n",
    "            \n",
    "            rule = {\n",
    "                'conditions': path,\n",
    "                'prediction': predicted_class,\n",
    "                'confidence': confidence,\n",
    "                'samples': int(total),\n",
    "                'distribution': f\"{int(values[0])} E, {int(values[1])} N\"\n",
    "            }\n",
    "            rules.append(rule)\n",
    "    \n",
    "    recurse(0, 0, [])\n",
    "    return rules\n",
    "\n",
    "# Extract rules from the balanced tree\n",
    "class_names = ['Enrolled', 'Not Enrolled']\n",
    "rules = extract_rules(balanced_tree, feature_names, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rules predicting \"Not Enrolled\" with high confidence\n",
    "print(\"High-Confidence Rules for At-Risk Students (Not Enrolled):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by confidence and filter for \"Not Enrolled\" predictions\n",
    "at_risk_rules = [r for r in rules if r['prediction'] == 'Not Enrolled' and r['confidence'] >= 0.5]\n",
    "at_risk_rules = sorted(at_risk_rules, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "for i, rule in enumerate(at_risk_rules[:5], 1):\n",
    "    print(f\"\\nRule {i} (Confidence: {rule['confidence']:.1%}, Samples: {rule['samples']}):\")\n",
    "    print(f\"  IF {' AND '.join(rule['conditions'])}\")\n",
    "    print(f\"  THEN Predict: {rule['prediction']}\")\n",
    "    print(f\"  Distribution: {rule['distribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rules predicting \"Enrolled\" with high confidence\n",
    "print(\"High-Confidence Rules for Retained Students (Enrolled):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "enrolled_rules = [r for r in rules if r['prediction'] == 'Enrolled' and r['confidence'] >= 0.9]\n",
    "enrolled_rules = sorted(enrolled_rules, key=lambda x: x['samples'], reverse=True)\n",
    "\n",
    "for i, rule in enumerate(enrolled_rules[:5], 1):\n",
    "    print(f\"\\nRule {i} (Confidence: {rule['confidence']:.1%}, Samples: {rule['samples']}):\")\n",
    "    print(f\"  IF {' AND '.join(rule['conditions'])}\")\n",
    "    print(f\"  THEN Predict: {rule['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "Decision trees provide built-in feature importance scores based on how much each feature reduces impurity across all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the balanced tree\n",
    "importances = balanced_tree.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Filter to show only features with non-zero importance\n",
    "importance_df = importance_df[importance_df['Importance'] > 0]\n",
    "\n",
    "print(\"Feature Importances (Balanced Decision Tree):\")\n",
    "print(importance_df.sort_values('Importance', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig = go.Figure(go.Bar(\n",
    "    y=importance_df['Feature'],\n",
    "    x=importance_df['Importance'],\n",
    "    orientation='h',\n",
    "    marker=dict(color='steelblue')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance (Balanced Decision Tree)',\n",
    "    xaxis_title='Importance (Gini Impurity Reduction)',\n",
    "    yaxis_title='Feature',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across models\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Basic': basic_tree.feature_importances_,\n",
    "    'Constrained': constrained_tree.feature_importances_,\n",
    "    'Balanced': balanced_tree.feature_importances_\n",
    "})\n",
    "\n",
    "# Get top features by average importance\n",
    "importance_comparison['Average'] = importance_comparison[['Basic', 'Constrained', 'Balanced']].mean(axis=1)\n",
    "top_features = importance_comparison.nlargest(10, 'Average')\n",
    "\n",
    "print(\"Top 10 Features by Average Importance Across Models:\")\n",
    "print(top_features[['Feature', 'Basic', 'Constrained', 'Balanced', 'Average']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar chart comparing importance across models\n",
    "fig = go.Figure()\n",
    "\n",
    "top_features_sorted = top_features.sort_values('Average')\n",
    "\n",
    "for model in ['Basic', 'Constrained', 'Balanced']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=top_features_sorted['Feature'],\n",
    "        x=top_features_sorted[model],\n",
    "        name=model,\n",
    "        orientation='h'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance Comparison Across Decision Tree Models',\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Tree Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of tree structures\n",
    "def get_tree_stats(tree, name):\n",
    "    \"\"\"Get comprehensive statistics for a decision tree.\"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    \n",
    "    # Calculate average impurity reduction\n",
    "    impurities = tree_.impurity[tree_.impurity > 0]\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Depth': tree.get_depth(),\n",
    "        'Leaves': tree.get_n_leaves(),\n",
    "        'Total Nodes': tree_.node_count,\n",
    "        'Avg Leaf Samples': tree_.n_node_samples[tree_.feature == -2].mean(),\n",
    "        'Min Leaf Samples': tree_.n_node_samples[tree_.feature == -2].min(),\n",
    "        'Features Used': (tree.feature_importances_ > 0).sum()\n",
    "    }\n",
    "\n",
    "stats = [\n",
    "    get_tree_stats(basic_tree, 'Basic'),\n",
    "    get_tree_stats(constrained_tree, 'Constrained'),\n",
    "    get_tree_stats(balanced_tree, 'Balanced')\n",
    "]\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(\"\\nComprehensive Tree Structure Comparison:\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tree complexity comparison\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('Tree Depth', 'Number of Leaves', 'Features Used'))\n",
    "\n",
    "models = ['Basic', 'Constrained', 'Balanced']\n",
    "colors = ['coral', 'steelblue', 'seagreen']\n",
    "\n",
    "# Depth\n",
    "fig.add_trace(go.Bar(x=models, y=stats_df['Depth'], marker_color=colors, showlegend=False), row=1, col=1)\n",
    "\n",
    "# Leaves\n",
    "fig.add_trace(go.Bar(x=models, y=stats_df['Leaves'], marker_color=colors, showlegend=False), row=1, col=2)\n",
    "\n",
    "# Features Used\n",
    "fig.add_trace(go.Bar(x=models, y=stats_df['Features Used'], marker_color=colors, showlegend=False), row=1, col=3)\n",
    "\n",
    "fig.update_layout(height=400, title_text='Decision Tree Complexity Comparison')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "trained_models = {\n",
    "    'basic_decision_tree_trained': basic_dt_model,\n",
    "    'constrained_decision_tree_trained': constrained_dt_model,\n",
    "    'balanced_decision_tree_trained': balanced_dt_model\n",
    "}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    filepath = f'{models_path}{name}.pkl'\n",
    "    pickle.dump(model, open(filepath, 'wb'))\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names for later use\n",
    "feature_names_dict = {\n",
    "    'feature_names': feature_names,\n",
    "    'numerical_columns': numerical_columns,\n",
    "    'categorical_columns': ['GENDER', 'RACE_ETHNICITY', 'FIRST_GEN_STATUS']\n",
    "}\n",
    "\n",
    "pickle.dump(feature_names_dict, open(f'{models_path}decision_tree_feature_names.pkl', 'wb'))\n",
    "print(f\"\\nSaved feature names to: {models_path}decision_tree_feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this notebook, we trained and visualized decision tree models for student departure prediction.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Model | Depth | Leaves | Key Insight |\n",
    "|:------|:------|:-------|:------------|\n",
    "| **Basic** | Very Deep | Many | Overfits to training data |\n",
    "| **Constrained** | 5 | Moderate | Better generalization potential |\n",
    "| **Balanced** | 5 | Moderate | Handles class imbalance |\n",
    "\n",
    "### Most Important Features\n",
    "\n",
    "The decision trees identified several key predictors of student departure:\n",
    "- **GPA_1 and GPA_2**: First and second semester GPAs\n",
    "- **DFW_RATE_1 and DFW_RATE_2**: Course failure rates\n",
    "- **UNITS_ATTEMPTED**: Course load indicators\n",
    "\n",
    "### Interpretation Advantage\n",
    "\n",
    "Decision trees produce interpretable rules like:\n",
    "- \"If GPA_1 <= 1.8 AND DFW_RATE_1 > 0.3, then predict Not Enrolled\"\n",
    "\n",
    "These rules can be directly communicated to advisors and used in early alert systems.\n",
    "\n",
    "### Connection to ML Cycle\n",
    "\n",
    "We completed **Step 2: Train the Model**:\n",
    "- Trained three decision tree variants\n",
    "- Visualized tree structures\n",
    "- Extracted decision rules\n",
    "- Analyzed feature importance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Evaluate model performance on test data\n",
    "2. Tune hyperparameters using cross-validation\n",
    "3. Compare decision trees to logistic regression\n",
    "\n",
    "**Proceed to:** `2.4 Evaluate and Tune Decision Trees`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}