{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Introduction to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Module 1, we explored regularized logistic regression models. Now we turn to a fundamentally different approach to classification: **Decision Trees**. Unlike logistic regression, which models the probability of class membership using a smooth mathematical function, decision trees partition the feature space into rectangular regions using a series of simple rules.\n",
    "\n",
    "Decision trees are particularly valuable in higher education contexts because they produce models that are inherently interpretable. Administrators and advisors can easily understand and explain why a student was flagged as at-risk, which is crucial for designing effective interventions.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the structure and components of a decision tree\n",
    "2. Understand how decision trees make splitting decisions using impurity measures\n",
    "3. Describe the recursive partitioning algorithm\n",
    "4. Compare decision trees to logistic regression\n",
    "5. Identify when decision trees are appropriate for higher education problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **decision tree** is a supervised learning algorithm that makes predictions by learning a series of simple decision rules from the data. Think of it as a flowchart where each internal node asks a question about a feature, each branch represents an answer to that question, and each leaf node provides a prediction.\n",
    "\n",
    "**Intuition**: Imagine you're an academic advisor trying to determine if a student is at risk of not returning for their third semester. You might ask:\n",
    "1. \"Is their GPA below 2.0?\" If yes, they might be at risk.\n",
    "2. \"Did they fail any courses in the first semester?\" If yes, more concern.\n",
    "3. \"Are they a first-generation student?\" This might interact with other factors.\n",
    "\n",
    "A decision tree automates this process by finding the best questions to ask and in what order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Anatomy of a Decision Tree\n",
    "\n",
    "| Component | Description | Example |\n",
    "|:----------|:------------|:--------|\n",
    "| **Root Node** | The topmost node; first decision point | \"Is GPA_1 <= 2.5?\" |\n",
    "| **Internal Nodes** | Decision points that split the data | \"Is DFW_RATE_1 > 0.3?\" |\n",
    "| **Branches** | Outcomes of decisions (True/False) | Left: Yes, Right: No |\n",
    "| **Leaf Nodes** | Terminal nodes with predictions | \"Class: Not Enrolled\" |\n",
    "| **Depth** | Longest path from root to leaf | Depth = 3 means 3 decisions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a visual representation of a decision tree structure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Node positions (x, y)\n",
    "nodes = {\n",
    "    'root': (0.5, 1.0),\n",
    "    'left1': (0.25, 0.7),\n",
    "    'right1': (0.75, 0.7),\n",
    "    'left2': (0.125, 0.4),\n",
    "    'right2': (0.375, 0.4),\n",
    "    'left3': (0.625, 0.4),\n",
    "    'right3': (0.875, 0.4)\n",
    "}\n",
    "\n",
    "# Draw edges\n",
    "edges = [\n",
    "    ('root', 'left1'), ('root', 'right1'),\n",
    "    ('left1', 'left2'), ('left1', 'right2'),\n",
    "    ('right1', 'left3'), ('right1', 'right3')\n",
    "]\n",
    "\n",
    "for start, end in edges:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[nodes[start][0], nodes[end][0]],\n",
    "        y=[nodes[start][1], nodes[end][1]],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=2),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# Draw nodes\n",
    "# Internal nodes (decision nodes)\n",
    "internal = ['root', 'left1', 'right1']\n",
    "leaf = ['left2', 'right2', 'left3', 'right3']\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[nodes[n][0] for n in internal],\n",
    "    y=[nodes[n][1] for n in internal],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=50, color='lightblue', line=dict(color='blue', width=2)),\n",
    "    text=['GPA_1 <= 2.5?', 'DFW > 0.3?', 'UNITS < 12?'],\n",
    "    textposition='middle center',\n",
    "    textfont=dict(size=10),\n",
    "    name='Decision Nodes'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[nodes[n][0] for n in leaf],\n",
    "    y=[nodes[n][1] for n in leaf],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=50, color='lightgreen', symbol='square', line=dict(color='green', width=2)),\n",
    "    text=['Not Enrolled', 'Enrolled', 'Not Enrolled', 'Enrolled'],\n",
    "    textposition='middle center',\n",
    "    textfont=dict(size=9),\n",
    "    name='Leaf Nodes'\n",
    "))\n",
    "\n",
    "# Add branch labels\n",
    "fig.add_annotation(x=0.35, y=0.88, text='Yes', showarrow=False, font=dict(color='green'))\n",
    "fig.add_annotation(x=0.65, y=0.88, text='No', showarrow=False, font=dict(color='red'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Anatomy of a Decision Tree',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 A Simple Example\n",
    "\n",
    "Let's visualize how a decision tree partitions the feature space. Consider a simplified scenario with just two features: GPA and DFW Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic student data for visualization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enrolled students (higher GPA, lower DFW rate)\n",
    "n_enrolled = 100\n",
    "gpa_enrolled = np.random.normal(3.2, 0.5, n_enrolled)\n",
    "dfw_enrolled = np.random.normal(0.1, 0.1, n_enrolled)\n",
    "\n",
    "# Not enrolled students (lower GPA, higher DFW rate)\n",
    "n_not_enrolled = 40\n",
    "gpa_not_enrolled = np.random.normal(2.0, 0.6, n_not_enrolled)\n",
    "dfw_not_enrolled = np.random.normal(0.4, 0.15, n_not_enrolled)\n",
    "\n",
    "# Clip values to realistic ranges\n",
    "gpa_enrolled = np.clip(gpa_enrolled, 0, 4)\n",
    "gpa_not_enrolled = np.clip(gpa_not_enrolled, 0, 4)\n",
    "dfw_enrolled = np.clip(dfw_enrolled, 0, 1)\n",
    "dfw_not_enrolled = np.clip(dfw_not_enrolled, 0, 1)\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot enrolled students\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=gpa_enrolled, y=dfw_enrolled,\n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', size=8, opacity=0.7),\n",
    "    name='Enrolled (E)'\n",
    "))\n",
    "\n",
    "# Plot not enrolled students\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=gpa_not_enrolled, y=dfw_not_enrolled,\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=8, opacity=0.7),\n",
    "    name='Not Enrolled (N)'\n",
    "))\n",
    "\n",
    "# Add decision boundaries (simulating tree splits)\n",
    "# First split: GPA <= 2.5\n",
    "fig.add_vline(x=2.5, line_dash=\"dash\", line_color=\"black\", \n",
    "              annotation_text=\"Split 1: GPA <= 2.5\")\n",
    "\n",
    "# Second split: DFW > 0.25 (for GPA > 2.5 region)\n",
    "fig.add_shape(type=\"line\", x0=2.5, y0=0.25, x1=4.0, y1=0.25,\n",
    "              line=dict(color=\"black\", dash=\"dash\"))\n",
    "fig.add_annotation(x=3.25, y=0.28, text=\"Split 2: DFW > 0.25\", showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='How Decision Trees Partition Feature Space',\n",
    "    xaxis_title='First Semester GPA',\n",
    "    yaxis_title='DFW Rate',\n",
    "    height=500,\n",
    "    xaxis=dict(range=[0, 4]),\n",
    "    yaxis=dict(range=[0, 0.8])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: The decision tree creates rectangular regions by making axis-parallel splits. Students falling in different regions receive different predictions:\n",
    "- **Region 1** (GPA <= 2.5): Mostly \"Not Enrolled\" students\n",
    "- **Region 2** (GPA > 2.5 AND DFW > 0.25): Mixed, but concerning\n",
    "- **Region 3** (GPA > 2.5 AND DFW <= 0.25): Mostly \"Enrolled\" students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Decision Trees Make Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Impurity Measures\n",
    "\n",
    "Decision trees choose splits by finding the feature and threshold that best separates the classes. \"Best\" is defined by an **impurity measure**. A node is \"pure\" if it contains samples from only one class.\n",
    "\n",
    "The two most common impurity measures for classification are:\n",
    "1. **Gini Impurity** (default in scikit-learn)\n",
    "2. **Entropy** (Information Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gini Impurity\n",
    "\n",
    "**Gini Impurity** measures the probability of incorrectly classifying a randomly chosen sample if it were labeled according to the class distribution at that node.\n",
    "\n",
    "$$Gini = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "Where $p_i$ is the proportion of samples belonging to class $i$, and $C$ is the number of classes.\n",
    "\n",
    "**For binary classification (Enrolled vs. Not Enrolled):**\n",
    "$$Gini = 1 - p_{Enrolled}^2 - p_{NotEnrolled}^2$$\n",
    "\n",
    "| Scenario | $p_{Enrolled}$ | $p_{NotEnrolled}$ | Gini Impurity |\n",
    "|:---------|:---------------|:------------------|:--------------|\n",
    "| Pure node (all Enrolled) | 1.0 | 0.0 | 0.0 |\n",
    "| Pure node (all Not Enrolled) | 0.0 | 1.0 | 0.0 |\n",
    "| Maximum impurity (50-50) | 0.5 | 0.5 | 0.5 |\n",
    "| Typical imbalanced | 0.87 | 0.13 | 0.23 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gini Impurity\n",
    "p = np.linspace(0, 1, 100)\n",
    "gini = 1 - p**2 - (1-p)**2\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=p, y=gini,\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=3),\n",
    "    name='Gini Impurity'\n",
    "))\n",
    "\n",
    "# Add annotations for key points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 0.5, 1, 0.13],\n",
    "    y=[0, 0.5, 0, 1 - 0.13**2 - 0.87**2],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=12, color='red'),\n",
    "    text=['Pure<br>(all E)', 'Max Impurity<br>(50-50)', 'Pure<br>(all N)', 'Our Data<br>(87-13)'],\n",
    "    textposition=['bottom center', 'top center', 'bottom center', 'top right'],\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Gini Impurity vs. Class Proportion',\n",
    "    xaxis_title='Proportion of Not Enrolled (Class N)',\n",
    "    yaxis_title='Gini Impurity',\n",
    "    height=400,\n",
    "    xaxis=dict(range=[-0.05, 1.05]),\n",
    "    yaxis=dict(range=[-0.05, 0.55])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entropy and Information Gain\n",
    "\n",
    "**Entropy** comes from information theory and measures the amount of \"uncertainty\" or \"disorder\" in a node.\n",
    "\n",
    "$$Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "**Information Gain** is the reduction in entropy after a split:\n",
    "$$Information\\ Gain = Entropy_{parent} - \\sum_{children} \\frac{n_{child}}{n_{parent}} \\times Entropy_{child}$$\n",
    "\n",
    "The split that maximizes information gain (or equivalently, minimizes weighted child entropy) is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Gini and Entropy\n",
    "p = np.linspace(0.001, 0.999, 100)  # Avoid log(0)\n",
    "gini = 1 - p**2 - (1-p)**2\n",
    "entropy = -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=p, y=gini,\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=3),\n",
    "    name='Gini Impurity'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=p, y=entropy,\n",
    "    mode='lines',\n",
    "    line=dict(color='orange', width=3),\n",
    "    name='Entropy'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Gini Impurity vs. Entropy',\n",
    "    xaxis_title='Proportion of Class N',\n",
    "    yaxis_title='Impurity Measure',\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparing Gini and Entropy\n",
    "\n",
    "| Property | Gini Impurity | Entropy |\n",
    "|:---------|:--------------|:--------|\n",
    "| Range | [0, 0.5] for binary | [0, 1] for binary |\n",
    "| Computation | Faster (no logarithm) | Slower |\n",
    "| Interpretation | Probability of misclassification | Information content |\n",
    "| In practice | Often produces similar trees | Often produces similar trees |\n",
    "| Default in sklearn | Yes (`criterion='gini'`) | No (`criterion='entropy'`) |\n",
    "\n",
    "**Bottom line**: Both measures usually produce very similar trees. Gini is the default because it's computationally faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Tree Building Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Recursive Partitioning\n",
    "\n",
    "Decision trees are built using a **greedy, recursive algorithm** called CART (Classification and Regression Trees):\n",
    "\n",
    "1. **Start at the root** with all training samples\n",
    "2. **For each feature**:\n",
    "   - Consider all possible split thresholds\n",
    "   - Calculate the impurity reduction for each split\n",
    "3. **Select the best split** (feature + threshold with maximum impurity reduction)\n",
    "4. **Create two child nodes** based on the split\n",
    "5. **Recursively repeat** steps 2-4 for each child node\n",
    "6. **Stop** when a stopping criterion is met\n",
    "\n",
    "**Key insight**: The algorithm is \"greedy\" because it makes the locally optimal choice at each step, without considering future splits. This makes it fast but doesn't guarantee the globally optimal tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the splitting process\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'GPA': [3.5, 2.8, 1.9, 3.2, 2.1, 3.8, 2.5, 1.5],\n",
    "    'DFW_Rate': [0.0, 0.2, 0.5, 0.1, 0.4, 0.0, 0.3, 0.6],\n",
    "    'Enrolled': ['E', 'E', 'N', 'E', 'N', 'E', 'E', 'N']\n",
    "})\n",
    "\n",
    "print(\"Sample Student Data:\")\n",
    "print(sample_data.to_string(index=False))\n",
    "\n",
    "# Calculate Gini impurity for original data\n",
    "p_enrolled = (sample_data['Enrolled'] == 'E').mean()\n",
    "p_not_enrolled = 1 - p_enrolled\n",
    "gini_original = 1 - p_enrolled**2 - p_not_enrolled**2\n",
    "\n",
    "print(f\"\\nOriginal Distribution: {p_enrolled:.0%} Enrolled, {p_not_enrolled:.0%} Not Enrolled\")\n",
    "print(f\"Original Gini Impurity: {gini_original:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate different split points for GPA\n",
    "def calculate_weighted_gini(left_enrolled, left_total, right_enrolled, right_total):\n",
    "    \"\"\"Calculate weighted Gini impurity after a split.\"\"\"\n",
    "    total = left_total + right_total\n",
    "    \n",
    "    # Left node Gini\n",
    "    if left_total > 0:\n",
    "        p_left = left_enrolled / left_total\n",
    "        gini_left = 1 - p_left**2 - (1-p_left)**2\n",
    "    else:\n",
    "        gini_left = 0\n",
    "    \n",
    "    # Right node Gini\n",
    "    if right_total > 0:\n",
    "        p_right = right_enrolled / right_total\n",
    "        gini_right = 1 - p_right**2 - (1-p_right)**2\n",
    "    else:\n",
    "        gini_right = 0\n",
    "    \n",
    "    # Weighted average\n",
    "    weighted_gini = (left_total/total) * gini_left + (right_total/total) * gini_right\n",
    "    return weighted_gini, gini_left, gini_right\n",
    "\n",
    "# Test different GPA thresholds\n",
    "thresholds = [2.0, 2.3, 2.5, 3.0]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    left_mask = sample_data['GPA'] <= thresh\n",
    "    left_enrolled = (sample_data.loc[left_mask, 'Enrolled'] == 'E').sum()\n",
    "    left_total = left_mask.sum()\n",
    "    right_enrolled = (sample_data.loc[~left_mask, 'Enrolled'] == 'E').sum()\n",
    "    right_total = (~left_mask).sum()\n",
    "    \n",
    "    weighted, gini_l, gini_r = calculate_weighted_gini(left_enrolled, left_total, right_enrolled, right_total)\n",
    "    info_gain = gini_original - weighted\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': thresh,\n",
    "        'Left (<=)': f\"{left_enrolled}E, {left_total-left_enrolled}N\",\n",
    "        'Right (>)': f\"{right_enrolled}E, {right_total-right_enrolled}N\",\n",
    "        'Weighted Gini': round(weighted, 3),\n",
    "        'Info Gain': round(info_gain, 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nEvaluating GPA Split Thresholds:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nBest split: GPA <= {results_df.loc[results_df['Info Gain'].idxmax(), 'Threshold']} (highest information gain)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stopping Criteria\n",
    "\n",
    "Without constraints, a decision tree would keep splitting until every leaf is pure (contains only one class). This leads to **overfitting**. Several stopping criteria prevent this:\n",
    "\n",
    "| Criterion | Description | scikit-learn Parameter |\n",
    "|:----------|:------------|:-----------------------|\n",
    "| **Maximum Depth** | Limit tree depth | `max_depth` |\n",
    "| **Minimum Samples Split** | Minimum samples to split a node | `min_samples_split` |\n",
    "| **Minimum Samples Leaf** | Minimum samples in leaf nodes | `min_samples_leaf` |\n",
    "| **Maximum Leaf Nodes** | Maximum number of leaf nodes | `max_leaf_nodes` |\n",
    "| **Minimum Impurity Decrease** | Minimum improvement required | `min_impurity_decrease` |\n",
    "\n",
    "These hyperparameters are crucial for controlling model complexity and preventing overfitting. We will explore them in detail in notebook 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of max_depth on tree complexity\n",
    "depths = [1, 2, 3, 4, 5, 10, None]\n",
    "descriptions = ['Very simple', 'Simple', 'Moderate', 'Complex', 'More complex', 'Very complex', 'Unlimited']\n",
    "\n",
    "complexity_data = pd.DataFrame({\n",
    "    'max_depth': ['1', '2', '3', '4', '5', '10', 'None'],\n",
    "    'Max Leaf Nodes': [2, 4, 8, 16, 32, 1024, 'Unlimited'],\n",
    "    'Complexity': descriptions,\n",
    "    'Risk': ['High Bias', 'Moderate Bias', 'Balanced', 'Moderate Variance', 'High Variance', 'Very High Variance', 'Extreme Variance']\n",
    "})\n",
    "\n",
    "print(\"Effect of max_depth on Tree Complexity:\")\n",
    "print(complexity_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Trees vs. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand decision trees, let's compare them to the logistic regression models we built in Course 2 and Module 1.\n",
    "\n",
    "| Aspect | Decision Trees | Logistic Regression |\n",
    "|:-------|:---------------|:--------------------|\n",
    "| **Decision Boundary** | Rectangular (axis-parallel) | Linear (smooth) |\n",
    "| **Feature Interactions** | Captured automatically | Must be engineered |\n",
    "| **Preprocessing** | Minimal (no scaling needed) | Important (scaling, encoding) |\n",
    "| **Interpretability** | Visual rules | Coefficients/odds ratios |\n",
    "| **Handling Missing Data** | Can be handled | Requires imputation |\n",
    "| **Outliers** | Robust | Can be sensitive |\n",
    "| **Overfitting Risk** | High (without constraints) | Lower (with regularization) |\n",
    "| **Ensemble Potential** | Foundation for Random Forest, XGBoost | Limited |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary differences\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Generate two overlapping classes\n",
    "X1_class0 = np.random.multivariate_normal([3, 0.15], [[0.3, 0], [0, 0.01]], n_samples//2)\n",
    "X1_class1 = np.random.multivariate_normal([2, 0.35], [[0.5, 0], [0, 0.015]], n_samples//2)\n",
    "\n",
    "X_demo = np.vstack([X1_class0, X1_class1])\n",
    "y_demo = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# Clip to realistic ranges\n",
    "X_demo[:, 0] = np.clip(X_demo[:, 0], 0, 4)  # GPA\n",
    "X_demo[:, 1] = np.clip(X_demo[:, 1], 0, 1)  # DFW Rate\n",
    "\n",
    "# Train models\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "dt.fit(X_demo, y_demo)\n",
    "lr.fit(X_demo, y_demo)\n",
    "\n",
    "# Create mesh for decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(0, 4, 100), np.linspace(0, 0.7, 100))\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Get predictions\n",
    "Z_dt = dt.predict_proba(mesh_points)[:, 1].reshape(xx.shape)\n",
    "Z_lr = lr.predict_proba(mesh_points)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Decision Tree', 'Logistic Regression'))\n",
    "\n",
    "# Decision Tree\n",
    "fig.add_trace(go.Contour(x=np.linspace(0, 4, 100), y=np.linspace(0, 0.7, 100), z=Z_dt,\n",
    "                         colorscale='RdBu', showscale=False, opacity=0.6,\n",
    "                         contours=dict(start=0, end=1, size=0.1)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=X_demo[y_demo==0, 0], y=X_demo[y_demo==0, 1],\n",
    "                         mode='markers', marker=dict(color='blue', size=6), \n",
    "                         name='Enrolled', showlegend=True), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=X_demo[y_demo==1, 0], y=X_demo[y_demo==1, 1],\n",
    "                         mode='markers', marker=dict(color='red', size=6), \n",
    "                         name='Not Enrolled', showlegend=True), row=1, col=1)\n",
    "\n",
    "# Logistic Regression\n",
    "fig.add_trace(go.Contour(x=np.linspace(0, 4, 100), y=np.linspace(0, 0.7, 100), z=Z_lr,\n",
    "                         colorscale='RdBu', showscale=False, opacity=0.6,\n",
    "                         contours=dict(start=0, end=1, size=0.1)), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=X_demo[y_demo==0, 0], y=X_demo[y_demo==0, 1],\n",
    "                         mode='markers', marker=dict(color='blue', size=6), \n",
    "                         showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=X_demo[y_demo==1, 0], y=X_demo[y_demo==1, 1],\n",
    "                         mode='markers', marker=dict(color='red', size=6), \n",
    "                         showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='GPA', row=1, col=1)\n",
    "fig.update_xaxes(title_text='GPA', row=1, col=2)\n",
    "fig.update_yaxes(title_text='DFW Rate', row=1, col=1)\n",
    "fig.update_yaxes(title_text='DFW Rate', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=450, title_text='Decision Boundaries: Decision Tree vs. Logistic Regression')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: Notice how the decision tree creates rectangular regions with sharp boundaries, while logistic regression creates a smooth, diagonal boundary. The tree naturally captures the non-linear relationship where students with low GPA AND high DFW rate are at highest risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Decision Trees\n",
    "\n",
    "1. **Interpretability**: Easy to visualize and explain to non-technical stakeholders\n",
    "2. **No Feature Scaling**: Works with raw feature values\n",
    "3. **Handles Mixed Data**: Works with both numerical and categorical features\n",
    "4. **Captures Non-linearity**: Automatically finds non-linear relationships\n",
    "5. **Feature Interactions**: Naturally captures interactions between features\n",
    "6. **Feature Importance**: Built-in feature importance scores\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "\n",
    "1. **Overfitting**: Prone to overfitting without proper constraints\n",
    "2. **Instability**: Small changes in data can produce very different trees\n",
    "3. **Greedy Algorithm**: May not find globally optimal tree\n",
    "4. **Axis-Parallel Splits**: Cannot capture diagonal boundaries efficiently\n",
    "5. **Imbalanced Data**: Can be biased toward majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Criterion': ['Interpretability', 'Preprocessing Required', 'Handles Non-linearity',\n",
    "                  'Feature Interactions', 'Overfitting Risk', 'Ensemble Foundation',\n",
    "                  'Computational Cost', 'Probability Calibration'],\n",
    "    'Decision Tree': ['Excellent', 'Minimal', 'Yes (automatic)', 'Yes (automatic)',\n",
    "                      'High', 'Excellent', 'Low', 'Poor'],\n",
    "    'Logistic Regression': ['Good (coefficients)', 'Important', 'No (needs engineering)',\n",
    "                            'No (needs engineering)', 'Lower', 'Limited', 'Low', 'Good']\n",
    "})\n",
    "\n",
    "print(\"Decision Trees vs. Logistic Regression:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Trees in Higher Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are particularly well-suited for higher education analytics for several reasons:\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Student Risk Identification**\n",
    "   - Identify at-risk students using interpretable rules\n",
    "   - \"If GPA < 2.0 AND DFW Rate > 0.3, then high risk\"\n",
    "\n",
    "2. **Intervention Targeting**\n",
    "   - Rules translate directly to actionable interventions\n",
    "   - Advisors can understand and trust the model's logic\n",
    "\n",
    "3. **Policy Development**\n",
    "   - Tree structure can inform academic policies\n",
    "   - Identify thresholds for early warning systems\n",
    "\n",
    "4. **Stakeholder Communication**\n",
    "   - Easy to explain to administrators, faculty, and students\n",
    "   - Visual representation aids understanding\n",
    "\n",
    "### Example Rules from Student Departure Prediction\n",
    "\n",
    "A decision tree might learn rules like:\n",
    "\n",
    "```\n",
    "Rule 1: IF GPA_1 <= 1.8 THEN Predict \"Not Enrolled\" (High Risk)\n",
    "Rule 2: IF GPA_1 > 1.8 AND DFW_RATE_1 > 0.5 THEN Predict \"Not Enrolled\" (Moderate Risk)\n",
    "Rule 3: IF GPA_1 > 2.5 AND DFW_RATE_1 <= 0.2 THEN Predict \"Enrolled\" (Low Risk)\n",
    "```\n",
    "\n",
    "These rules can be directly communicated to advisors and integrated into early alert systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "1. **Decision Tree Structure**: Root nodes, internal nodes, branches, and leaf nodes\n",
    "\n",
    "2. **Splitting Criteria**: Gini impurity and entropy measure node purity\n",
    "\n",
    "3. **Tree Building**: Recursive partitioning with greedy optimization\n",
    "\n",
    "4. **Stopping Criteria**: max_depth, min_samples_split, etc. prevent overfitting\n",
    "\n",
    "5. **Comparison with Logistic Regression**: Different decision boundaries and tradeoffs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|:--------|:---------|\n",
    "| Decision Trees | Partition feature space with simple rules |\n",
    "| Gini Impurity | Probability of misclassification; lower is purer |\n",
    "| Entropy | Information uncertainty; lower is purer |\n",
    "| Overfitting | Trees without constraints memorize training data |\n",
    "| Interpretability | Major advantage for stakeholder communication |\n",
    "\n",
    "### Connection to ML Cycle\n",
    "\n",
    "| ML Cycle Step | Decision Tree Context |\n",
    "|:--------------|:----------------------|\n",
    "| **Build** | Choose tree structure and hyperparameters |\n",
    "| **Train** | Fit tree using recursive partitioning |\n",
    "| **Predict** | Follow decision rules to leaf nodes |\n",
    "| **Evaluate** | Assess using classification metrics |\n",
    "| **Improve** | Tune hyperparameters to balance bias/variance |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will build decision tree classification models in scikit-learn, creating pipelines that can be trained on our student departure data.\n",
    "\n",
    "**Proceed to:** `2.2 Build a Decision Tree Classification Model`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}