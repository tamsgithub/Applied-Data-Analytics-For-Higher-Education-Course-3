{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 **Build** a Neural Network with Keras - Predict Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### **1. Build the Model : Create the Neural Network architecture with Keras.**  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we learned the theory behind neural networks. Now we put that knowledge into practice by building neural network models using **TensorFlow** and **Keras**.\n",
    "\n",
    "Keras provides a simple, intuitive API for creating neural networks. We will use the **Sequential API** to stack layers and build models for predicting student departure.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the TensorFlow/Keras ecosystem\n",
    "2. Create neural networks using the Sequential API\n",
    "3. Configure Dense (fully connected) layers with appropriate activations\n",
    "4. Compile models with loss functions, optimizers, and metrics\n",
    "5. Visualize and summarize model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Scikit-learn for preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "# Display settings\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "module5_filepath = f'{course3_filepath}module_5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_training['SEM_3_STATUS'].value_counts(normalize=True))\n",
    "print(f\"\\nClass imbalance ratio: {df_training['SEM_3_STATUS'].value_counts()[0] / df_training['SEM_3_STATUS'].value_counts()[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is TensorFlow?\n",
    "\n",
    "**TensorFlow** is Google's open-source machine learning framework. It provides:\n",
    "\n",
    "- Efficient numerical computation\n",
    "- Automatic differentiation (computing gradients for backpropagation)\n",
    "- GPU acceleration for faster training\n",
    "- Tools for deploying models to production\n",
    "\n",
    "**Tensors** are multi-dimensional arrays - the fundamental data structure:\n",
    "- 0D tensor: scalar (single number)\n",
    "- 1D tensor: vector (list of numbers)\n",
    "- 2D tensor: matrix (table of numbers)\n",
    "- 3D+ tensor: higher-dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tensors\n",
    "# 0D: scalar\n",
    "scalar = tf.constant(5)\n",
    "print(f\"Scalar (0D): {scalar}, shape: {scalar.shape}\")\n",
    "\n",
    "# 1D: vector\n",
    "vector = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"Vector (1D): {vector}, shape: {vector.shape}\")\n",
    "\n",
    "# 2D: matrix (like a batch of samples with features)\n",
    "matrix = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Matrix (2D): \\n{matrix}, shape: {matrix.shape}\")\n",
    "\n",
    "# Example: A batch of 3 students with 4 features each\n",
    "student_batch = tf.constant([\n",
    "    [3.5, 0.1, 15, 1],  # Student 1: GPA, DFW_rate, units, first_gen\n",
    "    [2.8, 0.3, 12, 0],  # Student 2\n",
    "    [3.9, 0.0, 18, 1],  # Student 3\n",
    "])\n",
    "print(f\"\\nStudent batch shape: {student_batch.shape}\")\n",
    "print(f\"(3 students, 4 features each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras: The High-Level API\n",
    "\n",
    "**Keras** is TensorFlow's high-level API that makes building neural networks simple and intuitive.\n",
    "\n",
    "**Why use Keras?**\n",
    "- Simple, readable code\n",
    "- Modular and composable\n",
    "- Works with both beginners and experts\n",
    "- Integrated directly into TensorFlow\n",
    "\n",
    "**Two main ways to build models in Keras:**\n",
    "1. **Sequential API**: For simple, linear stacks of layers (what we'll use)\n",
    "2. **Functional API**: For complex architectures with multiple inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches conceptually\n",
    "print(\"Sequential API (what we'll use):\")\n",
    "print(\"\"\"  \n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(10,)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nFunctional API (for complex architectures):\")\n",
    "print(\"\"\"\n",
    "inputs = Input(shape=(10,))\n",
    "x = Dense(16, activation='relu')(inputs)\n",
    "x = Dense(8, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating a Sequential Model\n",
    "\n",
    "The Sequential model is a linear stack of layers. You can create it in two ways:\n",
    "\n",
    "**Method 1**: Pass a list of layers to the constructor\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "**Method 2**: Add layers one by one\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Both methods produce the same result. We'll primarily use Method 1 for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Sequential model\n",
    "simple_model = Sequential([\n",
    "    Input(shape=(10,)),  # Input layer: 10 features\n",
    "    Dense(8, activation='relu'),  # Hidden layer: 8 neurons, ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer: 1 neuron, Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# View model summary\n",
    "print(\"Simple Neural Network Model:\")\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Summary:**\n",
    "\n",
    "- **Layer**: Name and type of each layer\n",
    "- **Output Shape**: Shape of the output from each layer (None = batch size, determined at runtime)\n",
    "- **Param #**: Number of trainable parameters (weights + biases)\n",
    "\n",
    "For a Dense layer: `params = (input_size * output_size) + output_size`\n",
    "- Example: 10 inputs -> 8 neurons = (10 * 8) + 8 = 88 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding Layers\n",
    "\n",
    "Keras provides many layer types. For tabular data, the most important is the **Dense** layer.\n",
    "\n",
    "Common layer types in Keras:\n",
    "\n",
    "| Layer | Use Case | Description |\n",
    "|:------|:---------|:------------|\n",
    "| `Dense` | All neural networks | Fully connected layer |\n",
    "| `Dropout` | Regularization | Randomly drops neurons during training |\n",
    "| `BatchNormalization` | Training stability | Normalizes layer inputs |\n",
    "| `Conv2D` | Image data | Convolutional layer |\n",
    "| `LSTM` | Sequential data | Long Short-Term Memory |\n",
    "\n",
    "For our student departure prediction, we'll use `Dense` and `Dropout` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dense Layer Parameters\n",
    "\n",
    "The `Dense` layer is the workhorse of neural networks for tabular data.\n",
    "\n",
    "```python\n",
    "Dense(\n",
    "    units,              # Number of neurons (required)\n",
    "    activation=None,    # Activation function\n",
    "    use_bias=True,      # Include bias term?\n",
    "    kernel_initializer='glorot_uniform',  # How to initialize weights\n",
    "    bias_initializer='zeros',             # How to initialize biases\n",
    "    kernel_regularizer=None,              # L1/L2 regularization on weights\n",
    "    bias_regularizer=None,                # Regularization on biases\n",
    ")\n",
    "```\n",
    "\n",
    "**Most important parameters:**\n",
    "- `units`: Number of neurons in the layer\n",
    "- `activation`: Which activation function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Dense layer configurations\n",
    "print(\"Common Dense Layer Configurations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hidden layer with ReLU\n",
    "layer1 = Dense(16, activation='relu')\n",
    "print(f\"\\n1. Hidden layer: Dense(16, activation='relu')\")\n",
    "print(f\"   - 16 neurons with ReLU activation\")\n",
    "print(f\"   - Good default for hidden layers\")\n",
    "\n",
    "# Output layer for binary classification\n",
    "layer2 = Dense(1, activation='sigmoid')\n",
    "print(f\"\\n2. Binary output: Dense(1, activation='sigmoid')\")\n",
    "print(f\"   - 1 neuron with Sigmoid activation\")\n",
    "print(f\"   - Outputs probability between 0 and 1\")\n",
    "\n",
    "# Output layer for multi-class classification\n",
    "layer3 = Dense(5, activation='softmax')\n",
    "print(f\"\\n3. Multi-class output: Dense(5, activation='softmax')\")\n",
    "print(f\"   - 5 neurons (one per class) with Softmax activation\")\n",
    "print(f\"   - Outputs probabilities that sum to 1\")\n",
    "\n",
    "# Dense with L2 regularization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "layer4 = Dense(16, activation='relu', kernel_regularizer=l2(0.01))\n",
    "print(f\"\\n4. Regularized: Dense(16, activation='relu', kernel_regularizer=l2(0.01))\")\n",
    "print(f\"   - Adds L2 penalty to weights (prevents overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Activation Functions in Keras\n",
    "\n",
    "Keras provides all common activation functions as strings or through the `activations` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions available in Keras\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Get activations from Keras\n",
    "activations = {\n",
    "    'relu': tf.keras.activations.relu(x).numpy(),\n",
    "    'sigmoid': tf.keras.activations.sigmoid(x).numpy(),\n",
    "    'tanh': tf.keras.activations.tanh(x).numpy(),\n",
    "    'softplus': tf.keras.activations.softplus(x).numpy(),\n",
    "    'elu': tf.keras.activations.elu(x).numpy(),\n",
    "    'selu': tf.keras.activations.selu(x).numpy(),\n",
    "}\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=list(activations.keys()))\n",
    "\n",
    "colors = px.colors.qualitative.Set2\n",
    "for idx, (name, y) in enumerate(activations.items()):\n",
    "    row = idx // 3 + 1\n",
    "    col = idx % 3 + 1\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=y, mode='lines',\n",
    "        line=dict(color=colors[idx], width=3),\n",
    "        name=name, showlegend=False\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_xaxes(title='z')\n",
    "fig.update_yaxes(title='f(z)')\n",
    "fig.update_layout(\n",
    "    title='Activation Functions Available in Keras',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function guide for our problem\n",
    "activation_guide = pd.DataFrame({\n",
    "    'Activation': ['relu', 'sigmoid', 'tanh', 'softmax', 'linear'],\n",
    "    'Use Case': [\n",
    "        'Hidden layers (default choice)',\n",
    "        'Binary classification output',\n",
    "        'Hidden layers (alternative to ReLU)',\n",
    "        'Multi-class classification output',\n",
    "        'Regression output'\n",
    "    ],\n",
    "    'Output Range': [\n",
    "        '[0, infinity)',\n",
    "        '(0, 1)',\n",
    "        '(-1, 1)',\n",
    "        '(0, 1) - sums to 1',\n",
    "        '(-infinity, infinity)'\n",
    "    ],\n",
    "    'For Student Departure': [\n",
    "        'YES - use in hidden layers',\n",
    "        'YES - use in output layer',\n",
    "        'Alternative for hidden layers',\n",
    "        'NO - not binary classification',\n",
    "        'NO - not regression'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Activation Function Guide for Student Departure Prediction:\")\n",
    "activation_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Neural Networks for Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Preprocessing Pipeline\n",
    "\n",
    "Neural networks require:\n",
    "1. **Scaled numeric features**: Networks are sensitive to feature scales\n",
    "2. **Encoded categorical features**: Convert categories to numbers\n",
    "\n",
    "We'll use the same preprocessing as our previous models for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "minmax_columns = [\n",
    "    'HS_GPA',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2'\n",
    "]\n",
    "\n",
    "standard_columns = [\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'GENDER',\n",
    "    'RACE_ETHNICITY',\n",
    "    'FIRST_GEN_STATUS'\n",
    "]\n",
    "\n",
    "# All features\n",
    "all_features = minmax_columns + standard_columns + categorical_columns\n",
    "print(f\"Feature groups:\")\n",
    "print(f\"  MinMax scaled: {len(minmax_columns)} features\")\n",
    "print(f\"  Standard scaled: {len(standard_columns)} features\")\n",
    "print(f\"  Categorical (one-hot): {len(categorical_columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), minmax_columns),\n",
    "        ('standard', StandardScaler(), standard_columns),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                  drop=['Female', 'Other', 'Unknown'], \n",
    "                                  sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = df_training[all_features]\n",
    "y = df_training['SEM_3_STATUS']\n",
    "\n",
    "# Fit preprocessor and transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "onehot_features = list(preprocessor.transformers_[2][1].get_feature_names_out(categorical_columns))\n",
    "feature_names = minmax_columns + standard_columns + onehot_features\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"After preprocessing: {X_processed.shape[1]}\")\n",
    "print(f\"\\nFeature names after preprocessing:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"\\nInput shape for neural network: {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the input dimension for model building\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model 1: Simple Neural Network\n",
    "\n",
    "Let's start with a simple architecture: one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Simple Neural Network\n",
    "# Architecture: Input -> 8 neurons (ReLU) -> Output (Sigmoid)\n",
    "\n",
    "def create_simple_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create a simple neural network with one hidden layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Sequential\n",
    "        Compiled neural network model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(input_dim,), name='input_layer'),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        Dense(8, activation='relu', name='hidden_layer_1'),\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ], name='simple_nn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model_simple = create_simple_model(input_dim)\n",
    "\n",
    "# Display summary\n",
    "print(\"Model 1: Simple Neural Network\")\n",
    "print(\"=\"*60)\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model 2: Deeper Neural Network\n",
    "\n",
    "Now let's add more hidden layers to capture more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Deeper Neural Network\n",
    "# Architecture: Input -> 16 -> 8 -> 4 -> Output\n",
    "\n",
    "def create_deep_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create a deeper neural network with multiple hidden layers.\n",
    "    Uses a \"funnel\" architecture (progressively fewer neurons).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Sequential\n",
    "        Compiled neural network model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(input_dim,), name='input_layer'),\n",
    "        \n",
    "        # Hidden layer 1 - widest\n",
    "        Dense(16, activation='relu', name='hidden_layer_1'),\n",
    "        \n",
    "        # Hidden layer 2 - narrower\n",
    "        Dense(8, activation='relu', name='hidden_layer_2'),\n",
    "        \n",
    "        # Hidden layer 3 - narrowest\n",
    "        Dense(4, activation='relu', name='hidden_layer_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ], name='deep_nn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model_deep = create_deep_model(input_dim)\n",
    "\n",
    "# Display summary\n",
    "print(\"Model 2: Deep Neural Network\")\n",
    "print(\"=\"*60)\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Model 3: Wide Neural Network\n",
    "\n",
    "An alternative approach: fewer layers but more neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Wide Neural Network\n",
    "# Architecture: Input -> 32 -> 16 -> Output\n",
    "\n",
    "def create_wide_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create a wide neural network with more neurons per layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Sequential\n",
    "        Compiled neural network model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(input_dim,), name='input_layer'),\n",
    "        \n",
    "        # Hidden layer 1 - wide\n",
    "        Dense(32, activation='relu', name='hidden_layer_1'),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        Dense(16, activation='relu', name='hidden_layer_2'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ], name='wide_nn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model_wide = create_wide_model(input_dim)\n",
    "\n",
    "# Display summary\n",
    "print(\"Model 3: Wide Neural Network\")\n",
    "print(\"=\"*60)\n",
    "model_wide.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model architectures\n",
    "models = {\n",
    "    'Simple NN': model_simple,\n",
    "    'Deep NN': model_deep,\n",
    "    'Wide NN': model_wide\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for name, model in models.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Hidden Layers': len(model.layers) - 1,  # Excluding output\n",
    "        'Total Parameters': model.count_params(),\n",
    "        'Architecture': ' -> '.join([str(l.units) for l in model.layers if hasattr(l, 'units')])\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Architecture Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compiling Models\n",
    "\n",
    "Before training, we must **compile** the model by specifying:\n",
    "1. **Loss function**: What to minimize\n",
    "2. **Optimizer**: How to update weights\n",
    "3. **Metrics**: What to track during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Loss Functions\n",
    "\n",
    "The loss function measures prediction error. For classification:\n",
    "\n",
    "| Problem Type | Loss Function | Keras Name |\n",
    "|:-------------|:--------------|:-----------|\n",
    "| Binary Classification | Binary Cross-Entropy | `'binary_crossentropy'` |\n",
    "| Multi-class (one-hot) | Categorical Cross-Entropy | `'categorical_crossentropy'` |\n",
    "| Multi-class (integers) | Sparse Categorical Cross-Entropy | `'sparse_categorical_crossentropy'` |\n",
    "| Regression | Mean Squared Error | `'mse'` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loss functions\n",
    "print(\"Loss Function for Student Departure Prediction:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nProblem type: Binary classification (departed vs. retained)\")\n",
    "print(\"Output activation: Sigmoid (probability 0-1)\")\n",
    "print(\"Loss function: Binary Cross-Entropy\")\n",
    "print(\"\\nKeras code: loss='binary_crossentropy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Optimizers\n",
    "\n",
    "Optimizers control how weights are updated. Common choices:\n",
    "\n",
    "| Optimizer | Description | When to Use |\n",
    "|:----------|:------------|:------------|\n",
    "| `SGD` | Stochastic Gradient Descent | Simple, requires tuning learning rate |\n",
    "| `Adam` | Adaptive Moment Estimation | Default choice, works well out-of-box |\n",
    "| `RMSprop` | Root Mean Square Propagation | Good for recurrent networks |\n",
    "| `Adagrad` | Adaptive Gradient | Good for sparse data |\n",
    "\n",
    "**Adam** is our default choice - it adapts learning rates automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate optimizer configuration\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# Adam with default learning rate (0.001)\n",
    "optimizer_adam = Adam()\n",
    "print(f\"Adam optimizer:\")\n",
    "print(f\"  Learning rate: {optimizer_adam.learning_rate.numpy()}\")\n",
    "\n",
    "# Adam with custom learning rate\n",
    "optimizer_adam_custom = Adam(learning_rate=0.0005)\n",
    "print(f\"\\nAdam with custom learning rate:\")\n",
    "print(f\"  Learning rate: {optimizer_adam_custom.learning_rate.numpy()}\")\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer_sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
    "print(f\"\\nSGD with momentum:\")\n",
    "print(f\"  Learning rate: {optimizer_sgd.learning_rate.numpy()}\")\n",
    "print(f\"  Momentum: {optimizer_sgd.momentum.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Metrics\n",
    "\n",
    "Metrics are tracked during training but don't affect learning (unlike loss).\n",
    "\n",
    "Common metrics for classification:\n",
    "- `'accuracy'`: Overall correct predictions\n",
    "- `'precision'`: True positives / (True positives + False positives)\n",
    "- `'recall'`: True positives / (True positives + False negatives)\n",
    "- `'AUC'`: Area Under ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all models\n",
    "def compile_model(model, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Compile a Keras model for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Sequential\n",
    "        The model to compile\n",
    "    learning_rate : float\n",
    "        Learning rate for Adam optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Sequential\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Compile all models\n",
    "model_simple = compile_model(create_simple_model(input_dim))\n",
    "model_deep = compile_model(create_deep_model(input_dim))\n",
    "model_wide = compile_model(create_wide_model(input_dim))\n",
    "\n",
    "print(\"All models compiled successfully!\")\n",
    "print(\"\\nCompilation settings:\")\n",
    "print(\"  Optimizer: Adam (learning_rate=0.001)\")\n",
    "print(\"  Loss: binary_crossentropy\")\n",
    "print(\"  Metrics: accuracy, precision, recall, AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View compiled model configuration\n",
    "print(\"Compiled Model Configuration (Simple NN):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimizer: {model_simple.optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {model_simple.optimizer.learning_rate.numpy()}\")\n",
    "print(f\"Loss function: {model_simple.loss}\")\n",
    "print(f\"Metrics: {[m.name for m in model_simple.metrics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture(model, title='Neural Network Architecture'):\n",
    "    \"\"\"\n",
    "    Create a visual representation of a neural network architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        The model to visualize\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Extract layer information\n",
    "    layers_info = []\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'units'):\n",
    "            layers_info.append({\n",
    "                'name': layer.name,\n",
    "                'units': layer.units,\n",
    "                'activation': layer.activation.__name__ if hasattr(layer, 'activation') else 'none'\n",
    "            })\n",
    "    \n",
    "    # Also include input shape\n",
    "    input_shape = model.input_shape[1]\n",
    "    all_layers = [{'name': 'Input', 'units': input_shape, 'activation': 'none'}] + layers_info\n",
    "    \n",
    "    n_layers = len(all_layers)\n",
    "    max_units = max([l['units'] for l in all_layers])\n",
    "    \n",
    "    # Colors\n",
    "    colors = ['lightblue'] + ['lightgreen'] * (n_layers - 2) + ['lightyellow']\n",
    "    border_colors = ['darkblue'] + ['darkgreen'] * (n_layers - 2) + ['orange']\n",
    "    \n",
    "    # Draw connections and neurons\n",
    "    for layer_idx, layer_info in enumerate(all_layers):\n",
    "        n_neurons = min(layer_info['units'], 10)  # Cap display at 10\n",
    "        actual_neurons = layer_info['units']\n",
    "        x = layer_idx * 2\n",
    "        \n",
    "        # Center neurons\n",
    "        start_y = (10 - n_neurons) / 2\n",
    "        \n",
    "        # Draw connections to next layer\n",
    "        if layer_idx < n_layers - 1:\n",
    "            next_neurons = min(all_layers[layer_idx + 1]['units'], 10)\n",
    "            next_start_y = (10 - next_neurons) / 2\n",
    "            \n",
    "            for i in range(n_neurons):\n",
    "                for j in range(next_neurons):\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[x, x + 2],\n",
    "                        y=[start_y + i, next_start_y + j],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='lightgray', width=0.3),\n",
    "                        showlegend=False,\n",
    "                        hoverinfo='skip'\n",
    "                    ))\n",
    "        \n",
    "        # Draw neurons\n",
    "        for neuron_idx in range(n_neurons):\n",
    "            y = start_y + neuron_idx\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[x], y=[y],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=20,\n",
    "                    color=colors[layer_idx],\n",
    "                    line=dict(width=2, color=border_colors[layer_idx])\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ))\n",
    "        \n",
    "        # Add \"...\" if more neurons exist\n",
    "        if actual_neurons > 10:\n",
    "            fig.add_annotation(\n",
    "                x=x, y=start_y + n_neurons,\n",
    "                text=f'...({actual_neurons} total)',\n",
    "                showarrow=False,\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "        \n",
    "        # Add layer label\n",
    "        activation_text = f\"({layer_info['activation']})\" if layer_info['activation'] != 'none' else ''\n",
    "        fig.add_annotation(\n",
    "            x=x, y=-1.5,\n",
    "            text=f\"{layer_info['name']}<br>{actual_neurons} units {activation_text}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=9)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-3, 12]),\n",
    "        height=500,\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Visualize all models\n",
    "fig = visualize_architecture(model_simple, 'Simple Neural Network Architecture')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_architecture(model_deep, 'Deep Neural Network Architecture')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_architecture(model_wide, 'Wide Neural Network Architecture')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures visually\n",
    "models_comparison = {\n",
    "    'Simple NN': model_simple,\n",
    "    'Deep NN': model_deep,\n",
    "    'Wide NN': model_wide\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot parameter counts\n",
    "names = list(models_comparison.keys())\n",
    "params = [m.count_params() for m in models_comparison.values()]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=names,\n",
    "    y=params,\n",
    "    marker_color=['steelblue', 'darkgreen', 'darkorange'],\n",
    "    text=params,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Complexity Comparison: Number of Parameters',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Total Parameters',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for module 5 models\n",
    "import os\n",
    "models_path = f'{module5_filepath}models/'\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save preprocessor\n",
    "pickle.dump(preprocessor, open(f'{models_path}preprocessor.pkl', 'wb'))\n",
    "print(f\"Saved preprocessor to: {models_path}preprocessor.pkl\")\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'minmax_columns': minmax_columns,\n",
    "    'standard_columns': standard_columns,\n",
    "    'categorical_columns': categorical_columns,\n",
    "    'all_features': all_features,\n",
    "    'feature_names_processed': feature_names,\n",
    "    'input_dim': input_dim\n",
    "}\n",
    "pickle.dump(feature_info, open(f'{models_path}feature_info.pkl', 'wb'))\n",
    "print(f\"Saved feature info to: {models_path}feature_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model creation functions (for later training)\n",
    "# In Keras, we typically save weights after training, not the model object\n",
    "# For now, let's save the model configurations\n",
    "\n",
    "model_configs = {\n",
    "    'simple_nn': {\n",
    "        'function': 'create_simple_model',\n",
    "        'layers': [8, 1],\n",
    "        'description': 'Single hidden layer (8 neurons)'\n",
    "    },\n",
    "    'deep_nn': {\n",
    "        'function': 'create_deep_model',\n",
    "        'layers': [16, 8, 4, 1],\n",
    "        'description': 'Three hidden layers (16, 8, 4 neurons)'\n",
    "    },\n",
    "    'wide_nn': {\n",
    "        'function': 'create_wide_model',\n",
    "        'layers': [32, 16, 1],\n",
    "        'description': 'Two hidden layers (32, 16 neurons)'\n",
    "    }\n",
    "}\n",
    "\n",
    "pickle.dump(model_configs, open(f'{models_path}model_configs.pkl', 'wb'))\n",
    "print(f\"Saved model configs to: {models_path}model_configs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data splits for training\n",
    "data_splits = {\n",
    "    'X_train': X_train,\n",
    "    'X_val': X_val,\n",
    "    'y_train': y_train.values,\n",
    "    'y_val': y_val.values\n",
    "}\n",
    "\n",
    "pickle.dump(data_splits, open(f'{models_path}data_splits.pkl', 'wb'))\n",
    "print(f\"Saved data splits to: {models_path}data_splits.pkl\")\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of saved files\n",
    "print(\"Files saved for training:\")\n",
    "print(\"=\"*60)\n",
    "for file in os.listdir(models_path):\n",
    "    filepath = f'{models_path}{file}'\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f\"  {file}: {size/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we built three neural network architectures for predicting student departure using TensorFlow and Keras.\n",
    "\n",
    "### Models Built\n",
    "\n",
    "| Model | Architecture | Parameters | Description |\n",
    "|:------|:-------------|:-----------|:------------|\n",
    "| **Simple NN** | Input -> 8 -> 1 | ~100 | Single hidden layer |\n",
    "| **Deep NN** | Input -> 16 -> 8 -> 4 -> 1 | ~300 | Multiple hidden layers (funnel) |\n",
    "| **Wide NN** | Input -> 32 -> 16 -> 1 | ~700 | Fewer but wider layers |\n",
    "\n",
    "### Key Keras Concepts\n",
    "\n",
    "| Concept | Description | Our Choice |\n",
    "|:--------|:------------|:-----------|\n",
    "| **Sequential API** | Linear stack of layers | Used for all models |\n",
    "| **Dense Layer** | Fully connected neurons | Main building block |\n",
    "| **Activation** | Non-linear transformation | ReLU (hidden), Sigmoid (output) |\n",
    "| **Loss Function** | What to minimize | Binary cross-entropy |\n",
    "| **Optimizer** | How to update weights | Adam (default lr=0.001) |\n",
    "| **Metrics** | What to track | Accuracy, Precision, Recall, AUC |\n",
    "\n",
    "### Model Building Steps\n",
    "\n",
    "1. **Define architecture**: Choose layers and neurons\n",
    "2. **Add layers**: Stack Dense layers with activations\n",
    "3. **Compile**: Specify optimizer, loss, and metrics\n",
    "4. **Ready for training!**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will train these models and learn about:\n",
    "- Epochs and batch sizes\n",
    "- Callbacks for monitoring training\n",
    "- Early stopping to prevent overfitting\n",
    "- Visualizing training history (loss curves)\n",
    "\n",
    "**Proceed to:** `5.3 Train Neural Networks`"
   ]
  }
 ]
}