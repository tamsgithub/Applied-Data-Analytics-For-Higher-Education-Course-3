{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In previous modules, we explored regularized logistic regression, decision trees, and random forests for predicting student departure. Now we introduce **neural networks** (also called **deep learning** when networks have many layers) - a powerful and flexible family of models inspired by the human brain.\n",
    "\n",
    "Neural networks have revolutionized fields like image recognition, natural language processing, and speech recognition. For tabular data like our student departure dataset, they offer an alternative approach that can sometimes capture complex patterns that other models miss.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the connection between logistic regression and neural networks\n",
    "2. Describe the components of a neural network: neurons, layers, and connections\n",
    "3. Understand different activation functions and when to use them\n",
    "4. Explain the backpropagation algorithm at a conceptual level\n",
    "5. Compare neural networks with tree-based models for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Logistic Regression to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Perceptron: The Building Block\n",
    "\n",
    "A **perceptron** (or neuron) is the fundamental building block of neural networks. Remarkably, it's almost identical to logistic regression!\n",
    "\n",
    "**Logistic Regression:**\n",
    "$$\\hat{y} = \\sigma(w_1x_1 + w_2x_2 + ... + w_nx_n + b) = \\sigma(\\vec{w} \\cdot \\vec{x} + b)$$\n",
    "\n",
    "**Perceptron:**\n",
    "$$\\text{output} = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b) = f(\\vec{w} \\cdot \\vec{x} + b)$$\n",
    "\n",
    "The only difference is that neural networks can use different **activation functions** $f$ (not just the sigmoid $\\sigma$).\n",
    "\n",
    "**Key insight**: Logistic regression IS a neural network - specifically, a network with one neuron and a sigmoid activation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Visualize a single neuron/perceptron\n",
    "fig = go.Figure()\n",
    "\n",
    "# Input nodes\n",
    "input_labels = ['x1', 'x2', 'x3', 'bias']\n",
    "input_x = [0, 0, 0, 0]\n",
    "input_y = [3, 2, 1, 0]\n",
    "\n",
    "# Add input nodes\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=input_x, y=input_y,\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=40, color='lightblue', line=dict(width=2, color='darkblue')),\n",
    "    text=input_labels,\n",
    "    textposition='middle left',\n",
    "    name='Inputs'\n",
    "))\n",
    "\n",
    "# Add neuron (output)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[2], y=[1.5],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=60, color='lightgreen', line=dict(width=2, color='darkgreen')),\n",
    "    text=['f(sum)'],\n",
    "    textposition='middle center',\n",
    "    name='Neuron'\n",
    "))\n",
    "\n",
    "# Add output\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[4], y=[1.5],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=40, color='lightyellow', line=dict(width=2, color='orange')),\n",
    "    text=['output'],\n",
    "    textposition='middle right',\n",
    "    name='Output'\n",
    "))\n",
    "\n",
    "# Add connections (weights)\n",
    "weight_labels = ['w1', 'w2', 'w3', 'b']\n",
    "for i, (y, label) in enumerate(zip(input_y, weight_labels)):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0.2, 1.7], y=[y, 1.5],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=2),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    # Add weight label\n",
    "    mid_x = 0.95\n",
    "    mid_y = (y + 1.5) / 2\n",
    "    fig.add_annotation(x=mid_x, y=mid_y, text=label, showarrow=False, \n",
    "                       font=dict(size=12, color='darkblue'))\n",
    "\n",
    "# Add output connection\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[2.3, 3.7], y=[1.5, 1.5],\n",
    "    mode='lines',\n",
    "    line=dict(color='gray', width=2),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='A Single Neuron (Perceptron) - The Building Block',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-1, 5]),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-0.5, 4]),\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How a neuron works:**\n",
    "\n",
    "1. **Inputs** ($x_1, x_2, ..., x_n$): Features from your data\n",
    "2. **Weights** ($w_1, w_2, ..., w_n$): Learned parameters that determine importance\n",
    "3. **Bias** ($b$): An offset term (like the intercept in linear regression)\n",
    "4. **Weighted Sum**: $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n",
    "5. **Activation Function**: $f(z)$ transforms the sum to produce the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Limitation of Single Neurons\n",
    "\n",
    "A single neuron (like logistic regression) can only learn **linear decision boundaries**. It cannot capture complex, non-linear relationships in data.\n",
    "\n",
    "**The XOR Problem**: A classic example where a single neuron fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the XOR problem\n",
    "# XOR: output is 1 when inputs differ, 0 when they're the same\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Linearly Separable (AND/OR)',\n",
    "    'NOT Linearly Separable (XOR)'\n",
    "))\n",
    "\n",
    "# AND problem (linearly separable)\n",
    "and_x = [0, 0, 1, 1]\n",
    "and_y = [0, 1, 0, 1]\n",
    "and_labels = [0, 0, 0, 1]  # AND: only 1 when both inputs are 1\n",
    "and_colors = ['red' if l == 0 else 'green' for l in and_labels]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=and_x, y=and_y,\n",
    "    mode='markers',\n",
    "    marker=dict(size=20, color=and_colors),\n",
    "    showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# Add linear separator for AND\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[-0.2, 1.2], y=[1.2, -0.2],\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=2, dash='dash'),\n",
    "    showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# XOR problem (NOT linearly separable)\n",
    "xor_x = [0, 0, 1, 1]\n",
    "xor_y = [0, 1, 0, 1]\n",
    "xor_labels = [0, 1, 1, 0]  # XOR: 1 when inputs differ\n",
    "xor_colors = ['red' if l == 0 else 'green' for l in xor_labels]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xor_x, y=xor_y,\n",
    "    mode='markers',\n",
    "    marker=dict(size=20, color=xor_colors),\n",
    "    showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "# Add annotation showing no single line can separate\n",
    "fig.add_annotation(\n",
    "    x=0.5, y=0.5, text='No single line\\ncan separate!',\n",
    "    showarrow=False, font=dict(size=12, color='red'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title='x1', range=[-0.3, 1.3])\n",
    "fig.update_yaxes(title='x2', range=[-0.3, 1.3])\n",
    "fig.update_layout(\n",
    "    title='Why We Need Multiple Neurons: The XOR Problem',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: By combining multiple neurons in layers, neural networks can learn complex, non-linear decision boundaries - including XOR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Layers: Input, Hidden, and Output\n",
    "\n",
    "A neural network organizes neurons into **layers**:\n",
    "\n",
    "1. **Input Layer**: Receives the features (no computation, just passes data forward)\n",
    "2. **Hidden Layers**: Where the \"magic\" happens - learns complex patterns\n",
    "3. **Output Layer**: Produces the final prediction\n",
    "\n",
    "**Terminology:**\n",
    "- **Dense/Fully Connected Layer**: Every neuron connects to every neuron in the next layer\n",
    "- **Deep Network**: A network with multiple hidden layers\n",
    "- **Shallow Network**: A network with one or no hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_network(layer_sizes, layer_names=None, title='Neural Network Architecture'):\n",
    "    \"\"\"\n",
    "    Draw a neural network diagram using plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_sizes : list\n",
    "        Number of neurons in each layer (e.g., [10, 8, 4, 1])\n",
    "    layer_names : list, optional\n",
    "        Names for each layer\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    n_layers = len(layer_sizes)\n",
    "    max_neurons = max(layer_sizes)\n",
    "    \n",
    "    # Colors for different layer types\n",
    "    colors = ['lightblue'] + ['lightgreen'] * (n_layers - 2) + ['lightyellow']\n",
    "    border_colors = ['darkblue'] + ['darkgreen'] * (n_layers - 2) + ['orange']\n",
    "    \n",
    "    if layer_names is None:\n",
    "        layer_names = ['Input'] + [f'Hidden {i+1}' for i in range(n_layers - 2)] + ['Output']\n",
    "    \n",
    "    # Draw neurons and connections\n",
    "    for layer_idx, n_neurons in enumerate(layer_sizes):\n",
    "        x = layer_idx * 2\n",
    "        \n",
    "        # Center neurons vertically\n",
    "        start_y = (max_neurons - n_neurons) / 2\n",
    "        \n",
    "        for neuron_idx in range(n_neurons):\n",
    "            y = start_y + neuron_idx\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if layer_idx < n_layers - 1:\n",
    "                next_n_neurons = layer_sizes[layer_idx + 1]\n",
    "                next_start_y = (max_neurons - next_n_neurons) / 2\n",
    "                \n",
    "                for next_neuron_idx in range(next_n_neurons):\n",
    "                    next_y = next_start_y + next_neuron_idx\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[x, x + 2],\n",
    "                        y=[y, next_y],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='lightgray', width=0.5),\n",
    "                        showlegend=False,\n",
    "                        hoverinfo='skip'\n",
    "                    ))\n",
    "            \n",
    "            # Draw neuron\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[x], y=[y],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=25,\n",
    "                    color=colors[layer_idx],\n",
    "                    line=dict(width=2, color=border_colors[layer_idx])\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ))\n",
    "        \n",
    "        # Add layer label\n",
    "        fig.add_annotation(\n",
    "            x=x, y=-1,\n",
    "            text=f'{layer_names[layer_idx]}<br>({n_neurons} neurons)',\n",
    "            showarrow=False,\n",
    "            font=dict(size=10)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        height=500,\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example: A network for our student departure problem\n",
    "# Input: 10 features, Hidden layers: 8 and 4 neurons, Output: 1 (binary classification)\n",
    "fig = draw_neural_network(\n",
    "    layer_sizes=[10, 8, 4, 1],\n",
    "    layer_names=['Input\\n(Features)', 'Hidden 1', 'Hidden 2', 'Output\\n(Departed?)'],\n",
    "    title='Example Neural Network for Student Departure Prediction'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture Explanation:**\n",
    "\n",
    "- **Input Layer (10 neurons)**: One for each feature (GPA, DFW rate, demographics, etc.)\n",
    "- **Hidden Layer 1 (8 neurons)**: Learns initial patterns from raw features\n",
    "- **Hidden Layer 2 (4 neurons)**: Combines patterns into higher-level representations\n",
    "- **Output Layer (1 neuron)**: Produces probability of departure (0 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Network Depth and Width\n",
    "\n",
    "Two key decisions when designing a neural network:\n",
    "\n",
    "**Depth (Number of Layers):**\n",
    "- More layers can learn more complex patterns\n",
    "- But deeper networks are harder to train and prone to overfitting\n",
    "- For tabular data, 1-3 hidden layers is usually sufficient\n",
    "\n",
    "**Width (Neurons per Layer):**\n",
    "- More neurons can capture more nuances\n",
    "- But more neurons mean more parameters and risk of overfitting\n",
    "- Common pattern: start wide, narrow toward output (\"funnel\" shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different architectures\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "    'Shallow & Narrow', 'Medium Depth', 'Deep & Wide'\n",
    "))\n",
    "\n",
    "architectures = [\n",
    "    {'sizes': [10, 4, 1], 'params': 'Few parameters'},\n",
    "    {'sizes': [10, 8, 4, 1], 'params': 'Moderate parameters'},\n",
    "    {'sizes': [10, 16, 8, 4, 1], 'params': 'Many parameters'}\n",
    "]\n",
    "\n",
    "# Calculate parameters for each architecture\n",
    "for arch in architectures:\n",
    "    total_params = 0\n",
    "    for i in range(len(arch['sizes']) - 1):\n",
    "        # weights + biases\n",
    "        total_params += arch['sizes'][i] * arch['sizes'][i+1] + arch['sizes'][i+1]\n",
    "    arch['total_params'] = total_params\n",
    "\n",
    "# Create comparison bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "names = ['Shallow\\n[10, 4, 1]', 'Medium\\n[10, 8, 4, 1]', 'Deep\\n[10, 16, 8, 4, 1]']\n",
    "params = [arch['total_params'] for arch in architectures]\n",
    "layers = [len(arch['sizes']) for arch in architectures]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Total Parameters',\n",
    "    x=names,\n",
    "    y=params,\n",
    "    marker_color='darkblue',\n",
    "    text=params,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Network Architecture Comparison: Parameter Count',\n",
    "    xaxis_title='Architecture',\n",
    "    yaxis_title='Number of Parameters',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Parameter breakdown:\")\n",
    "for name, arch in zip(names, architectures):\n",
    "    print(f\"\\n{name.replace(chr(10), ' ')}:\")\n",
    "    sizes = arch['sizes']\n",
    "    for i in range(len(sizes) - 1):\n",
    "        weights = sizes[i] * sizes[i+1]\n",
    "        biases = sizes[i+1]\n",
    "        print(f\"  Layer {i+1}: {sizes[i]} -> {sizes[i+1]} = {weights} weights + {biases} biases = {weights + biases}\")\n",
    "    print(f\"  Total: {arch['total_params']} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: For our student departure dataset (~10 features), a network with 1-2 hidden layers of 8-32 neurons each is a good starting point. We don't need a very deep network for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Why Activation Functions Matter\n",
    "\n",
    "Without activation functions, a neural network would just be a series of linear transformations - mathematically equivalent to a single linear model!\n",
    "\n",
    "**Linear combination of linear functions = Still linear**\n",
    "\n",
    "$$f(g(x)) = f(ax + b) = c(ax + b) + d = (ca)x + (cb + d)$$\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing neural networks to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Common Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize common activation functions\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Sigmoid', 'Tanh', 'ReLU', 'Leaky ReLU'\n",
    "))\n",
    "\n",
    "# Sigmoid\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid(x), mode='lines', \n",
    "                         line=dict(color='blue', width=3), name='Sigmoid'), row=1, col=1)\n",
    "fig.add_hline(y=0.5, line_dash='dash', line_color='gray', row=1, col=1)\n",
    "\n",
    "# Tanh\n",
    "fig.add_trace(go.Scatter(x=x, y=tanh(x), mode='lines', \n",
    "                         line=dict(color='green', width=3), name='Tanh'), row=1, col=2)\n",
    "fig.add_hline(y=0, line_dash='dash', line_color='gray', row=1, col=2)\n",
    "\n",
    "# ReLU\n",
    "fig.add_trace(go.Scatter(x=x, y=relu(x), mode='lines', \n",
    "                         line=dict(color='red', width=3), name='ReLU'), row=2, col=1)\n",
    "\n",
    "# Leaky ReLU\n",
    "fig.add_trace(go.Scatter(x=x, y=leaky_relu(x), mode='lines', \n",
    "                         line=dict(color='orange', width=3), name='Leaky ReLU'), row=2, col=2)\n",
    "\n",
    "fig.update_xaxes(title='z (input)')\n",
    "fig.update_yaxes(title='f(z) (output)')\n",
    "fig.update_layout(\n",
    "    title='Common Activation Functions',\n",
    "    height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Summary\n",
    "\n",
    "| Function | Formula | Range | Use Case |\n",
    "|:---------|:--------|:------|:---------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | (0, 1) | Output layer for binary classification |\n",
    "| **Tanh** | $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | Hidden layers (centers data) |\n",
    "| **ReLU** | $\\max(0, z)$ | [0, infinity) | Most common for hidden layers |\n",
    "| **Leaky ReLU** | $\\max(0.01z, z)$ | (-infinity, infinity) | Prevents \"dying ReLU\" problem |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Choosing the Right Activation\n",
    "\n",
    "**For Hidden Layers:**\n",
    "- **ReLU** is the default choice (simple, fast, works well)\n",
    "- Use **Leaky ReLU** if you encounter \"dying neurons\" (all outputs become 0)\n",
    "\n",
    "**For Output Layer:**\n",
    "- **Sigmoid**: Binary classification (outputs probability 0-1)\n",
    "- **Softmax**: Multi-class classification (outputs probability distribution)\n",
    "- **Linear (no activation)**: Regression (predicting continuous values)\n",
    "\n",
    "**For our student departure problem:**\n",
    "- Hidden layers: ReLU\n",
    "- Output layer: Sigmoid (binary classification: departed or retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why ReLU is preferred: derivative comparison\n",
    "x = np.linspace(-3, 3, 200)\n",
    "\n",
    "# Derivatives\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Sigmoid Derivative', 'ReLU Derivative'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid_derivative(x), mode='lines',\n",
    "                         line=dict(color='blue', width=3)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=relu_derivative(x), mode='lines',\n",
    "                         line=dict(color='red', width=3)), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title='z')\n",
    "fig.update_yaxes(title=\"f'(z)\")\n",
    "fig.update_layout(\n",
    "    title='Why ReLU Trains Faster: Gradient Comparison',\n",
    "    height=350,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=2.5, y=0.1, text='Max gradient\\n= 0.25',\n",
    "    showarrow=True, arrowhead=2, row=1, col=1\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=1.5, y=1, text='Constant gradient\\n= 1',\n",
    "    showarrow=True, arrowhead=2, row=1, col=2\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Sigmoid's gradient is at most 0.25, while ReLU's gradient is 1 for positive values. This makes ReLU much faster to train (avoids the \"vanishing gradient\" problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How Neural Networks Learn: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn through an iterative process of:\n",
    "1. Making predictions (forward pass)\n",
    "2. Measuring error (loss function)\n",
    "3. Computing gradients (backpropagation)\n",
    "4. Updating weights (optimization)\n",
    "\n",
    "Let's walk through each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Forward Pass\n",
    "\n",
    "Data flows from input to output, layer by layer:\n",
    "\n",
    "1. Input features enter the network\n",
    "2. Each layer computes: $\\text{output} = f(W \\cdot \\text{input} + b)$\n",
    "3. Final layer produces prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate forward pass with a simple example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple network: 2 inputs -> 2 hidden -> 1 output\n",
    "# Initialize weights and biases\n",
    "W1 = np.array([[0.1, 0.2], [0.3, 0.4]])  # 2x2\n",
    "b1 = np.array([0.1, 0.1])  # 2\n",
    "W2 = np.array([[0.5], [0.6]])  # 2x1\n",
    "b2 = np.array([0.1])  # 1\n",
    "\n",
    "# Input example (e.g., GPA=3.5, DFW_rate=0.1)\n",
    "x = np.array([3.5, 0.1])\n",
    "\n",
    "print(\"Forward Pass Example\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nInput: x = {x}\")\n",
    "\n",
    "# Layer 1\n",
    "z1 = np.dot(x, W1) + b1\n",
    "print(f\"\\nLayer 1 (before activation):\")\n",
    "print(f\"  z1 = x * W1 + b1 = {z1}\")\n",
    "\n",
    "a1 = relu(z1)  # ReLU activation\n",
    "print(f\"  a1 = ReLU(z1) = {a1}\")\n",
    "\n",
    "# Layer 2 (output)\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "print(f\"\\nLayer 2 (before activation):\")\n",
    "print(f\"  z2 = a1 * W2 + b2 = {z2}\")\n",
    "\n",
    "output = sigmoid(z2)  # Sigmoid for final output\n",
    "print(f\"  output = Sigmoid(z2) = {output}\")\n",
    "\n",
    "print(f\"\\nFinal prediction: {output[0]:.4f}\")\n",
    "print(f\"Interpretation: {output[0]*100:.1f}% probability of departure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loss Function\n",
    "\n",
    "The **loss function** measures how wrong our predictions are. For binary classification, we use **binary cross-entropy**:\n",
    "\n",
    "$$\\text{Loss} = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "This is the same loss function used in logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binary cross-entropy loss\n",
    "y_pred = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Loss when true label is 1\n",
    "loss_y1 = -np.log(y_pred)\n",
    "# Loss when true label is 0\n",
    "loss_y0 = -np.log(1 - y_pred)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_pred, y=loss_y1,\n",
    "    mode='lines',\n",
    "    name='True Label = 1 (Departed)',\n",
    "    line=dict(color='red', width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_pred, y=loss_y0,\n",
    "    mode='lines',\n",
    "    name='True Label = 0 (Retained)',\n",
    "    line=dict(color='blue', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Binary Cross-Entropy Loss',\n",
    "    xaxis_title='Predicted Probability',\n",
    "    yaxis_title='Loss',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.9, y=-np.log(0.9),\n",
    "    text='Low loss (correct)',\n",
    "    showarrow=True, arrowhead=2\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.1, y=-np.log(0.1),\n",
    "    text='High loss (wrong)',\n",
    "    showarrow=True, arrowhead=2\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: When the true label is 1 (departed), predicting 0.9 gives low loss, while predicting 0.1 gives very high loss. The loss function penalizes confident wrong predictions heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Backward Pass (Backpropagation)\n",
    "\n",
    "**Backpropagation** computes how much each weight contributed to the error using the **chain rule** from calculus.\n",
    "\n",
    "Starting from the output:\n",
    "1. Compute how the loss changes with respect to the output\n",
    "2. Propagate this error backward through each layer\n",
    "3. Calculate gradients for each weight and bias\n",
    "\n",
    "**Intuition**: Backpropagation answers \"How much would the loss change if I slightly changed this weight?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the backpropagation concept\n",
    "fig = go.Figure()\n",
    "\n",
    "# Forward pass arrows (blue)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1, 2, 3], y=[0, 0, 0, 0],\n",
    "    mode='lines+markers+text',\n",
    "    line=dict(color='blue', width=3),\n",
    "    marker=dict(size=30, color='lightblue', line=dict(width=2, color='blue')),\n",
    "    text=['Input', 'Hidden 1', 'Hidden 2', 'Output'],\n",
    "    textposition='bottom center',\n",
    "    name='Forward Pass'\n",
    "))\n",
    "\n",
    "# Add forward arrows\n",
    "for i in range(3):\n",
    "    fig.add_annotation(\n",
    "        x=i+0.5, y=0.1,\n",
    "        ax=i+0.3, ay=0.1,\n",
    "        xref='x', yref='y',\n",
    "        axref='x', ayref='y',\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowcolor='blue'\n",
    "    )\n",
    "\n",
    "# Backward pass arrows (red)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[3, 2, 1, 0], y=[-0.5, -0.5, -0.5, -0.5],\n",
    "    mode='lines+markers+text',\n",
    "    line=dict(color='red', width=3),\n",
    "    marker=dict(size=30, color='lightyellow', line=dict(width=2, color='red')),\n",
    "    text=['Loss', 'Gradient', 'Gradient', 'Gradient'],\n",
    "    textposition='top center',\n",
    "    name='Backward Pass'\n",
    "))\n",
    "\n",
    "# Add backward arrows\n",
    "for i in range(3, 0, -1):\n",
    "    fig.add_annotation(\n",
    "        x=i-0.5, y=-0.4,\n",
    "        ax=i-0.3, ay=-0.4,\n",
    "        xref='x', yref='y',\n",
    "        axref='x', ayref='y',\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowcolor='red'\n",
    "    )\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=1.5, y=0.3,\n",
    "    text='Forward Pass: Compute predictions',\n",
    "    showarrow=False,\n",
    "    font=dict(color='blue', size=14)\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=1.5, y=-0.8,\n",
    "    text='Backward Pass: Compute gradients',\n",
    "    showarrow=False,\n",
    "    font=dict(color='red', size=14)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Backpropagation: Forward and Backward Passes',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-0.5, 3.5]),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-1, 0.6]),\n",
    "    height=350,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Gradient Descent Optimization\n",
    "\n",
    "Once we have gradients, we update weights to minimize the loss:\n",
    "\n",
    "$$w_{new} = w_{old} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
    "\n",
    "Where $\\eta$ is the **learning rate** - how big of steps we take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent with different learning rates\n",
    "def loss_function(w):\n",
    "    return (w - 2)**2 + 1\n",
    "\n",
    "def gradient(w):\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "w_range = np.linspace(-2, 6, 100)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "    'Learning Rate = 0.1 (Too Small)',\n",
    "    'Learning Rate = 0.5 (Just Right)',\n",
    "    'Learning Rate = 1.1 (Too Large)'\n",
    "))\n",
    "\n",
    "learning_rates = [0.1, 0.5, 1.1]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for col, (lr, color) in enumerate(zip(learning_rates, colors), 1):\n",
    "    # Plot loss function\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=w_range, y=loss_function(w_range),\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=2),\n",
    "        showlegend=False\n",
    "    ), row=1, col=col)\n",
    "    \n",
    "    # Simulate gradient descent\n",
    "    w = 5.0  # Start point\n",
    "    path_w = [w]\n",
    "    path_loss = [loss_function(w)]\n",
    "    \n",
    "    for _ in range(10):\n",
    "        w = w - lr * gradient(w)\n",
    "        path_w.append(w)\n",
    "        path_loss.append(loss_function(w))\n",
    "    \n",
    "    # Plot path\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=path_w, y=path_loss,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=color, width=2),\n",
    "        marker=dict(size=8),\n",
    "        showlegend=False\n",
    "    ), row=1, col=col)\n",
    "\n",
    "fig.update_xaxes(title='Weight (w)')\n",
    "fig.update_yaxes(title='Loss')\n",
    "fig.update_layout(\n",
    "    title='Effect of Learning Rate on Gradient Descent',\n",
    "    height=350\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- **Too small**: Slow convergence, may get stuck\n",
    "- **Just right**: Smooth convergence to minimum\n",
    "- **Too large**: Overshoots, may diverge\n",
    "\n",
    "Modern optimizers like **Adam** automatically adjust learning rates during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Networks vs. Traditional ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 When to Use Neural Networks\n",
    "\n",
    "**Neural networks excel at:**\n",
    "- Large datasets (more data = better performance)\n",
    "- Complex patterns with many interactions\n",
    "- Image, text, and sequential data\n",
    "- Problems where feature engineering is difficult\n",
    "\n",
    "**Tree-based models (Random Forests, Gradient Boosting) excel at:**\n",
    "- Smaller tabular datasets\n",
    "- Mixed feature types (categorical + numerical)\n",
    "- When interpretability matters\n",
    "- When training time is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparison with Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Data Size Needed',\n",
    "        'Training Speed',\n",
    "        'Handles Missing Data',\n",
    "        'Feature Scaling Required',\n",
    "        'Handles Categorical Features',\n",
    "        'Interpretability',\n",
    "        'Hyperparameter Tuning',\n",
    "        'Risk of Overfitting',\n",
    "        'Capture Non-linear Patterns',\n",
    "        'GPU Acceleration'\n",
    "    ],\n",
    "    'Neural Networks': [\n",
    "        'Large (1000s+)',\n",
    "        'Slow',\n",
    "        'No (requires preprocessing)',\n",
    "        'Yes (critical)',\n",
    "        'Requires encoding',\n",
    "        'Low (black box)',\n",
    "        'Complex (many parameters)',\n",
    "        'High',\n",
    "        'Excellent',\n",
    "        'Yes'\n",
    "    ],\n",
    "    'Random Forests': [\n",
    "        'Small to Medium (100s)',\n",
    "        'Fast',\n",
    "        'Yes (built-in)',\n",
    "        'No',\n",
    "        'Native support',\n",
    "        'Medium',\n",
    "        'Easier (fewer parameters)',\n",
    "        'Low (ensemble averaging)',\n",
    "        'Good',\n",
    "        'No'\n",
    "    ],\n",
    "    'Logistic Regression': [\n",
    "        'Small (100s)',\n",
    "        'Very Fast',\n",
    "        'No (requires preprocessing)',\n",
    "        'Recommended',\n",
    "        'Requires encoding',\n",
    "        'High (coefficients)',\n",
    "        'Simple (C, penalty)',\n",
    "        'Medium',\n",
    "        'Limited (linear only)',\n",
    "        'No'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison for tabular data\n",
    "models = ['Logistic Regression', 'Random Forest', 'Neural Network']\n",
    "metrics = ['Interpretability', 'Training Speed', 'Small Data Performance', \n",
    "           'Large Data Performance', 'Non-linear Patterns']\n",
    "\n",
    "# Scores (1-5 scale)\n",
    "scores = {\n",
    "    'Logistic Regression': [5, 5, 4, 2, 1],\n",
    "    'Random Forest': [3, 4, 4, 3, 4],\n",
    "    'Neural Network': [1, 2, 2, 5, 5]\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "for model, color in zip(models, colors):\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=scores[model] + [scores[model][0]],  # Close the polygon\n",
    "        theta=metrics + [metrics[0]],\n",
    "        fill='toself',\n",
    "        name=model,\n",
    "        line=dict(color=color)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(visible=True, range=[0, 5])\n",
    "    ),\n",
    "    title='Model Comparison for Tabular Data',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight for Student Departure Prediction:**\n",
    "\n",
    "Our dataset has:\n",
    "- ~5000 students (moderate size)\n",
    "- ~10 features (small)\n",
    "- Mix of numeric and categorical\n",
    "\n",
    "This is actually a case where **tree-based models often perform as well or better** than neural networks. However, neural networks are worth trying because:\n",
    "1. They may capture different patterns\n",
    "2. They can be easily extended with more data\n",
    "3. Understanding neural networks is valuable for more complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Networks for Student Departure Prediction\n",
    "\n",
    "How will we apply neural networks to our student departure problem?\n",
    "\n",
    "**Input Features (10 neurons):**\n",
    "- Academic: HS_GPA, GPA_1, GPA_2, DFW_RATE_1, DFW_RATE_2\n",
    "- Course load: UNITS_ATTEMPTED_1, UNITS_ATTEMPTED_2\n",
    "- Demographics: GENDER, RACE_ETHNICITY, FIRST_GEN_STATUS (one-hot encoded)\n",
    "\n",
    "**Architecture Options:**\n",
    "1. Shallow: Input -> 8 neurons -> Output\n",
    "2. Medium: Input -> 16 -> 8 -> Output\n",
    "3. Deeper: Input -> 32 -> 16 -> 8 -> Output\n",
    "\n",
    "**Output (1 neuron):**\n",
    "- Probability of departure (0 to 1)\n",
    "- Using sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our proposed architecture for student departure\n",
    "fig = draw_neural_network(\n",
    "    layer_sizes=[10, 16, 8, 1],\n",
    "    layer_names=['Input\\n(10 features)', 'Hidden 1\\n(ReLU)', 'Hidden 2\\n(ReLU)', 'Output\\n(Sigmoid)'],\n",
    "    title='Proposed Neural Network for Student Departure Prediction'\n",
    ")\n",
    "\n",
    "# Add feature labels\n",
    "feature_names = ['HS_GPA', 'GPA_1', 'GPA_2', 'DFW_1', 'DFW_2', \n",
    "                 'UNITS_1', 'UNITS_2', 'GENDER', 'RACE', 'FIRST_GEN']\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the upcoming notebooks, we will:**\n",
    "\n",
    "1. **5.2 Build**: Create neural networks using TensorFlow/Keras\n",
    "2. **5.3 Train**: Learn about epochs, batches, and callbacks\n",
    "3. **5.4 Evaluate & Tune**: Compare with our baseline models and optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduced neural networks and their core concepts.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|:--------|:------------|\n",
    "| **Neuron/Perceptron** | Basic unit: weighted sum + activation function |\n",
    "| **Layers** | Input (features), Hidden (learning), Output (prediction) |\n",
    "| **Activation Functions** | Introduce non-linearity (ReLU, Sigmoid, Tanh) |\n",
    "| **Backpropagation** | Algorithm to compute gradients for learning |\n",
    "| **Gradient Descent** | Optimization method to update weights |\n",
    "\n",
    "### Activation Function Guide\n",
    "\n",
    "| Layer Type | Recommended Activation | Why |\n",
    "|:-----------|:-----------------------|:----|\n",
    "| Hidden Layers | ReLU | Fast training, avoids vanishing gradients |\n",
    "| Binary Output | Sigmoid | Outputs probability (0-1) |\n",
    "| Multi-class Output | Softmax | Outputs probability distribution |\n",
    "| Regression Output | Linear (none) | Continuous values |\n",
    "\n",
    "### Neural Networks vs. Other Models\n",
    "\n",
    "For tabular data like student departure:\n",
    "- Neural networks can work well but require more tuning\n",
    "- Tree-based models are often competitive or better\n",
    "- Neural networks shine with large datasets and complex patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will build our first neural network using TensorFlow and Keras.\n",
    "\n",
    "**Proceed to:** `5.2 Build a Neural Network with Keras`"
   ]
  }
 ]
}