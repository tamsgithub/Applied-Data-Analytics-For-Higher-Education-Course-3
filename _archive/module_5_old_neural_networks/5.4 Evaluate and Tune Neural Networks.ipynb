{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 **Evaluate and Tune** Neural Networks - Predict Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the Neural Network architecture with Keras.  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### **4. Evaluate the Model : Assess performance using evaluation metrics.**  \n",
    "### **5. Improve the Model : Tune hyperparameters for optimal performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this final notebook of Module 5, we comprehensively evaluate our trained neural networks and explore techniques to improve their performance.\n",
    "\n",
    "We will:\n",
    "1. Evaluate models on the held-out test set\n",
    "2. Compare neural networks with tree-based models (Random Forest)\n",
    "3. Apply **dropout regularization** to prevent overfitting\n",
    "4. Tune network architecture and hyperparameters\n",
    "5. Provide recommendations for when to use neural networks\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Evaluate neural network performance using multiple metrics\n",
    "2. Compare neural networks with tree-based models for tabular data\n",
    "3. Implement dropout regularization in Keras\n",
    "4. Tune neural network architecture (layers, neurons)\n",
    "5. Make informed decisions about when to use neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Display settings\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "module5_filepath = f'{course3_filepath}module_5/'\n",
    "models_path = f'{module5_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained models\n",
    "model_simple = keras.models.load_model(f'{models_path}simple_nn_trained.keras')\n",
    "model_deep = keras.models.load_model(f'{models_path}deep_nn_trained.keras')\n",
    "model_wide = keras.models.load_model(f'{models_path}wide_nn_trained.keras')\n",
    "\n",
    "models = {\n",
    "    'Simple NN': model_simple,\n",
    "    'Deep NN': model_deep,\n",
    "    'Wide NN': model_wide\n",
    "}\n",
    "\n",
    "print(\"Loaded trained models:\")\n",
    "for name, model in models.items():\n",
    "    print(f\"  {name}: {model.count_params()} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessor and feature info\n",
    "preprocessor = pickle.load(open(f'{models_path}preprocessor.pkl', 'rb'))\n",
    "feature_info = pickle.load(open(f'{models_path}feature_info.pkl', 'rb'))\n",
    "\n",
    "all_features = feature_info['all_features']\n",
    "input_dim = feature_info['input_dim']\n",
    "\n",
    "print(f\"Features: {all_features}\")\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "print(f\"Test data shape: {df_testing.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_testing['SEM_3_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test = df_testing[all_features]\n",
    "y_test = df_testing['SEM_3_STATUS'].values\n",
    "\n",
    "# Transform features\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Test set: {X_test_processed.shape[0]} samples, {X_test_processed.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load training data for later use\n",
    "data_splits = pickle.load(open(f'{models_path}data_splits.pkl', 'rb'))\n",
    "X_train = data_splits['X_train']\n",
    "X_val = data_splits['X_val']\n",
    "y_train = data_splits['y_train']\n",
    "y_val = data_splits['y_val']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_processed.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate on Test Data\n",
    "\n",
    "The test set provides an unbiased estimate of model performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test data\n",
    "test_predictions = {}\n",
    "test_probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Get probability predictions\n",
    "    probs = model.predict(X_test_processed, verbose=0)\n",
    "    test_probabilities[name] = probs.flatten()\n",
    "    \n",
    "    # Convert to class predictions\n",
    "    test_predictions[name] = (probs > 0.5).astype(int).flatten()\n",
    "    \n",
    "print(\"Test predictions generated for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "test_metrics = []\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = test_predictions[name]\n",
    "    y_prob = test_probabilities[name]\n",
    "    \n",
    "    test_metrics.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob)\n",
    "    })\n",
    "\n",
    "test_metrics_df = pd.DataFrame(test_metrics)\n",
    "print(\"Test Set Performance:\")\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=list(models.keys()))\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(test_predictions.items(), 1):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm,\n",
    "            x=['Pred: Retained', 'Pred: Departed'],\n",
    "            y=['True: Retained', 'True: Departed'],\n",
    "            colorscale='Blues',\n",
    "            showscale=False,\n",
    "            text=cm,\n",
    "            texttemplate='%{text}',\n",
    "            textfont=dict(size=14)\n",
    "        ),\n",
    "        row=1, col=idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrices (Test Set)',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "for name in models.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Classification Report: {name}\")\n",
    "    print('='*60)\n",
    "    print(classification_report(y_test, test_predictions[name], \n",
    "                               target_names=['Retained', 'Departed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ROC Curves and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {'Simple NN': 'blue', 'Deep NN': 'green', 'Wide NN': 'orange'}\n",
    "\n",
    "for name in models.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_probabilities[name])\n",
    "    auc = roc_auc_score(y_test, test_probabilities[name])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{name} (AUC={auc:.4f})',\n",
    "        line=dict(color=colors[name], width=2)\n",
    "    ))\n",
    "\n",
    "# Add diagonal reference line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random (AUC=0.5)',\n",
    "    line=dict(color='gray', width=1, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curves - Neural Networks (Test Set)',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare with Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Random Forest Model\n",
    "\n",
    "Let's compare our neural networks with the Random Forest model from Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Random Forest model from Module 3\n",
    "try:\n",
    "    rf_model = pickle.load(open(f'{course3_filepath}models/rf_best_trained_model.pkl', 'rb'))\n",
    "    print(\"Random Forest model loaded successfully.\")\n",
    "    print(f\"Number of trees: {rf_model.named_steps['classifier'].n_estimators}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Random Forest model not found. Training a new one...\")\n",
    "    \n",
    "    # If not found, we'll create and train a Random Forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "    \n",
    "    # Load raw training data\n",
    "    df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "    \n",
    "    # Define columns\n",
    "    minmax_columns = ['HS_GPA', 'GPA_1', 'GPA_2', 'DFW_RATE_1', 'DFW_RATE_2']\n",
    "    standard_columns = ['UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2']\n",
    "    categorical_columns = ['GENDER', 'RACE_ETHNICITY', 'FIRST_GEN_STATUS']\n",
    "    \n",
    "    # Build preprocessor\n",
    "    rf_preprocessor = ColumnTransformer([\n",
    "        ('minmax', MinMaxScaler(), minmax_columns),\n",
    "        ('standard', StandardScaler(), standard_columns),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                  drop=['Female', 'Other', 'Unknown'], \n",
    "                                  sparse_output=False), categorical_columns)\n",
    "    ])\n",
    "    \n",
    "    # Build pipeline\n",
    "    rf_model = Pipeline([\n",
    "        ('preprocessing', rf_preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    rf_model.fit(df_training[all_features], df_training['SEM_3_STATUS'])\n",
    "    print(\"Random Forest trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Random Forest predictions on test data\n",
    "rf_predictions = rf_model.predict(df_testing[all_features])\n",
    "rf_probabilities = rf_model.predict_proba(df_testing[all_features])[:, 1]\n",
    "\n",
    "print(\"Random Forest predictions generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Random Forest to comparison\n",
    "all_models = {\n",
    "    'Simple NN': (test_predictions['Simple NN'], test_probabilities['Simple NN']),\n",
    "    'Deep NN': (test_predictions['Deep NN'], test_probabilities['Deep NN']),\n",
    "    'Wide NN': (test_predictions['Wide NN'], test_probabilities['Wide NN']),\n",
    "    'Random Forest': (rf_predictions, rf_probabilities)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name, (y_pred, y_prob) in all_models.items():\n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Neural Network' if 'NN' in name else 'Tree-Based',\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Model Comparison (Test Set):\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors_all = ['blue', 'green', 'orange', 'purple']\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    values = [row[m] for m in metrics]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=row['Model'],\n",
    "        x=metrics,\n",
    "        y=values,\n",
    "        marker_color=colors_all[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison: Neural Networks vs. Random Forest',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves including Random Forest\n",
    "fig = go.Figure()\n",
    "\n",
    "all_colors = {'Simple NN': 'blue', 'Deep NN': 'green', 'Wide NN': 'orange', 'Random Forest': 'purple'}\n",
    "\n",
    "for name, (y_pred, y_prob) in all_models.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{name} (AUC={auc:.4f})',\n",
    "        line=dict(color=all_colors[name], width=2)\n",
    "    ))\n",
    "\n",
    "# Add diagonal\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random',\n",
    "    line=dict(color='gray', width=1, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curves: Neural Networks vs. Random Forest',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 When to Use Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommendation table\n",
    "recommendations = pd.DataFrame({\n",
    "    'Factor': [\n",
    "        'Dataset Size',\n",
    "        'Feature Types',\n",
    "        'Training Time',\n",
    "        'Interpretability',\n",
    "        'Missing Data',\n",
    "        'Feature Engineering',\n",
    "        'Non-linear Patterns',\n",
    "        'Scalability'\n",
    "    ],\n",
    "    'Random Forest': [\n",
    "        'Works well with smaller datasets (100s-1000s)',\n",
    "        'Handles mixed types natively',\n",
    "        'Generally faster',\n",
    "        'Feature importance available',\n",
    "        'Handles naturally',\n",
    "        'Less needed',\n",
    "        'Good',\n",
    "        'Memory intensive for large forests'\n",
    "    ],\n",
    "    'Neural Network': [\n",
    "        'Needs more data (1000s+)',\n",
    "        'Requires encoding/scaling',\n",
    "        'Slower (requires tuning)',\n",
    "        'Black box (needs SHAP/LIME)',\n",
    "        'Requires imputation',\n",
    "        'Can learn features automatically',\n",
    "        'Excellent',\n",
    "        'GPU acceleration available'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"When to Use Each Model:\")\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for our dataset\n",
    "print(\"Recommendation for Student Departure Prediction:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOur dataset characteristics:\")\n",
    "print(f\"  - Size: ~{len(y_train)} training samples (moderate)\")\n",
    "print(f\"  - Features: {input_dim} (small)\")\n",
    "print(\"  - Feature types: Mixed (numeric + categorical)\")\n",
    "print(\"  - Problem: Binary classification\")\n",
    "print(\"\\nBased on these characteristics:\")\n",
    "print(\"  - Both Random Forest and Neural Networks can work well\")\n",
    "print(\"  - Random Forest may be preferred for:\")\n",
    "print(\"    * Faster training and deployment\")\n",
    "print(\"    * Better interpretability for stakeholders\")\n",
    "print(\"    * Robust performance without extensive tuning\")\n",
    "print(\"  - Neural Networks may be preferred for:\")\n",
    "print(\"    * When more data becomes available\")\n",
    "print(\"    * When deployed as part of larger deep learning system\")\n",
    "print(\"    * When capture complex interactions is critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 What is Dropout?\n",
    "\n",
    "**Dropout** is a regularization technique that randomly \"drops\" neurons during training:\n",
    "\n",
    "- Each neuron has a probability `p` of being temporarily removed\n",
    "- Forces the network to not rely on any single neuron\n",
    "- Creates an implicit ensemble of many sub-networks\n",
    "- At test time, all neurons are used (with scaled weights)\n",
    "\n",
    "**Typical dropout rates:**\n",
    "- Input layer: 0.1-0.2 (lighter dropout)\n",
    "- Hidden layers: 0.2-0.5 (heavier dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dropout concept\n",
    "def visualize_dropout():\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "        'Without Dropout (All Neurons Active)',\n",
    "        'With Dropout (Some Neurons Dropped)'\n",
    "    ))\n",
    "    \n",
    "    # Network without dropout\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Draw neurons\n",
    "    layer_positions = [0, 1.5, 3]\n",
    "    layer_sizes = [4, 6, 2]\n",
    "    \n",
    "    for col in [1, 2]:\n",
    "        dropped = np.zeros((3, 6), dtype=bool)\n",
    "        if col == 2:  # With dropout\n",
    "            dropped[1] = np.random.random(6) < 0.3  # 30% dropout\n",
    "        \n",
    "        for layer_idx, (x, n_neurons) in enumerate(zip(layer_positions, layer_sizes)):\n",
    "            for neuron_idx in range(n_neurons):\n",
    "                y = (6 - n_neurons) / 2 + neuron_idx\n",
    "                is_dropped = dropped[layer_idx, neuron_idx] if neuron_idx < 6 else False\n",
    "                \n",
    "                color = 'lightgray' if is_dropped else (['lightblue', 'lightgreen', 'lightyellow'][layer_idx])\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[x], y=[y],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=25, color=color, line=dict(width=2, color='gray')),\n",
    "                    showlegend=False\n",
    "                ), row=1, col=col)\n",
    "                \n",
    "                # Draw connections\n",
    "                if layer_idx < 2:\n",
    "                    next_size = layer_sizes[layer_idx + 1]\n",
    "                    for next_idx in range(next_size):\n",
    "                        next_y = (6 - next_size) / 2 + next_idx\n",
    "                        \n",
    "                        # Check if either neuron is dropped\n",
    "                        next_dropped = dropped[layer_idx + 1, next_idx] if next_idx < 6 else False\n",
    "                        line_color = 'lightgray' if (is_dropped or next_dropped) else 'gray'\n",
    "                        line_width = 0.5 if (is_dropped or next_dropped) else 1\n",
    "                        \n",
    "                        fig.add_trace(go.Scatter(\n",
    "                            x=[x, layer_positions[layer_idx + 1]],\n",
    "                            y=[y, next_y],\n",
    "                            mode='lines',\n",
    "                            line=dict(color=line_color, width=line_width),\n",
    "                            showlegend=False\n",
    "                        ), row=1, col=col)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Dropout Regularization: Randomly Dropping Neurons During Training',\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = visualize_dropout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with dropout\n",
    "def create_dropout_model(input_dim, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Create a neural network with dropout regularization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "    dropout_rate : float\n",
    "        Probability of dropping neurons (0-1)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden layer 1 with dropout\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Hidden layer 2 with dropout\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Output layer (no dropout)\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='dropout_nn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile\n",
    "model_dropout = create_dropout_model(input_dim, dropout_rate=0.3)\n",
    "model_dropout.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Model with Dropout:\")\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train and Evaluate Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "n_class_0 = (y_train == 0).sum()\n",
    "n_class_1 = (y_train == 1).sum()\n",
    "total = len(y_train)\n",
    "\n",
    "class_weights = {\n",
    "    0: total / (2 * n_class_0),\n",
    "    1: total / (2 * n_class_1)\n",
    "}\n",
    "\n",
    "# Training callbacks\n",
    "callbacks_dropout = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the dropout model\n",
    "print(\"Training Neural Network with Dropout...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_dropout = model_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_dropout,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_dropout = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time_dropout:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dropout model training history\n",
    "hist = history_dropout.history\n",
    "epochs = range(1, len(hist['loss']) + 1)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Loss', 'AUC'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=hist['loss'], name='Train', line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_loss'], name='Val', line=dict(color='red')), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=hist['auc'], name='Train', line=dict(color='blue'), showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_auc'], name='Val', line=dict(color='red'), showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title='Epoch')\n",
    "fig.update_layout(\n",
    "    title='Dropout Neural Network Training History',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate dropout model on test set\n",
    "dropout_probs = model_dropout.predict(X_test_processed, verbose=0).flatten()\n",
    "dropout_preds = (dropout_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"Dropout Neural Network Test Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, dropout_preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, dropout_preds):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, dropout_preds):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, dropout_preds):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, dropout_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Architecture Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Hyperparameter Search Space\n",
    "\n",
    "Key hyperparameters for neural network architecture:\n",
    "\n",
    "| Hyperparameter | Typical Range | Effect |\n",
    "|:---------------|:--------------|:-------|\n",
    "| Number of layers | 1-5 | Complexity of patterns learned |\n",
    "| Neurons per layer | 8-256 | Model capacity |\n",
    "| Dropout rate | 0.1-0.5 | Regularization strength |\n",
    "| Learning rate | 0.0001-0.01 | Training speed/stability |\n",
    "| Batch size | 16-256 | Training dynamics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Manual Architecture Tuning\n",
    "\n",
    "Let's try several different architectures and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures to test\n",
    "architectures = {\n",
    "    'Tiny (8)': [8],\n",
    "    'Small (16)': [16],\n",
    "    'Medium (32, 16)': [32, 16],\n",
    "    'Large (64, 32, 16)': [64, 32, 16],\n",
    "    'Deep (32, 32, 32, 16)': [32, 32, 32, 16],\n",
    "    'Wide (128, 64)': [128, 64]\n",
    "}\n",
    "\n",
    "def create_model_from_architecture(input_dim, hidden_layers, dropout_rate=0.2):\n",
    "    \"\"\"Create a model from a list of hidden layer sizes.\"\"\"\n",
    "    layers = [Input(shape=(input_dim,))]\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        layers.append(Dense(units, activation='relu'))\n",
    "        layers.append(Dropout(dropout_rate))\n",
    "    \n",
    "    layers.append(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Architectures to test:\")\n",
    "for name, layers in architectures.items():\n",
    "    model = create_model_from_architecture(input_dim, layers)\n",
    "    print(f\"  {name}: {model.count_params()} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each architecture\n",
    "architecture_results = []\n",
    "\n",
    "for name, hidden_layers in architectures.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model_from_architecture(input_dim, hidden_layers)\n",
    "    \n",
    "    # Train with early stopping\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    probs = model.predict(X_test_processed, verbose=0).flatten()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    architecture_results.append({\n",
    "        'Architecture': name,\n",
    "        'Hidden Layers': str(hidden_layers),\n",
    "        'Parameters': model.count_params(),\n",
    "        'Epochs': len(history.history['loss']),\n",
    "        'Train Time (s)': round(training_time, 2),\n",
    "        'Test Accuracy': round(accuracy_score(y_test, preds), 4),\n",
    "        'Test AUC': round(roc_auc_score(y_test, probs), 4)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test AUC: {roc_auc_score(y_test, probs):.4f}\")\n",
    "\n",
    "print(\"\\nArchitecture tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "arch_df = pd.DataFrame(architecture_results)\n",
    "arch_df = arch_df.sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"Architecture Tuning Results (sorted by Test AUC):\")\n",
    "arch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize architecture comparison\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Test AUC by Architecture',\n",
    "    'Parameters vs. Performance'\n",
    "))\n",
    "\n",
    "# Bar chart of AUC\n",
    "fig.add_trace(go.Bar(\n",
    "    x=arch_df['Architecture'],\n",
    "    y=arch_df['Test AUC'],\n",
    "    marker_color='steelblue',\n",
    "    text=arch_df['Test AUC'].round(4),\n",
    "    textposition='outside'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Scatter plot of parameters vs AUC\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=arch_df['Parameters'],\n",
    "    y=arch_df['Test AUC'],\n",
    "    mode='markers+text',\n",
    "    text=arch_df['Architecture'],\n",
    "    textposition='top center',\n",
    "    marker=dict(size=15, color='steelblue')\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title='Architecture', row=1, col=1)\n",
    "fig.update_xaxes(title='Number of Parameters', row=1, col=2)\n",
    "fig.update_yaxes(title='Test AUC')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Neural Network Architecture Comparison',\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best architecture\n",
    "best_arch = arch_df.iloc[0]\n",
    "print(f\"Best Architecture: {best_arch['Architecture']}\")\n",
    "print(f\"  Hidden Layers: {best_arch['Hidden Layers']}\")\n",
    "print(f\"  Parameters: {best_arch['Parameters']}\")\n",
    "print(f\"  Test AUC: {best_arch['Test AUC']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates with the best architecture\n",
    "learning_rates = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "\n",
    "# Get best architecture\n",
    "best_layers = eval(best_arch['Hidden Layers'])  # Convert string back to list\n",
    "\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model_from_architecture(input_dim, best_layers, dropout_rate=0.2)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    probs = model.predict(X_test_processed, verbose=0).flatten()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Epochs': len(history.history['loss']),\n",
    "        'Test Accuracy': round(accuracy_score(y_test, preds), 4),\n",
    "        'Test AUC': round(roc_auc_score(y_test, probs), 4)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test AUC: {roc_auc_score(y_test, probs):.4f}\")\n",
    "\n",
    "print(\"\\nLearning rate tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display learning rate results\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "lr_df = lr_df.sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"Learning Rate Tuning Results:\")\n",
    "lr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate results\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=lr_df['Learning Rate'],\n",
    "    y=lr_df['Test AUC'],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=12, color='steelblue'),\n",
    "    line=dict(color='steelblue', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Test AUC vs. Learning Rate',\n",
    "    xaxis_title='Learning Rate',\n",
    "    yaxis_title='Test AUC',\n",
    "    xaxis_type='log',  # Log scale for learning rate\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results including tuned models\n",
    "final_comparison = []\n",
    "\n",
    "# Original models\n",
    "for name, (y_pred, y_prob) in all_models.items():\n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Neural Network' if 'NN' in name else 'Tree-Based',\n",
    "        'Test AUC': round(roc_auc_score(y_test, y_prob), 4),\n",
    "        'Test F1': round(f1_score(y_test, y_pred), 4)\n",
    "    })\n",
    "\n",
    "# Dropout model\n",
    "final_comparison.append({\n",
    "    'Model': 'Dropout NN',\n",
    "    'Type': 'Neural Network',\n",
    "    'Test AUC': round(roc_auc_score(y_test, dropout_probs), 4),\n",
    "    'Test F1': round(f1_score(y_test, dropout_preds), 4)\n",
    "})\n",
    "\n",
    "# Best tuned architecture\n",
    "final_comparison.append({\n",
    "    'Model': f'Tuned NN ({best_arch[\"Architecture\"]})',\n",
    "    'Type': 'Neural Network (Tuned)',\n",
    "    'Test AUC': best_arch['Test AUC'],\n",
    "    'Test F1': round(f1_score(y_test, preds), 4)  # from last trained model\n",
    "})\n",
    "\n",
    "final_df = pd.DataFrame(final_comparison)\n",
    "final_df = final_df.sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"Final Model Comparison (sorted by Test AUC):\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Color by type\n",
    "colors_final = ['blue' if 'Neural' in t else 'green' for t in final_df['Type']]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=final_df['Model'],\n",
    "    y=final_df['Test AUC'],\n",
    "    marker_color=colors_final,\n",
    "    text=final_df['Test AUC'],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Final Model Comparison: All Models (Test Set)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Test AUC',\n",
    "    yaxis=dict(range=[0.5, 1.0]),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.5, y=0.95,\n",
    "    xref='paper', yref='paper',\n",
    "    text='Blue = Neural Network, Green = Tree-Based',\n",
    "    showarrow=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommendations\n",
    "print(\"Neural Network Best Practices for Student Departure Prediction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATA PREPARATION\")\n",
    "print(\"   - Always scale numeric features (MinMax or Standard scaling)\")\n",
    "print(\"   - One-hot encode categorical variables\")\n",
    "print(\"   - Use class weights for imbalanced data\")\n",
    "\n",
    "print(\"\\n2. ARCHITECTURE SELECTION\")\n",
    "print(\"   - Start simple (1-2 hidden layers)\")\n",
    "print(\"   - For tabular data: 16-64 neurons per layer is usually sufficient\")\n",
    "print(\"   - Use 'funnel' shape (wider at input, narrower toward output)\")\n",
    "\n",
    "print(\"\\n3. REGULARIZATION\")\n",
    "print(\"   - Use dropout (0.2-0.3 is a good starting point)\")\n",
    "print(\"   - Early stopping prevents overfitting automatically\")\n",
    "print(\"   - Consider L2 regularization on Dense layers\")\n",
    "\n",
    "print(\"\\n4. TRAINING\")\n",
    "print(\"   - Use Adam optimizer (adaptive learning rate)\")\n",
    "print(\"   - Batch size of 32 works well for most cases\")\n",
    "print(\"   - Set max epochs high and use early stopping\")\n",
    "\n",
    "print(\"\\n5. EVALUATION\")\n",
    "print(\"   - Always use a held-out test set for final evaluation\")\n",
    "print(\"   - Monitor multiple metrics (AUC, F1, not just accuracy)\")\n",
    "print(\"   - Plot learning curves to diagnose issues\")\n",
    "\n",
    "print(\"\\n6. MODEL SELECTION\")\n",
    "print(\"   - For small-medium tabular datasets, tree-based models often match NN\")\n",
    "print(\"   - Neural networks shine with more data and complex patterns\")\n",
    "print(\"   - Consider ensemble of both approaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best neural network model\n",
    "# Get the best performing NN based on final comparison\n",
    "nn_results = final_df[final_df['Type'].str.contains('Neural')]\n",
    "best_nn = nn_results.iloc[0]\n",
    "\n",
    "print(f\"Best Neural Network Model: {best_nn['Model']}\")\n",
    "print(f\"Test AUC: {best_nn['Test AUC']}\")\n",
    "\n",
    "# Save the dropout model as it's likely one of the best\n",
    "model_dropout.save(f'{models_path}best_neural_network.keras')\n",
    "print(f\"\\nSaved best neural network to: {models_path}best_neural_network.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluated and tuned neural networks for student departure prediction.\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Test Set Performance Comparison:**\n",
    "\n",
    "| Model Type | Best AUC | Notes |\n",
    "|:-----------|:---------|:------|\n",
    "| Random Forest | Check above | Robust, interpretable |\n",
    "| Simple NN | Check above | Fast, few parameters |\n",
    "| Deep NN | Check above | More complex patterns |\n",
    "| Dropout NN | Check above | Better regularization |\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "| Technique | Purpose | Implementation |\n",
    "|:----------|:--------|:---------------|\n",
    "| **Dropout** | Prevents over-reliance on neurons | `Dropout(0.3)` layer |\n",
    "| **Early Stopping** | Stops when val_loss plateaus | `EarlyStopping(patience=15)` |\n",
    "| **Learning Rate Decay** | Finer convergence | `ReduceLROnPlateau()` |\n",
    "| **Class Weights** | Handles imbalance | `class_weight={0: w0, 1: w1}` |\n",
    "\n",
    "### Architecture Tuning Insights\n",
    "\n",
    "1. **Depth**: 2-3 hidden layers often sufficient for tabular data\n",
    "2. **Width**: 16-64 neurons per layer is reasonable\n",
    "3. **Shape**: Funnel (wide to narrow) works well\n",
    "4. **More parameters != better performance**\n",
    "\n",
    "### Neural Networks vs. Tree-Based Models\n",
    "\n",
    "For our student departure dataset:\n",
    "- Both approaches achieve competitive performance\n",
    "- Tree-based models are faster and more interpretable\n",
    "- Neural networks may improve with more data\n",
    "- Consider ensemble approaches for best results\n",
    "\n",
    "### Module 5 Complete!\n",
    "\n",
    "You have now learned:\n",
    "- How neural networks work (perceptrons, layers, backpropagation)\n",
    "- How to build networks with Keras Sequential API\n",
    "- How to train with epochs, batches, and callbacks\n",
    "- How to evaluate and tune neural network architectures\n",
    "- When to use neural networks vs. other approaches\n",
    "\n",
    "**Congratulations on completing Module 5!**"
   ]
  }
 ]
}