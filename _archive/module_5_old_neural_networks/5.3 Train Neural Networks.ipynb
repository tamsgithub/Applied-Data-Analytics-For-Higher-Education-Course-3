{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 **Train** Neural Networks - Predict Student Departure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the Neural Network architecture with Keras.  \n",
    "### **2. Train the Model : Fit the model on the training data.**  \n",
    "### **3. Generate Predictions : Use the trained model to make predictions.**  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook, we built neural network architectures using Keras. Now we train these models on our student departure data.\n",
    "\n",
    "Training neural networks involves several concepts that differ from traditional machine learning:\n",
    "- **Epochs**: Multiple passes through the entire dataset\n",
    "- **Batch size**: Processing data in chunks\n",
    "- **Callbacks**: Hooks to monitor and control training\n",
    "- **Early stopping**: Preventing overfitting automatically\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand epochs, batch sizes, and their effects on training\n",
    "2. Use callbacks to monitor and control the training process\n",
    "3. Implement early stopping to prevent overfitting\n",
    "4. Visualize training history with loss curves\n",
    "5. Generate predictions from trained neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, History\n",
    "\n",
    "# Display settings\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_filepath = f'{root_filepath}course_3/'\n",
    "module5_filepath = f'{course3_filepath}module_5/'\n",
    "models_path = f'{module5_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved data splits from previous notebook\n",
    "data_splits = pickle.load(open(f'{models_path}data_splits.pkl', 'rb'))\n",
    "\n",
    "X_train = data_splits['X_train']\n",
    "X_val = data_splits['X_val']\n",
    "y_train = data_splits['y_train']\n",
    "y_val = data_splits['y_val']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"\\nTarget distribution (training):\")\n",
    "print(f\"  Class 0 (Retained): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Departed): {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature information\n",
    "feature_info = pickle.load(open(f'{models_path}feature_info.pkl', 'rb'))\n",
    "input_dim = feature_info['input_dim']\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Training Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Epochs\n",
    "\n",
    "An **epoch** is one complete pass through the entire training dataset.\n",
    "\n",
    "- **1 epoch** = model has seen every training sample once\n",
    "- **10 epochs** = model has seen every training sample 10 times\n",
    "\n",
    "**How many epochs?**\n",
    "- Too few: Model hasn't learned enough (underfitting)\n",
    "- Too many: Model memorizes training data (overfitting)\n",
    "- Solution: Use **early stopping** to find the sweet spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the concept of epochs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate training and validation loss over epochs\n",
    "epochs = np.arange(1, 101)\n",
    "train_loss = 0.7 * np.exp(-epochs/20) + 0.1 + np.random.normal(0, 0.02, len(epochs))\n",
    "val_loss = 0.7 * np.exp(-epochs/25) + 0.15 + 0.003 * epochs + np.random.normal(0, 0.02, len(epochs))\n",
    "\n",
    "# Find optimal stopping point\n",
    "optimal_epoch = np.argmin(val_loss) + 1\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs, y=train_loss,\n",
    "    mode='lines',\n",
    "    name='Training Loss',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs, y=val_loss,\n",
    "    mode='lines',\n",
    "    name='Validation Loss',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Add regions\n",
    "fig.add_vrect(x0=1, x1=20, fillcolor='yellow', opacity=0.2, layer='below', line_width=0)\n",
    "fig.add_vrect(x0=optimal_epoch, x1=100, fillcolor='red', opacity=0.1, layer='below', line_width=0)\n",
    "\n",
    "fig.add_vline(x=optimal_epoch, line_dash='dash', line_color='green', \n",
    "              annotation_text=f'Optimal: Epoch {optimal_epoch}')\n",
    "\n",
    "fig.add_annotation(x=10, y=0.55, text='Underfitting', showarrow=False, font=dict(size=12))\n",
    "fig.add_annotation(x=70, y=0.55, text='Overfitting', showarrow=False, font=dict(size=12))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training vs. Validation Loss Over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Batch Size\n",
    "\n",
    "**Batch size** is the number of samples processed before updating weights.\n",
    "\n",
    "| Batch Size | Description | Trade-offs |\n",
    "|:-----------|:------------|:-----------|\n",
    "| **1** (Stochastic) | Update after each sample | Noisy but fast updates |\n",
    "| **32-256** (Mini-batch) | Update after small batches | Balance of speed and stability |\n",
    "| **All data** (Batch) | Update after entire dataset | Stable but slow |\n",
    "\n",
    "**Common choices**: 32, 64, 128, or 256. We'll use **32** as our default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch size effects\n",
    "n_samples = X_train.shape[0]\n",
    "batch_sizes = [1, 16, 32, 64, 128, 256, n_samples]\n",
    "\n",
    "data = []\n",
    "for bs in batch_sizes:\n",
    "    iterations_per_epoch = np.ceil(n_samples / bs)\n",
    "    data.append({\n",
    "        'Batch Size': bs if bs != n_samples else 'Full',\n",
    "        'Iterations per Epoch': int(iterations_per_epoch),\n",
    "        'Memory Usage': 'Low' if bs <= 32 else ('Medium' if bs <= 128 else 'High'),\n",
    "        'Update Stability': 'Noisy' if bs <= 16 else ('Balanced' if bs <= 128 else 'Stable')\n",
    "    })\n",
    "\n",
    "batch_df = pd.DataFrame(data)\n",
    "print(f\"Batch Size Analysis (Training samples: {n_samples})\")\n",
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient updates with different batch sizes\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate optimization path with different batch sizes\n",
    "def simulate_optimization(batch_size, n_steps=50):\n",
    "    path = [(5, 5)]  # Starting point\n",
    "    noise_scale = 1.0 / np.sqrt(batch_size)  # Smaller batch = more noise\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        x, y = path[-1]\n",
    "        # Move toward minimum (0, 0) with noise\n",
    "        dx = -0.1 * x + np.random.normal(0, noise_scale)\n",
    "        dy = -0.1 * y + np.random.normal(0, noise_scale)\n",
    "        path.append((x + dx, y + dy))\n",
    "    \n",
    "    return path\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "    'Small Batch (8)', 'Medium Batch (64)', 'Large Batch (256)'\n",
    "))\n",
    "\n",
    "batch_configs = [(8, 'red'), (64, 'green'), (256, 'blue')]\n",
    "\n",
    "for col, (bs, color) in enumerate(batch_configs, 1):\n",
    "    path = simulate_optimization(bs)\n",
    "    x_path = [p[0] for p in path]\n",
    "    y_path = [p[1] for p in path]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_path, y=y_path,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=color, width=1),\n",
    "        marker=dict(size=4),\n",
    "        showlegend=False\n",
    "    ), row=1, col=col)\n",
    "    \n",
    "    # Add target (minimum)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0], y=[0],\n",
    "        mode='markers',\n",
    "        marker=dict(size=15, color='gold', symbol='star'),\n",
    "        showlegend=False\n",
    "    ), row=1, col=col)\n",
    "\n",
    "fig.update_xaxes(range=[-3, 6])\n",
    "fig.update_yaxes(range=[-3, 6])\n",
    "fig.update_layout(\n",
    "    title='Optimization Path with Different Batch Sizes (Star = Minimum)',\n",
    "    height=350\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- **Small batch**: Noisy path but explores more (can escape local minima)\n",
    "- **Medium batch**: Good balance of exploration and stability\n",
    "- **Large batch**: Stable path but may converge to sharp minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Iterations vs. Epochs\n",
    "\n",
    "Important distinction:\n",
    "- **Iteration** (or step): One batch processed, one weight update\n",
    "- **Epoch**: All batches processed, entire dataset seen once\n",
    "\n",
    "$$\\text{Iterations per epoch} = \\lceil \\frac{\\text{Number of samples}}{\\text{Batch size}} \\rceil$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate iterations for our data\n",
    "batch_size = 32\n",
    "n_samples = X_train.shape[0]\n",
    "iterations_per_epoch = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {n_samples}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Iterations per epoch: {iterations_per_epoch}\")\n",
    "print(f\"\\nFor 100 epochs:\")\n",
    "print(f\"  Total iterations: {iterations_per_epoch * 100:,}\")\n",
    "print(f\"  Total weight updates: {iterations_per_epoch * 100:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Callbacks: Monitoring and Controlling Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What are Callbacks?\n",
    "\n",
    "**Callbacks** are functions called at specific points during training:\n",
    "- At the start/end of training\n",
    "- At the start/end of each epoch\n",
    "- At the start/end of each batch\n",
    "\n",
    "**Common uses:**\n",
    "- Stop training early if no improvement\n",
    "- Save model weights at checkpoints\n",
    "- Adjust learning rate during training\n",
    "- Log metrics to visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Early Stopping\n",
    "\n",
    "**Early stopping** prevents overfitting by stopping training when validation performance stops improving.\n",
    "\n",
    "```python\n",
    "EarlyStopping(\n",
    "    monitor='val_loss',      # What metric to watch\n",
    "    patience=10,             # Epochs to wait before stopping\n",
    "    restore_best_weights=True,  # Restore best model weights\n",
    "    min_delta=0.001          # Minimum change to qualify as improvement\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Monitor validation loss\n",
    "    patience=15,                  # Wait 15 epochs for improvement\n",
    "    restore_best_weights=True,    # Restore best weights when stopped\n",
    "    min_delta=0.001,              # Minimum change to count as improvement\n",
    "    verbose=1                     # Print when early stopping triggers\n",
    ")\n",
    "\n",
    "print(\"Early Stopping Configuration:\")\n",
    "print(f\"  Monitor: {early_stopping.monitor}\")\n",
    "print(f\"  Patience: {early_stopping.patience} epochs\")\n",
    "print(f\"  Restore best weights: {early_stopping.restore_best_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how early stopping works\n",
    "np.random.seed(42)\n",
    "\n",
    "epochs = np.arange(1, 81)\n",
    "val_loss = 0.6 * np.exp(-epochs/20) + 0.15 + 0.002 * epochs + np.random.normal(0, 0.015, len(epochs))\n",
    "\n",
    "# Find where early stopping would trigger (patience=15)\n",
    "best_epoch = np.argmin(val_loss[:50]) + 1  # Minimum in first 50 epochs\n",
    "stop_epoch = best_epoch + 15  # Patience of 15\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot full loss curve (faded for epochs after stopping)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs, y=val_loss,\n",
    "    mode='lines',\n",
    "    name='Validation Loss (if no early stopping)',\n",
    "    line=dict(color='lightcoral', width=2, dash='dot')\n",
    "))\n",
    "\n",
    "# Plot loss up to early stopping\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs[:stop_epoch], y=val_loss[:stop_epoch],\n",
    "    mode='lines',\n",
    "    name='Validation Loss (with early stopping)',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Mark best epoch\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[best_epoch], y=[val_loss[best_epoch-1]],\n",
    "    mode='markers',\n",
    "    name=f'Best Epoch ({best_epoch})',\n",
    "    marker=dict(size=15, color='green', symbol='star')\n",
    "))\n",
    "\n",
    "# Mark stopping point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[stop_epoch], y=[val_loss[stop_epoch-1]],\n",
    "    mode='markers',\n",
    "    name=f'Early Stop ({stop_epoch})',\n",
    "    marker=dict(size=12, color='orange', symbol='x')\n",
    "))\n",
    "\n",
    "# Add patience region\n",
    "fig.add_vrect(x0=best_epoch, x1=stop_epoch, fillcolor='yellow', opacity=0.2, \n",
    "              layer='below', line_width=0,\n",
    "              annotation_text='Patience Period (15 epochs)', annotation_position='top')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='How Early Stopping Works',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Validation Loss',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Checkpoint\n",
    "\n",
    "**ModelCheckpoint** saves model weights at specified intervals.\n",
    "\n",
    "```python\n",
    "ModelCheckpoint(\n",
    "    filepath='best_model.keras',  # Where to save\n",
    "    monitor='val_loss',           # What metric to watch\n",
    "    save_best_only=True,          # Only save if improved\n",
    "    save_weights_only=False       # Save entire model or just weights\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model checkpoint callback\n",
    "import os\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "def create_checkpoint(model_name):\n",
    "    return ModelCheckpoint(\n",
    "        filepath=f'{models_path}{model_name}_best.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "print(\"Model checkpoint will save the best model to:\") \n",
    "print(f\"  {models_path}[model_name]_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Learning Rate Scheduler\n",
    "\n",
    "**ReduceLROnPlateau** reduces learning rate when a metric stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate reducer callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,       # Reduce LR by half\n",
    "    patience=5,       # Wait 5 epochs\n",
    "    min_lr=0.00001,   # Minimum learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Learning Rate Reducer Configuration:\")\n",
    "print(f\"  Monitor: {reduce_lr.monitor}\")\n",
    "print(f\"  Factor: {reduce_lr.factor} (reduce by half)\")\n",
    "print(f\"  Patience: {reduce_lr.patience} epochs\")\n",
    "print(f\"  Minimum LR: {reduce_lr.min_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Recreate the Models\n",
    "\n",
    "Let's recreate our three model architectures from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation functions\n",
    "def create_simple_model(input_dim):\n",
    "    \"\"\"Simple neural network: 1 hidden layer.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='simple_nn')\n",
    "    return model\n",
    "\n",
    "def create_deep_model(input_dim):\n",
    "    \"\"\"Deep neural network: 3 hidden layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='deep_nn')\n",
    "    return model\n",
    "\n",
    "def create_wide_model(input_dim):\n",
    "    \"\"\"Wide neural network: 2 wide hidden layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='wide_nn')\n",
    "    return model\n",
    "\n",
    "def compile_model(model, learning_rate=0.001):\n",
    "    \"\"\"Compile model for binary classification.\"\"\"\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Model creation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100           # Maximum epochs (early stopping will likely stop before)\n",
    "BATCH_SIZE = 32        # Samples per batch\n",
    "LEARNING_RATE = 0.001  # Initial learning rate\n",
    "\n",
    "# Class weights to handle imbalance\n",
    "# Calculate class weights\n",
    "n_class_0 = (y_train == 0).sum()\n",
    "n_class_1 = (y_train == 1).sum()\n",
    "total = len(y_train)\n",
    "\n",
    "class_weights = {\n",
    "    0: total / (2 * n_class_0),\n",
    "    1: total / (2 * n_class_1)\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nClass weights (to handle imbalance):\")\n",
    "print(f\"  Class 0 (Retained): {class_weights[0]:.4f}\")\n",
    "print(f\"  Class 1 (Departed): {class_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile Simple NN\n",
    "model_simple = create_simple_model(input_dim)\n",
    "model_simple = compile_model(model_simple, LEARNING_RATE)\n",
    "\n",
    "# Create callbacks for this model\n",
    "callbacks_simple = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    create_checkpoint('simple_nn'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Simple Neural Network:\")\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Simple NN\n",
    "print(\"\\nTraining Simple Neural Network...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_simple = model_simple.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_simple,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_simple = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time_simple:.2f} seconds\")\n",
    "print(f\"Final epoch: {len(history_simple.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile Deep NN\n",
    "model_deep = create_deep_model(input_dim)\n",
    "model_deep = compile_model(model_deep, LEARNING_RATE)\n",
    "\n",
    "# Create callbacks for this model\n",
    "callbacks_deep = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    create_checkpoint('deep_nn'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Deep Neural Network:\")\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Deep NN\n",
    "print(\"\\nTraining Deep Neural Network...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_deep = model_deep.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_deep,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_deep = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time_deep:.2f} seconds\")\n",
    "print(f\"Final epoch: {len(history_deep.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train Wide Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile Wide NN\n",
    "model_wide = create_wide_model(input_dim)\n",
    "model_wide = compile_model(model_wide, LEARNING_RATE)\n",
    "\n",
    "# Create callbacks for this model\n",
    "callbacks_wide = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    create_checkpoint('wide_nn'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Wide Neural Network:\")\n",
    "model_wide.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Wide NN\n",
    "print(\"\\nTraining Wide Neural Network...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_wide = model_wide.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_wide,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_wide = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time_wide:.2f} seconds\")\n",
    "print(f\"Final epoch: {len(history_wide.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loss Curves\n",
    "\n",
    "Loss curves show how the loss changes during training. They help diagnose:\n",
    "- **Underfitting**: Both losses high and not decreasing\n",
    "- **Overfitting**: Training loss low, validation loss increasing\n",
    "- **Good fit**: Both losses decrease and stabilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras.callbacks.History\n",
    "        Training history object\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist['loss']) + 1)\n",
    "    \n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "        'Loss', 'Accuracy', 'Precision', 'AUC'\n",
    "    ))\n",
    "    \n",
    "    # Loss\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['loss'], name='Train Loss', \n",
    "                             line=dict(color='blue')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_loss'], name='Val Loss', \n",
    "                             line=dict(color='red')), row=1, col=1)\n",
    "    \n",
    "    # Accuracy\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['accuracy'], name='Train Acc', \n",
    "                             line=dict(color='blue'), showlegend=False), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_accuracy'], name='Val Acc', \n",
    "                             line=dict(color='red'), showlegend=False), row=1, col=2)\n",
    "    \n",
    "    # Precision\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['precision'], name='Train Prec', \n",
    "                             line=dict(color='blue'), showlegend=False), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_precision'], name='Val Prec', \n",
    "                             line=dict(color='red'), showlegend=False), row=2, col=1)\n",
    "    \n",
    "    # AUC\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['auc'], name='Train AUC', \n",
    "                             line=dict(color='blue'), showlegend=False), row=2, col=2)\n",
    "    fig.add_trace(go.Scatter(x=list(epochs), y=hist['val_auc'], name='Val AUC', \n",
    "                             line=dict(color='red'), showlegend=False), row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title='Epoch')\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot Simple NN history\n",
    "fig = plot_training_history(history_simple, 'Simple Neural Network Training History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Deep NN history\n",
    "fig = plot_training_history(history_deep, 'Deep Neural Network Training History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Wide NN history\n",
    "fig = plot_training_history(history_wide, 'Wide Neural Network Training History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Metric Curves\n",
    "\n",
    "Let's compare all three models' loss curves side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models' loss curves\n",
    "histories = {\n",
    "    'Simple NN': history_simple,\n",
    "    'Deep NN': history_deep,\n",
    "    'Wide NN': history_wide\n",
    "}\n",
    "\n",
    "colors = {'Simple NN': 'blue', 'Deep NN': 'green', 'Wide NN': 'orange'}\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Training Loss', 'Validation Loss'))\n",
    "\n",
    "for name, history in histories.items():\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    color = colors[name]\n",
    "    \n",
    "    # Training loss\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(epochs), y=history.history['loss'],\n",
    "        name=f'{name}', line=dict(color=color, width=2)\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Validation loss\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(epochs), y=history.history['val_loss'],\n",
    "        name=f'{name}', line=dict(color=color, width=2),\n",
    "        showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title='Epoch')\n",
    "fig.update_yaxes(title='Loss')\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Loss Curves',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare validation AUC\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, history in histories.items():\n",
    "    epochs = range(1, len(history.history['val_auc']) + 1)\n",
    "    color = colors[name]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(epochs), y=history.history['val_auc'],\n",
    "        name=name, line=dict(color=color, width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Validation AUC Over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Validation AUC',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Detecting Overfitting\n",
    "\n",
    "Signs of overfitting:\n",
    "1. Training loss continues to decrease\n",
    "2. Validation loss starts to increase\n",
    "3. Large gap between training and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting by comparing final train vs. validation metrics\n",
    "overfitting_analysis = []\n",
    "\n",
    "for name, history in histories.items():\n",
    "    hist = history.history\n",
    "    final_epoch = len(hist['loss'])\n",
    "    \n",
    "    overfitting_analysis.append({\n",
    "        'Model': name,\n",
    "        'Final Epoch': final_epoch,\n",
    "        'Train Loss': f\"{hist['loss'][-1]:.4f}\",\n",
    "        'Val Loss': f\"{hist['val_loss'][-1]:.4f}\",\n",
    "        'Loss Gap': f\"{hist['val_loss'][-1] - hist['loss'][-1]:.4f}\",\n",
    "        'Train AUC': f\"{hist['auc'][-1]:.4f}\",\n",
    "        'Val AUC': f\"{hist['val_auc'][-1]:.4f}\",\n",
    "        'AUC Gap': f\"{hist['auc'][-1] - hist['val_auc'][-1]:.4f}\"\n",
    "    })\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_analysis)\n",
    "print(\"Overfitting Analysis (Larger gaps may indicate overfitting):\")\n",
    "overfitting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train vs validation gap\n",
    "fig = go.Figure()\n",
    "\n",
    "model_names = list(histories.keys())\n",
    "\n",
    "for name, history in histories.items():\n",
    "    train_auc = history.history['auc'][-1]\n",
    "    val_auc = history.history['val_auc'][-1]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Training AUC',\n",
    "        x=[name],\n",
    "        y=[train_auc],\n",
    "        marker_color='lightblue',\n",
    "        showlegend=(name == model_names[0])\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Validation AUC',\n",
    "        x=[name],\n",
    "        y=[val_auc],\n",
    "        marker_color='darkblue',\n",
    "        showlegend=(name == model_names[0])\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training vs. Validation AUC (Check for Overfitting)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='AUC',\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0.5, 1.0]),\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Making Predictions\n",
    "\n",
    "Neural networks in Keras can generate two types of predictions:\n",
    "- `model.predict(X)`: Probability scores (continuous 0-1)\n",
    "- Convert to classes by thresholding (typically 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on validation set\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "models = {\n",
    "    'Simple NN': model_simple,\n",
    "    'Deep NN': model_deep,\n",
    "    'Wide NN': model_wide\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Get probability predictions\n",
    "    probs = model.predict(X_val, verbose=0)\n",
    "    probabilities[name] = probs.flatten()\n",
    "    \n",
    "    # Convert to class predictions (threshold = 0.5)\n",
    "    predictions[name] = (probs > 0.5).astype(int).flatten()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Predicted 0 (Retained): {(predictions[name] == 0).sum()}\")\n",
    "    print(f\"  Predicted 1 (Departed): {(predictions[name] == 1).sum()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Prediction Probabilities\n",
    "\n",
    "Let's examine the distribution of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probability distributions\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=list(models.keys()))\n",
    "\n",
    "for col, (name, probs) in enumerate(probabilities.items(), 1):\n",
    "    # Actual 0 (Retained)\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=probs[y_val == 0],\n",
    "        name='Actual: Retained',\n",
    "        opacity=0.7,\n",
    "        marker_color='blue',\n",
    "        showlegend=(col == 1)\n",
    "    ), row=1, col=col)\n",
    "    \n",
    "    # Actual 1 (Departed)\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=probs[y_val == 1],\n",
    "        name='Actual: Departed',\n",
    "        opacity=0.7,\n",
    "        marker_color='red',\n",
    "        showlegend=(col == 1)\n",
    "    ), row=1, col=col)\n",
    "    \n",
    "    # Add threshold line\n",
    "    fig.add_vline(x=0.5, line_dash='dash', line_color='green', row=1, col=col)\n",
    "\n",
    "fig.update_xaxes(title='Predicted Probability')\n",
    "fig.update_yaxes(title='Count')\n",
    "fig.update_layout(\n",
    "    title='Predicted Probability Distribution by Actual Class',\n",
    "    barmode='overlay',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "print(\"Sample Predictions (Simple NN):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_indices = np.random.choice(len(X_val), 10, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    actual = \"Departed\" if y_val[idx] == 1 else \"Retained\"\n",
    "    predicted = \"Departed\" if predictions['Simple NN'][idx] == 1 else \"Retained\"\n",
    "    prob = probabilities['Simple NN'][idx]\n",
    "    correct = \"YES\" if y_val[idx] == predictions['Simple NN'][idx] else \"NO\"\n",
    "    \n",
    "    print(f\"  Sample {idx}: Actual={actual}, Predicted={predicted} (prob={prob:.3f}) - Correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "training_times = {\n",
    "    'Simple NN': training_time_simple,\n",
    "    'Deep NN': training_time_deep,\n",
    "    'Wide NN': training_time_wide\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = predictions[name]\n",
    "    y_prob = probabilities[name]\n",
    "    history = histories[name]\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'Parameters': models[name].count_params(),\n",
    "        'Epochs': len(history.history['loss']),\n",
    "        'Training Time (s)': f\"{training_times[name]:.2f}\",\n",
    "        'Val Accuracy': f\"{accuracy_score(y_val, y_pred):.4f}\",\n",
    "        'Val Precision': f\"{precision_score(y_val, y_pred):.4f}\",\n",
    "        'Val Recall': f\"{recall_score(y_val, y_pred):.4f}\",\n",
    "        'Val F1': f\"{f1_score(y_val, y_pred):.4f}\",\n",
    "        'Val AUC': f\"{roc_auc_score(y_val, y_prob):.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Training Results Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "metrics = ['Val Accuracy', 'Val Precision', 'Val Recall', 'Val F1', 'Val AUC']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors_list = list(colors.values())\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [float(comparison_df[comparison_df['Model'] == name][metric].values[0]) \n",
    "              for name in models.keys()]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric.replace('Val ', ''),\n",
    "        x=list(models.keys()),\n",
    "        y=values\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Neural Network Performance Comparison (Validation Set)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training efficiency: Parameters vs. Performance\n",
    "fig = go.Figure()\n",
    "\n",
    "params = [models[name].count_params() for name in models.keys()]\n",
    "aucs = [float(comparison_df[comparison_df['Model'] == name]['Val AUC'].values[0]) \n",
    "        for name in models.keys()]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=params,\n",
    "    y=aucs,\n",
    "    mode='markers+text',\n",
    "    text=list(models.keys()),\n",
    "    textposition='top center',\n",
    "    marker=dict(size=20, color=colors_list)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Efficiency: Parameters vs. Performance',\n",
    "    xaxis_title='Number of Parameters',\n",
    "    yaxis_title='Validation AUC',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "for name, model in models.items():\n",
    "    filename = name.lower().replace(' ', '_')\n",
    "    model.save(f'{models_path}{filename}_trained.keras')\n",
    "    print(f\"Saved: {models_path}{filename}_trained.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training histories for later analysis\n",
    "histories_data = {name: history.history for name, history in histories.items()}\n",
    "pickle.dump(histories_data, open(f'{models_path}training_histories.pkl', 'wb'))\n",
    "print(f\"Saved training histories to: {models_path}training_histories.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv(f'{models_path}training_comparison.csv', index=False)\n",
    "print(f\"Saved comparison results to: {models_path}training_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "import os\n",
    "print(\"Saved files:\")\n",
    "print(\"=\"*60)\n",
    "for file in sorted(os.listdir(models_path)):\n",
    "    filepath = f'{models_path}{file}'\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f\"  {file}: {size/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we trained three neural network models for student departure prediction.\n",
    "\n",
    "### Training Concepts\n",
    "\n",
    "| Concept | Description | Our Settings |\n",
    "|:--------|:------------|:-------------|\n",
    "| **Epochs** | Complete passes through data | Max 100 (early stopped) |\n",
    "| **Batch Size** | Samples per weight update | 32 |\n",
    "| **Early Stopping** | Stops when no improvement | Patience: 15 epochs |\n",
    "| **Learning Rate** | Step size for updates | 0.001 (with reduction) |\n",
    "\n",
    "### Callbacks Used\n",
    "\n",
    "| Callback | Purpose | Settings |\n",
    "|:---------|:--------|:---------|\n",
    "| **EarlyStopping** | Prevent overfitting | patience=15, restore_best_weights=True |\n",
    "| **ModelCheckpoint** | Save best model | save_best_only=True |\n",
    "| **ReduceLROnPlateau** | Adaptive learning rate | factor=0.5, patience=5 |\n",
    "\n",
    "### Training Results\n",
    "\n",
    "| Model | Parameters | Epochs | Val AUC |\n",
    "|:------|:-----------|:-------|:--------|\n",
    "| Simple NN | ~100 | Variable | Check results above |\n",
    "| Deep NN | ~300 | Variable | Check results above |\n",
    "| Wide NN | ~700 | Variable | Check results above |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Loss curves** help diagnose training issues (overfitting, underfitting)\n",
    "2. **Early stopping** prevents wasting time and overfitting\n",
    "3. **Class weights** help with imbalanced data\n",
    "4. **More parameters doesn't always mean better performance**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "- Evaluate models on the test set\n",
    "- Compare with tree-based models (Random Forest)\n",
    "- Tune hyperparameters (architecture, regularization)\n",
    "- Apply dropout regularization\n",
    "\n",
    "**Proceed to:** `5.4 Evaluate and Tune Neural Networks`"
   ]
  }
 ]
}