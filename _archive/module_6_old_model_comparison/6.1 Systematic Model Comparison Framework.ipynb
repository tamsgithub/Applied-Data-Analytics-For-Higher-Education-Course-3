{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-001",
   "metadata": {},
   "source": [
    "# 6.1 Systematic Model Comparison Framework\n",
    "\n",
    "## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-003",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Throughout Course 3, we have explored five distinct families of machine learning models for predicting student departure:\n",
    "\n",
    "1. **Module 1**: Regularized Logistic Regression (L1, L2, ElasticNet)\n",
    "2. **Module 2**: Decision Trees\n",
    "3. **Module 3**: Random Forests\n",
    "4. **Module 4**: Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "5. **Module 5**: Neural Networks (MLPClassifier)\n",
    "\n",
    "Each model family has unique strengths and weaknesses. In this module, we bring all models together for a systematic, head-to-head comparison. This framework will help you make informed decisions about which model to deploy in your institution's student success initiatives.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Train and evaluate all five model families on the same dataset\n",
    "2. Create comprehensive comparison tables across multiple metrics\n",
    "3. Visualize model performance using radar charts and comparison plots\n",
    "4. Understand the trade-offs between accuracy, interpretability, and training time\n",
    "5. Apply model selection criteria appropriate for higher education contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-004",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-005",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss, log_loss\n",
    ")\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-007",
   "metadata": {},
   "source": [
    "### 1.2 Load Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and testing datasets\n",
    "train_df = pd.read_csv('../../data/training.csv')\n",
    "test_df = pd.read_csv('../../data/testing.csv')\n",
    "\n",
    "print(f\"Training set: {train_df.shape[0]:,} students, {train_df.shape[1]} features\")\n",
    "print(f\"Testing set: {test_df.shape[0]:,} students, {test_df.shape[1]} features\")\n",
    "print(f\"\\nTarget variable distribution (Training):\")\n",
    "print(train_df['SEM_3_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target variable: 1 = Departed, 0 = Enrolled\n",
    "train_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "test_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "\n",
    "print(f\"Departure rate (Training): {train_df['DEPARTED'].mean():.2%}\")\n",
    "print(f\"Departure rate (Testing): {test_df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-010",
   "metadata": {},
   "source": [
    "### 1.3 Define Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories\n",
    "numeric_features = [\n",
    "    'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
    "    'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2',\n",
    "    'DFW_UNITS_1', 'DFW_UNITS_2',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'GRADE_POINTS_1', 'GRADE_POINTS_2'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'COLLEGE'\n",
    "]\n",
    "\n",
    "target = 'DEPARTED'\n",
    "\n",
    "print(f\"Number of numeric features: {len(numeric_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Total features: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# One-hot encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df[numeric_features + categorical_features], \n",
    "                               columns=categorical_features, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df[numeric_features + categorical_features], \n",
    "                              columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align columns between train and test\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Handle any missing values\n",
    "train_encoded = train_encoded.fillna(train_encoded.median())\n",
    "test_encoded = test_encoded.fillna(test_encoded.median())\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_encoded\n",
    "y_train = train_df[target]\n",
    "X_test = test_encoded\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nFeature columns: {X_train.columns.tolist()[:10]}... (showing first 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for models that require it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Scaled X_train mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Scaled X_train std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-014",
   "metadata": {},
   "source": [
    "## 2. Building All Models\n",
    "\n",
    "We will now train each model family with well-tuned hyperparameters. For this comparison, we use configurations that represent each model's typical best performance based on our exploration in previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all models and their results\n",
    "models = {}\n",
    "results = {}\n",
    "training_times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-016",
   "metadata": {},
   "source": [
    "### 2.1 Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularized Logistic Regression (Ridge)\n",
    "print(\"Training L2 Regularized Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_l2 = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.1,  # Inverse of regularization strength\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "lr_l2.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Logistic Regression (L2)'] = time.time() - start_time\n",
    "models['Logistic Regression (L2)'] = lr_l2\n",
    "\n",
    "print(f\"Training completed in {training_times['Logistic Regression (L2)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularized Logistic Regression (Lasso)\n",
    "print(\"Training L1 Regularized Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_l1 = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=0.1,\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "lr_l1.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Logistic Regression (L1)'] = time.time() - start_time\n",
    "models['Logistic Regression (L1)'] = lr_l1\n",
    "\n",
    "print(f\"Training completed in {training_times['Logistic Regression (L1)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Logistic Regression\n",
    "print(\"Training ElasticNet Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_elastic = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    C=0.1,\n",
    "    solver='saga',\n",
    "    l1_ratio=0.5,  # Balance between L1 and L2\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "lr_elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Logistic Regression (ElasticNet)'] = time.time() - start_time\n",
    "models['Logistic Regression (ElasticNet)'] = lr_elastic\n",
    "\n",
    "print(f\"Training completed in {training_times['Logistic Regression (ElasticNet)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-020",
   "metadata": {},
   "source": [
    "### 2.2 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with optimized hyperparameters\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "dt.fit(X_train, y_train)  # No scaling needed for tree-based models\n",
    "\n",
    "training_times['Decision Tree'] = time.time() - start_time\n",
    "models['Decision Tree'] = dt\n",
    "\n",
    "print(f\"Training completed in {training_times['Decision Tree']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-022",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with optimized hyperparameters\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "training_times['Random Forest'] = time.time() - start_time\n",
    "models['Random Forest'] = rf\n",
    "\n",
    "print(f\"Training completed in {training_times['Random Forest']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-024",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Gradient Boosting\n",
    "print(\"Training Gradient Boosting Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "training_times['Gradient Boosting'] = time.time() - start_time\n",
    "models['Gradient Boosting'] = gb\n",
    "\n",
    "print(f\"Training completed in {training_times['Gradient Boosting']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import XGBoost (optional)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    print(\"Training XGBoost Classifier...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    training_times['XGBoost'] = time.time() - start_time\n",
    "    models['XGBoost'] = xgb\n",
    "    \n",
    "    print(f\"Training completed in {training_times['XGBoost']:.2f} seconds\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import LightGBM (optional)\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    \n",
    "    print(\"Training LightGBM Classifier...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    training_times['LightGBM'] = time.time() - start_time\n",
    "    models['LightGBM'] = lgbm\n",
    "    \n",
    "    print(f\"Training completed in {training_times['LightGBM']:.2f} seconds\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"LightGBM not installed. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-028",
   "metadata": {},
   "source": [
    "### 2.5 Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (MLP)\n",
    "print(\"Training Neural Network (MLP) Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32, 16),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,  # L2 regularization\n",
    "    batch_size=32,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Neural Network'] = time.time() - start_time\n",
    "models['Neural Network'] = nn\n",
    "\n",
    "print(f\"Training completed in {training_times['Neural Network']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all trained models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<35} {'Training Time (s)':<20}\")\n",
    "print(\"-\"*60)\n",
    "for model_name, train_time in sorted(training_times.items(), key=lambda x: x[1]):\n",
    "    print(f\"{model_name:<35} {train_time:>15.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total models trained: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-031",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Model Evaluation\n",
    "\n",
    "Now we evaluate all models on the test set using multiple metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-032",
   "metadata": {},
   "source": [
    "### 3.1 Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, requires_scaling=False, X_test_scaled=None):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation returning multiple metrics.\n",
    "    \"\"\"\n",
    "    # Select appropriate test set\n",
    "    if requires_scaling and X_test_scaled is not None:\n",
    "        X_eval = X_test_scaled\n",
    "    else:\n",
    "        X_eval = X_test\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob),\n",
    "        'Avg Precision': average_precision_score(y_test, y_prob),\n",
    "        'Brier Score': brier_score_loss(y_test, y_prob),\n",
    "        'Log Loss': log_loss(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that require scaling\n",
    "scaling_required = {\n",
    "    'Logistic Regression (L2)': True,\n",
    "    'Logistic Regression (L1)': True,\n",
    "    'Logistic Regression (ElasticNet)': True,\n",
    "    'Decision Tree': False,\n",
    "    'Random Forest': False,\n",
    "    'Gradient Boosting': False,\n",
    "    'XGBoost': False,\n",
    "    'LightGBM': False,\n",
    "    'Neural Network': True\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "all_results = []\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    requires_scaling = scaling_required.get(model_name, False)\n",
    "    metrics, y_pred, y_prob = evaluate_model(\n",
    "        model, X_test, y_test, model_name, \n",
    "        requires_scaling=requires_scaling, \n",
    "        X_test_scaled=X_test_scaled\n",
    "    )\n",
    "    all_results.append(metrics)\n",
    "    predictions[model_name] = y_pred\n",
    "    probabilities[model_name] = y_prob\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "# Add training time\n",
    "results_df['Training Time (s)'] = results_df.index.map(training_times)\n",
    "\n",
    "print(\"Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results table\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON - PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "display_cols = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'Avg Precision']\n",
    "print(results_df[display_cols].round(4).to_string())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison bar chart\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=results_df.index,\n",
    "        y=results_df[metric],\n",
    "        marker_color=colors[i % len(colors)]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison Across Multiple Metrics',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-037",
   "metadata": {},
   "source": [
    "### 3.2 Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training time comparison chart\n",
    "time_df = pd.DataFrame({\n",
    "    'Model': list(training_times.keys()),\n",
    "    'Training Time (s)': list(training_times.values())\n",
    "}).sort_values('Training Time (s)', ascending=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=time_df['Training Time (s)'],\n",
    "    y=time_df['Model'],\n",
    "    orientation='h',\n",
    "    marker_color='steelblue',\n",
    "    text=time_df['Training Time (s)'].round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Training Time Comparison',\n",
    "    xaxis_title='Training Time (seconds)',\n",
    "    yaxis_title='Model',\n",
    "    height=450,\n",
    "    margin=dict(l=200)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-039",
   "metadata": {},
   "source": [
    "### 3.3 ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, (model_name, y_prob) in enumerate(probabilities.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (AUC={auc:.3f})',\n",
    "        line=dict(color=colors[i % len(colors)], width=2)\n",
    "    ))\n",
    "\n",
    "# Add diagonal reference line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random Classifier',\n",
    "    line=dict(color='gray', dash='dash', width=1)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curve Comparison - All Models',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=600,\n",
    "    legend=dict(x=0.6, y=0.1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    yaxis=dict(scaleanchor='x', scaleratio=1)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-041",
   "metadata": {},
   "source": [
    "### 3.4 Precision-Recall Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves for all models\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, (model_name, y_prob) in enumerate(probabilities.items()):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    ap = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=recall, y=precision,\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (AP={ap:.3f})',\n",
    "        line=dict(color=colors[i % len(colors)], width=2)\n",
    "    ))\n",
    "\n",
    "# Add baseline (prevalence)\n",
    "prevalence = y_test.mean()\n",
    "fig.add_hline(y=prevalence, line_dash='dash', line_color='gray',\n",
    "              annotation_text=f'Baseline (prevalence={prevalence:.2%})')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Precision-Recall Curve Comparison - All Models',\n",
    "    xaxis_title='Recall (True Positive Rate)',\n",
    "    yaxis_title='Precision (Positive Predictive Value)',\n",
    "    height=600,\n",
    "    legend=dict(x=0.02, y=0.02),\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-043",
   "metadata": {},
   "source": [
    "## 4. Multi-Dimensional Model Comparison\n",
    "\n",
    "Beyond raw performance metrics, models differ in interpretability, computational requirements, and suitability for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-044",
   "metadata": {},
   "source": [
    "### 4.1 Radar Chart: Model Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interpretability and other qualitative scores (1-10 scale)\n",
    "# These are based on general ML best practices\n",
    "qualitative_scores = {\n",
    "    'Logistic Regression (L2)': {\n",
    "        'Interpretability': 9,\n",
    "        'Training Speed': 10,\n",
    "        'Prediction Speed': 10,\n",
    "        'Handles Non-linearity': 3,\n",
    "        'Feature Interactions': 2,\n",
    "        'Robustness to Outliers': 5\n",
    "    },\n",
    "    'Logistic Regression (L1)': {\n",
    "        'Interpretability': 9,\n",
    "        'Training Speed': 9,\n",
    "        'Prediction Speed': 10,\n",
    "        'Handles Non-linearity': 3,\n",
    "        'Feature Interactions': 2,\n",
    "        'Robustness to Outliers': 5\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'Interpretability': 10,\n",
    "        'Training Speed': 9,\n",
    "        'Prediction Speed': 9,\n",
    "        'Handles Non-linearity': 8,\n",
    "        'Feature Interactions': 8,\n",
    "        'Robustness to Outliers': 7\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Interpretability': 5,\n",
    "        'Training Speed': 6,\n",
    "        'Prediction Speed': 7,\n",
    "        'Handles Non-linearity': 9,\n",
    "        'Feature Interactions': 9,\n",
    "        'Robustness to Outliers': 8\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'Interpretability': 4,\n",
    "        'Training Speed': 5,\n",
    "        'Prediction Speed': 7,\n",
    "        'Handles Non-linearity': 9,\n",
    "        'Feature Interactions': 9,\n",
    "        'Robustness to Outliers': 7\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'Interpretability': 4,\n",
    "        'Training Speed': 7,\n",
    "        'Prediction Speed': 8,\n",
    "        'Handles Non-linearity': 10,\n",
    "        'Feature Interactions': 10,\n",
    "        'Robustness to Outliers': 7\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'Interpretability': 4,\n",
    "        'Training Speed': 8,\n",
    "        'Prediction Speed': 9,\n",
    "        'Handles Non-linearity': 10,\n",
    "        'Feature Interactions': 10,\n",
    "        'Robustness to Outliers': 7\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'Interpretability': 2,\n",
    "        'Training Speed': 4,\n",
    "        'Prediction Speed': 8,\n",
    "        'Handles Non-linearity': 10,\n",
    "        'Feature Interactions': 10,\n",
    "        'Robustness to Outliers': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Only include models that we actually trained\n",
    "qualitative_scores = {k: v for k, v in qualitative_scores.items() if k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for model comparison\n",
    "categories = ['Interpretability', 'Training Speed', 'Prediction Speed', \n",
    "              'Handles Non-linearity', 'Feature Interactions', 'Robustness to Outliers']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# Select key models for radar chart (to avoid clutter)\n",
    "key_models = ['Logistic Regression (L2)', 'Decision Tree', 'Random Forest', \n",
    "              'Gradient Boosting', 'Neural Network']\n",
    "\n",
    "for i, model_name in enumerate(key_models):\n",
    "    if model_name in qualitative_scores:\n",
    "        values = [qualitative_scores[model_name][cat] for cat in categories]\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=categories + [categories[0]],\n",
    "            name=model_name,\n",
    "            line=dict(color=colors[i % len(colors)], width=2),\n",
    "            fill='toself',\n",
    "            fillcolor=colors[i % len(colors)],\n",
    "            opacity=0.3\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 10]\n",
    "        )\n",
    "    ),\n",
    "    title='Radar Chart: Model Capabilities Comparison',\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-047",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- **Logistic Regression**: Excels in interpretability and speed, but limited in capturing complex patterns\n",
    "- **Decision Tree**: Highly interpretable and handles non-linearity well\n",
    "- **Random Forest**: Strong overall performance but reduced interpretability\n",
    "- **Gradient Boosting**: Top predictive power, but slower and less interpretable\n",
    "- **Neural Network**: Flexible but acts as a \"black box\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-048",
   "metadata": {},
   "source": [
    "### 4.2 Interpretability vs Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpretability vs AUC scatter plot\n",
    "plot_data = []\n",
    "for model_name in models.keys():\n",
    "    if model_name in qualitative_scores:\n",
    "        plot_data.append({\n",
    "            'Model': model_name,\n",
    "            'Interpretability': qualitative_scores[model_name]['Interpretability'],\n",
    "            'ROC-AUC': results_df.loc[model_name, 'ROC-AUC'],\n",
    "            'Training Time': training_times[model_name]\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x='Interpretability',\n",
    "    y='ROC-AUC',\n",
    "    size='Training Time',\n",
    "    color='Model',\n",
    "    text='Model',\n",
    "    title='Interpretability vs. Predictive Performance Trade-off',\n",
    "    labels={\n",
    "        'Interpretability': 'Interpretability Score (1-10)',\n",
    "        'ROC-AUC': 'ROC-AUC Score',\n",
    "        'Training Time': 'Training Time (s)'\n",
    "    },\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis=dict(range=[0, 11]),\n",
    "    yaxis=dict(range=[0.5, 1.0])\n",
    ")\n",
    "\n",
    "# Add quadrant labels\n",
    "fig.add_annotation(x=2, y=0.95, text=\"High Performance,<br>Low Interpretability\",\n",
    "                   showarrow=False, font=dict(size=10, color='gray'))\n",
    "fig.add_annotation(x=8, y=0.95, text=\"High Performance,<br>High Interpretability\",\n",
    "                   showarrow=False, font=dict(size=10, color='gray'))\n",
    "fig.add_annotation(x=2, y=0.55, text=\"Low Performance,<br>Low Interpretability\",\n",
    "                   showarrow=False, font=dict(size=10, color='gray'))\n",
    "fig.add_annotation(x=8, y=0.55, text=\"Low Performance,<br>High Interpretability\",\n",
    "                   showarrow=False, font=dict(size=10, color='gray'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-050",
   "metadata": {},
   "source": [
    "**Key Insight**: The ideal model would be in the upper-right quadrant (high performance AND high interpretability). In practice, there is often a trade-off. The bubble size represents training time - larger bubbles indicate longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-051",
   "metadata": {},
   "source": [
    "### 4.3 Comprehensive Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'ROC-AUC': results_df.loc[model_name, 'ROC-AUC'],\n",
    "        'F1 Score': results_df.loc[model_name, 'F1 Score'],\n",
    "        'Recall': results_df.loc[model_name, 'Recall'],\n",
    "        'Training Time (s)': training_times[model_name],\n",
    "        'Interpretability': qualitative_scores.get(model_name, {}).get('Interpretability', 'N/A'),\n",
    "        'Scaling Required': 'Yes' if scaling_required.get(model_name, False) else 'No'\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of performance metrics\n",
    "heatmap_metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'Avg Precision']\n",
    "heatmap_data = results_df[heatmap_metrics]\n",
    "\n",
    "# Normalize each column to 0-1 for better visualization\n",
    "heatmap_normalized = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data.values,\n",
    "    x=heatmap_metrics,\n",
    "    y=heatmap_data.index,\n",
    "    colorscale='RdYlGn',\n",
    "    text=np.round(heatmap_data.values, 3),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 10},\n",
    "    hovertemplate='Model: %{y}<br>Metric: %{x}<br>Value: %{z:.4f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Heatmap',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Model',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-054",
   "metadata": {},
   "source": [
    "## 5. Higher Education Context Analysis\n",
    "\n",
    "Model selection in higher education involves more than just performance metrics. We must consider institutional constraints, stakeholder needs, and ethical implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-055",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection Criteria for Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define higher education specific criteria\n",
    "he_criteria = {\n",
    "    'Explainability to Advisors': {\n",
    "        'Description': 'Can academic advisors understand and explain predictions to students?',\n",
    "        'Logistic Regression (L2)': 9,\n",
    "        'Decision Tree': 10,\n",
    "        'Random Forest': 5,\n",
    "        'Gradient Boosting': 3,\n",
    "        'Neural Network': 1\n",
    "    },\n",
    "    'Actionable Insights': {\n",
    "        'Description': 'Does the model reveal what factors advisors can influence?',\n",
    "        'Logistic Regression (L2)': 8,\n",
    "        'Decision Tree': 9,\n",
    "        'Random Forest': 6,\n",
    "        'Gradient Boosting': 5,\n",
    "        'Neural Network': 2\n",
    "    },\n",
    "    'Regulatory Compliance': {\n",
    "        'Description': 'Can decisions be audited and explained for compliance?',\n",
    "        'Logistic Regression (L2)': 10,\n",
    "        'Decision Tree': 9,\n",
    "        'Random Forest': 5,\n",
    "        'Gradient Boosting': 4,\n",
    "        'Neural Network': 2\n",
    "    },\n",
    "    'Integration Ease': {\n",
    "        'Description': 'How easily can the model be integrated into existing systems?',\n",
    "        'Logistic Regression (L2)': 10,\n",
    "        'Decision Tree': 9,\n",
    "        'Random Forest': 7,\n",
    "        'Gradient Boosting': 6,\n",
    "        'Neural Network': 5\n",
    "    },\n",
    "    'Maintenance Burden': {\n",
    "        'Description': 'How much effort is needed to maintain and update the model?',\n",
    "        'Logistic Regression (L2)': 9,\n",
    "        'Decision Tree': 8,\n",
    "        'Random Forest': 6,\n",
    "        'Gradient Boosting': 5,\n",
    "        'Neural Network': 4\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display criteria\n",
    "print(\"Higher Education Model Selection Criteria:\")\n",
    "print(\"=\"*80)\n",
    "for criterion, data in he_criteria.items():\n",
    "    print(f\"\\n{criterion}:\")\n",
    "    print(f\"  {data['Description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Higher Education suitability radar chart\n",
    "he_models = ['Logistic Regression (L2)', 'Decision Tree', 'Random Forest', \n",
    "             'Gradient Boosting', 'Neural Network']\n",
    "he_categories = list(he_criteria.keys())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for i, model_name in enumerate(he_models):\n",
    "    if model_name in models:\n",
    "        values = [he_criteria[cat].get(model_name, 5) for cat in he_categories]\n",
    "        values.append(values[0])  # Close polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=he_categories + [he_categories[0]],\n",
    "            name=model_name,\n",
    "            line=dict(color=colors[i % len(colors)], width=2)\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 10]\n",
    "        )\n",
    "    ),\n",
    "    title='Higher Education Suitability: Model Comparison',\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-058",
   "metadata": {},
   "source": [
    "### 5.2 When to Use Which Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model recommendation framework\n",
    "recommendations = {\n",
    "    'Scenario': [\n",
    "        'Early Warning System for Advisors',\n",
    "        'Institutional Research Reports',\n",
    "        'Grant-funded Research Project',\n",
    "        'Real-time Dashboard Integration',\n",
    "        'Small Institution (<5,000 students)',\n",
    "        'Large Institution (>30,000 students)',\n",
    "        'Maximum Recall for At-Risk Students',\n",
    "        'Balanced Precision and Recall',\n",
    "        'Limited IT Resources',\n",
    "        'Advanced Data Science Team'\n",
    "    ],\n",
    "    'Recommended Model': [\n",
    "        'Decision Tree or Logistic Regression',\n",
    "        'Logistic Regression (high interpretability)',\n",
    "        'Gradient Boosting / XGBoost (max performance)',\n",
    "        'Logistic Regression (fast predictions)',\n",
    "        'Logistic Regression or Decision Tree',\n",
    "        'Random Forest or Gradient Boosting',\n",
    "        'Tune threshold on any high-recall model',\n",
    "        'Random Forest or Gradient Boosting',\n",
    "        'Logistic Regression (minimal maintenance)',\n",
    "        'XGBoost / LightGBM / Neural Network'\n",
    "    ],\n",
    "    'Rationale': [\n",
    "        'Advisors need to understand and explain predictions to students',\n",
    "        'Coefficients directly show factor importance for reports',\n",
    "        'Research publications value performance over interpretability',\n",
    "        'Logistic regression has fastest inference time',\n",
    "        'Simpler models less prone to overfitting on smaller datasets',\n",
    "        'Ensemble methods scale well and capture complex patterns',\n",
    "        'Adjust classification threshold to prioritize recall',\n",
    "        'Ensemble methods typically achieve best F1 scores',\n",
    "        'Simpler models require less tuning and monitoring',\n",
    "        'Complex models require expertise to tune and maintain'\n",
    "    ]\n",
    "}\n",
    "\n",
    "rec_df = pd.DataFrame(recommendations)\n",
    "\n",
    "print(\"MODEL RECOMMENDATION GUIDE FOR HIGHER EDUCATION\")\n",
    "print(\"=\"*120)\n",
    "for i, row in rec_df.iterrows():\n",
    "    print(f\"\\nScenario: {row['Scenario']}\")\n",
    "    print(f\"  Recommended: {row['Recommended Model']}\")\n",
    "    print(f\"  Rationale: {row['Rationale']}\")\n",
    "print(\"\\n\" + \"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision flowchart data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SELECTION DECISION FLOWCHART\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "START: What is your primary constraint?\n",
    "|\n",
    "+-- INTERPRETABILITY is critical?\n",
    "|   |\n",
    "|   +-- Yes --> Need feature selection?\n",
    "|   |           |\n",
    "|   |           +-- Yes --> Logistic Regression (L1)\n",
    "|   |           +-- No  --> Decision Tree or Logistic Regression (L2)\n",
    "|   |\n",
    "|   +-- No --> Continue below\n",
    "|\n",
    "+-- MAXIMUM PERFORMANCE is the goal?\n",
    "|   |\n",
    "|   +-- Yes --> Have large dataset (>10K samples)?\n",
    "|   |           |\n",
    "|   |           +-- Yes --> XGBoost / LightGBM\n",
    "|   |           +-- No  --> Random Forest\n",
    "|   |\n",
    "|   +-- No --> Continue below\n",
    "|\n",
    "+-- TRAINING TIME is a constraint?\n",
    "|   |\n",
    "|   +-- Yes --> Logistic Regression or Decision Tree\n",
    "|   +-- No  --> Random Forest or Gradient Boosting\n",
    "|\n",
    "+-- Default Recommendation:\n",
    "    --> Random Forest (good balance of all factors)\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-061",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"=\"*100)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Rank models by AUC\n",
    "ranked_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\nModels Ranked by ROC-AUC:\")\n",
    "print(\"-\"*60)\n",
    "for i, (model_name, row) in enumerate(ranked_df.iterrows(), 1):\n",
    "    print(f\"{i}. {model_name}: AUC = {row['ROC-AUC']:.4f}, F1 = {row['F1 Score']:.4f}\")\n",
    "\n",
    "# Identify best model by different criteria\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"BEST MODEL BY CRITERION:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Best ROC-AUC: {results_df['ROC-AUC'].idxmax()} ({results_df['ROC-AUC'].max():.4f})\")\n",
    "print(f\"Best F1 Score: {results_df['F1 Score'].idxmax()} ({results_df['F1 Score'].max():.4f})\")\n",
    "print(f\"Best Recall: {results_df['Recall'].idxmax()} ({results_df['Recall'].max():.4f})\")\n",
    "print(f\"Best Precision: {results_df['Precision'].idxmax()} ({results_df['Precision'].max():.4f})\")\n",
    "print(f\"Fastest Training: {min(training_times, key=training_times.get)} ({min(training_times.values()):.3f}s)\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-063",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "| Model Family | Strengths | Weaknesses | Best For |\n",
    "|:-------------|:----------|:-----------|:---------|\n",
    "| **Logistic Regression** | Highly interpretable, fast, coefficients show importance | Limited non-linearity | Reports, compliance, small data |\n",
    "| **Decision Tree** | Very interpretable, visual, handles non-linearity | Prone to overfitting | Advisor tools, simple rules |\n",
    "| **Random Forest** | Good performance, robust, feature importance | Less interpretable | Balanced needs, medium-large data |\n",
    "| **Gradient Boosting** | Top performance, handles complex patterns | Slow training, black-box | Research, maximum performance |\n",
    "| **Neural Network** | Flexible, powerful with lots of data | Black-box, needs tuning | Large data, complex patterns |\n",
    "\n",
    "### Recommendations for Higher Education\n",
    "\n",
    "1. **For most institutions**: Start with **Logistic Regression** or **Random Forest** - they offer the best balance of performance and interpretability\n",
    "\n",
    "2. **For advisor-facing tools**: Use **Decision Trees** or **Logistic Regression** where explanations are critical\n",
    "\n",
    "3. **For institutional research**: **Logistic Regression** provides clear coefficient interpretations for reports\n",
    "\n",
    "4. **For maximum performance**: **Gradient Boosting** (XGBoost/LightGBM) typically achieves the best metrics\n",
    "\n",
    "5. **Consider ensemble approaches**: Combine predictions from multiple models for robust results\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will select the best model for deployment, prepare it for production use, and discuss deployment considerations for higher education contexts.\n",
    "\n",
    "**Proceed to:** `6.2 Final Model Selection and Deployment`"
   ]
  }
 ]
}