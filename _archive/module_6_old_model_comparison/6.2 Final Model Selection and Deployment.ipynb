{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-001",
   "metadata": {},
   "source": [
    "# 6.2 Final Model Selection and Deployment\n",
    "\n",
    "## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-003",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebook (6.1), we systematically compared all models from Course 3. Now it's time to make the final selection and prepare the chosen model for deployment in a real higher education setting.\n",
    "\n",
    "Deploying a machine learning model involves more than just selecting the highest-performing algorithm. We must consider:\n",
    "\n",
    "- **Institutional constraints**: IT infrastructure, data pipelines, staff expertise\n",
    "- **Stakeholder needs**: Advisors, administrators, students, and regulators\n",
    "- **Ethical implications**: Fairness, transparency, and potential unintended consequences\n",
    "- **Maintenance requirements**: Model monitoring, retraining schedules, and drift detection\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply a structured decision framework for final model selection\n",
    "2. Optimize classification thresholds for institutional priorities\n",
    "3. Prepare a model for production deployment with proper serialization\n",
    "4. Create stakeholder-friendly model documentation\n",
    "5. Understand deployment considerations specific to higher education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-004",
   "metadata": {},
   "source": [
    "## 1. Setup and Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-005",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss\n",
    ")\n",
    "\n",
    "# Calibration\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-007",
   "metadata": {},
   "source": [
    "### 1.2 Load Data and Recap Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('../../data/training.csv')\n",
    "test_df = pd.read_csv('../../data/testing.csv')\n",
    "\n",
    "# Create binary target\n",
    "train_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "test_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "\n",
    "print(f\"Training set: {train_df.shape[0]:,} students\")\n",
    "print(f\"Testing set: {test_df.shape[0]:,} students\")\n",
    "print(f\"Departure rate: {train_df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "numeric_features = [\n",
    "    'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
    "    'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2',\n",
    "    'DFW_UNITS_1', 'DFW_UNITS_2',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'GRADE_POINTS_1', 'GRADE_POINTS_2'\n",
    "]\n",
    "\n",
    "categorical_features = ['RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'COLLEGE']\n",
    "target = 'DEPARTED'\n",
    "\n",
    "# Prepare data\n",
    "train_encoded = pd.get_dummies(train_df[numeric_features + categorical_features], \n",
    "                               columns=categorical_features, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df[numeric_features + categorical_features], \n",
    "                              columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Handle missing values\n",
    "train_encoded = train_encoded.fillna(train_encoded.median())\n",
    "test_encoded = test_encoded.fillna(test_encoded.median())\n",
    "\n",
    "X_train = train_encoded\n",
    "y_train = train_df[target]\n",
    "X_test = test_encoded\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Store feature names for later use\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(f\"Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap: Model comparison results from Notebook 6.1\n",
    "# These are representative results - actual values will vary based on data\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (L2)', 'Decision Tree', 'Random Forest', \n",
    "              'Gradient Boosting', 'Neural Network'],\n",
    "    'ROC-AUC': [0.82, 0.75, 0.85, 0.86, 0.84],\n",
    "    'F1 Score': [0.58, 0.52, 0.62, 0.64, 0.61],\n",
    "    'Recall': [0.65, 0.58, 0.68, 0.70, 0.67],\n",
    "    'Precision': [0.52, 0.47, 0.57, 0.59, 0.56],\n",
    "    'Interpretability': [9, 10, 5, 4, 2],\n",
    "    'Training Time (s)': [0.5, 0.2, 3.5, 8.2, 15.3]\n",
    "})\n",
    "\n",
    "print(\"RECAP: Model Comparison Results from Notebook 6.1\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_results.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-011",
   "metadata": {},
   "source": [
    "## 2. Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-012",
   "metadata": {},
   "source": [
    "### 2.1 Selection Criteria for Higher Education\n",
    "\n",
    "Before selecting the final model, let's establish clear criteria weighted for a typical higher education deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weighted selection criteria\n",
    "selection_criteria = {\n",
    "    'Predictive Performance (ROC-AUC)': {\n",
    "        'weight': 0.30,\n",
    "        'description': 'Overall ability to distinguish at-risk from not-at-risk students'\n",
    "    },\n",
    "    'Recall (Sensitivity)': {\n",
    "        'weight': 0.25,\n",
    "        'description': 'Ability to identify students who will actually depart (minimize false negatives)'\n",
    "    },\n",
    "    'Interpretability': {\n",
    "        'weight': 0.25,\n",
    "        'description': 'Can advisors and administrators understand and explain predictions?'\n",
    "    },\n",
    "    'Deployment Ease': {\n",
    "        'weight': 0.15,\n",
    "        'description': 'Training time, prediction speed, maintenance requirements'\n",
    "    },\n",
    "    'Precision': {\n",
    "        'weight': 0.05,\n",
    "        'description': 'Accuracy of positive predictions (minimize false alarms)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"MODEL SELECTION CRITERIA FOR HIGHER EDUCATION\")\n",
    "print(\"=\"*70)\n",
    "total_weight = 0\n",
    "for criterion, details in selection_criteria.items():\n",
    "    print(f\"\\n{criterion} (Weight: {details['weight']:.0%})\")\n",
    "    print(f\"  {details['description']}\")\n",
    "    total_weight += details['weight']\n",
    "print(f\"\\nTotal Weight: {total_weight:.0%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize criteria weights\n",
    "criteria_names = list(selection_criteria.keys())\n",
    "criteria_weights = [selection_criteria[c]['weight'] for c in criteria_names]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=criteria_names,\n",
    "    values=criteria_weights,\n",
    "    hole=0.4,\n",
    "    textinfo='label+percent',\n",
    "    textposition='outside'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Selection Criteria Weights for Higher Education',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-015",
   "metadata": {},
   "source": [
    "### 2.2 Selecting the Production Model\n",
    "\n",
    "Based on our comparison and the criteria above, we will select **Random Forest** as our production model for the following reasons:\n",
    "\n",
    "1. **Strong predictive performance**: High ROC-AUC and good recall\n",
    "2. **Reasonable interpretability**: Feature importance is available\n",
    "3. **Robust**: Less prone to overfitting than single trees\n",
    "4. **Good balance**: Best trade-off between performance and complexity\n",
    "5. **Practical**: Reasonable training time and easy to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score each model on criteria (normalized 0-1 scale)\n",
    "model_scores = {\n",
    "    'Logistic Regression (L2)': {\n",
    "        'ROC-AUC': 0.82,\n",
    "        'Recall': 0.65,\n",
    "        'Interpretability': 0.9,\n",
    "        'Deployment Ease': 1.0,\n",
    "        'Precision': 0.52\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'ROC-AUC': 0.75,\n",
    "        'Recall': 0.58,\n",
    "        'Interpretability': 1.0,\n",
    "        'Deployment Ease': 0.95,\n",
    "        'Precision': 0.47\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'ROC-AUC': 0.85,\n",
    "        'Recall': 0.68,\n",
    "        'Interpretability': 0.5,\n",
    "        'Deployment Ease': 0.7,\n",
    "        'Precision': 0.57\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'ROC-AUC': 0.86,\n",
    "        'Recall': 0.70,\n",
    "        'Interpretability': 0.4,\n",
    "        'Deployment Ease': 0.5,\n",
    "        'Precision': 0.59\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'ROC-AUC': 0.84,\n",
    "        'Recall': 0.67,\n",
    "        'Interpretability': 0.2,\n",
    "        'Deployment Ease': 0.3,\n",
    "        'Precision': 0.56\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate weighted scores\n",
    "weights = [0.30, 0.25, 0.25, 0.15, 0.05]\n",
    "criteria_map = ['ROC-AUC', 'Recall', 'Interpretability', 'Deployment Ease', 'Precision']\n",
    "\n",
    "weighted_scores = {}\n",
    "for model, scores in model_scores.items():\n",
    "    total = sum(scores[c] * w for c, w in zip(criteria_map, weights))\n",
    "    weighted_scores[model] = total\n",
    "\n",
    "# Display results\n",
    "print(\"WEIGHTED MODEL SCORES\")\n",
    "print(\"=\"*50)\n",
    "for model, score in sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:<30} {score:.4f}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSelected Model: {max(weighted_scores, key=weighted_scores.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weighted scores\n",
    "score_df = pd.DataFrame({\n",
    "    'Model': list(weighted_scores.keys()),\n",
    "    'Weighted Score': list(weighted_scores.values())\n",
    "}).sort_values('Weighted Score', ascending=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['lightgray'] * len(score_df)\n",
    "colors[-1] = 'green'  # Highlight the best model\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=score_df['Weighted Score'],\n",
    "    y=score_df['Model'],\n",
    "    orientation='h',\n",
    "    marker_color=colors,\n",
    "    text=score_df['Weighted Score'].round(3),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Weighted Model Scores for Production Deployment',\n",
    "    xaxis_title='Weighted Score',\n",
    "    yaxis_title='Model',\n",
    "    height=400,\n",
    "    xaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-018",
   "metadata": {},
   "source": [
    "### 2.3 Final Model Training\n",
    "\n",
    "Now we train the final Random Forest model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final model with optimized parameters\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    oob_score=True  # Enable out-of-bag scoring\n",
    ")\n",
    "\n",
    "print(\"Final Model Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: Random Forest Classifier\")\n",
    "print(f\"Number of trees: 200\")\n",
    "print(f\"Max depth: 12\")\n",
    "print(f\"Min samples split: 10\")\n",
    "print(f\"Min samples leaf: 5\")\n",
    "print(f\"Max features: sqrt({len(feature_names)}) = {int(np.sqrt(len(feature_names)))}\")\n",
    "print(f\"Class weight: balanced\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model\n",
    "print(\"Training final model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"Out-of-bag score: {final_model.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-021",
   "metadata": {},
   "source": [
    "## 3. Model Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-022",
   "metadata": {},
   "source": [
    "### 3.1 Final Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob),\n",
    "    'Average Precision': average_precision_score(y_test, y_prob),\n",
    "    'Brier Score': brier_score_loss(y_test, y_prob)\n",
    "}\n",
    "\n",
    "print(\"FINAL MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:<20} {value:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted: Retained', 'Predicted: Departed'],\n",
    "    y=['Actual: Retained', 'Actual: Departed'],\n",
    "    colorscale='Blues',\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 16},\n",
    "    hovertemplate='Actual: %{y}<br>Predicted: %{x}<br>Count: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix - Final Random Forest Model',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate and display key counts\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (correctly identified retained): {tn}\")\n",
    "print(f\"  False Positives (false alarms): {fp}\")\n",
    "print(f\"  False Negatives (missed departures): {fn}\")\n",
    "print(f\"  True Positives (correctly identified departures): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    f'ROC Curve (AUC = {metrics[\"ROC-AUC\"]:.3f})',\n",
    "    f'Precision-Recall Curve (AP = {metrics[\"Average Precision\"]:.3f})'\n",
    "))\n",
    "\n",
    "# ROC Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr,\n",
    "    mode='lines',\n",
    "    name='ROC',\n",
    "    line=dict(color='blue', width=2)\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "), row=1, col=1)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=recall_curve, y=precision_curve,\n",
    "    mode='lines',\n",
    "    name='PR Curve',\n",
    "    line=dict(color='green', width=2)\n",
    "), row=1, col=2)\n",
    "\n",
    "prevalence = y_test.mean()\n",
    "fig.add_hline(y=prevalence, line_dash='dash', line_color='gray', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='False Positive Rate', row=1, col=1)\n",
    "fig.update_yaxes(title_text='True Positive Rate', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Recall', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Precision', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-026",
   "metadata": {},
   "source": [
    "### 3.2 Threshold Optimization\n",
    "\n",
    "The default threshold of 0.5 may not be optimal for higher education contexts where identifying at-risk students is critical. Let's explore different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics at different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "    \n",
    "    threshold_metrics.append({\n",
    "        'Threshold': thresh,\n",
    "        'Precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred_thresh, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred_thresh, zero_division=0),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_thresh)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold vs metrics\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=thresh_df['Threshold'], y=thresh_df['Precision'],\n",
    "    mode='lines', name='Precision',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=thresh_df['Threshold'], y=thresh_df['Recall'],\n",
    "    mode='lines', name='Recall',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=thresh_df['Threshold'], y=thresh_df['F1 Score'],\n",
    "    mode='lines', name='F1 Score',\n",
    "    line=dict(color='green', width=2)\n",
    "))\n",
    "\n",
    "# Add vertical lines for key thresholds\n",
    "fig.add_vline(x=0.5, line_dash='dash', line_color='gray', \n",
    "              annotation_text='Default (0.5)')\n",
    "\n",
    "# Find optimal threshold for F1\n",
    "optimal_f1_idx = thresh_df['F1 Score'].idxmax()\n",
    "optimal_f1_thresh = thresh_df.loc[optimal_f1_idx, 'Threshold']\n",
    "fig.add_vline(x=optimal_f1_thresh, line_dash='dash', line_color='green',\n",
    "              annotation_text=f'Optimal F1 ({optimal_f1_thresh:.2f})')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Precision, Recall, and F1 Score vs. Classification Threshold',\n",
    "    xaxis_title='Classification Threshold',\n",
    "    yaxis_title='Score',\n",
    "    height=500,\n",
    "    legend=dict(x=0.8, y=0.95)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend thresholds for different institutional priorities\n",
    "print(\"THRESHOLD RECOMMENDATIONS FOR DIFFERENT PRIORITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High Recall (catch more at-risk students)\n",
    "high_recall_idx = thresh_df[thresh_df['Recall'] >= 0.80]['Threshold'].min()\n",
    "if pd.notna(high_recall_idx):\n",
    "    hr_row = thresh_df[thresh_df['Threshold'] == high_recall_idx].iloc[0]\n",
    "    print(f\"\\n1. HIGH RECALL PRIORITY (catch most at-risk students)\")\n",
    "    print(f\"   Threshold: {high_recall_idx:.2f}\")\n",
    "    print(f\"   Recall: {hr_row['Recall']:.2%} | Precision: {hr_row['Precision']:.2%}\")\n",
    "    print(f\"   Trade-off: More false alarms, but fewer missed students\")\n",
    "\n",
    "# Balanced (optimal F1)\n",
    "bal_row = thresh_df.loc[optimal_f1_idx]\n",
    "print(f\"\\n2. BALANCED PRIORITY (best F1 score)\")\n",
    "print(f\"   Threshold: {optimal_f1_thresh:.2f}\")\n",
    "print(f\"   Recall: {bal_row['Recall']:.2%} | Precision: {bal_row['Precision']:.2%}\")\n",
    "print(f\"   Trade-off: Good balance of precision and recall\")\n",
    "\n",
    "# High Precision (fewer false alarms)\n",
    "high_prec_idx = thresh_df[thresh_df['Precision'] >= 0.65]['Threshold'].min()\n",
    "if pd.notna(high_prec_idx):\n",
    "    hp_row = thresh_df[thresh_df['Threshold'] == high_prec_idx].iloc[0]\n",
    "    print(f\"\\n3. HIGH PRECISION PRIORITY (minimize false alarms)\")\n",
    "    print(f\"   Threshold: {high_prec_idx:.2f}\")\n",
    "    print(f\"   Recall: {hp_row['Recall']:.2%} | Precision: {hp_row['Precision']:.2%}\")\n",
    "    print(f\"   Trade-off: Fewer false alarms, but may miss some students\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-030",
   "metadata": {},
   "source": [
    "### 3.3 Calibration Analysis\n",
    "\n",
    "Calibration measures how well the predicted probabilities match actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy='uniform')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Perfect calibration line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Perfectly Calibrated',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "# Model calibration\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_pred, y=prob_true,\n",
    "    mode='lines+markers',\n",
    "    name='Random Forest',\n",
    "    line=dict(color='blue', width=2),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Calibration Curve - Random Forest Model',\n",
    "    xaxis_title='Mean Predicted Probability',\n",
    "    yaxis_title='Fraction of Positives',\n",
    "    height=500,\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Brier Score: {metrics['Brier Score']:.4f}\")\n",
    "print(\"(Lower is better - 0 is perfect, 0.25 is random for balanced classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of predicted probabilities\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    'Probability Distribution by Actual Outcome',\n",
    "    'Histogram of All Predictions'\n",
    "))\n",
    "\n",
    "# By actual outcome\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_prob[y_test == 0],\n",
    "    name='Actually Retained',\n",
    "    opacity=0.7,\n",
    "    marker_color='blue',\n",
    "    nbinsx=30\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_prob[y_test == 1],\n",
    "    name='Actually Departed',\n",
    "    opacity=0.7,\n",
    "    marker_color='red',\n",
    "    nbinsx=30\n",
    "), row=1, col=1)\n",
    "\n",
    "# All predictions\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_prob,\n",
    "    name='All Predictions',\n",
    "    marker_color='green',\n",
    "    nbinsx=30,\n",
    "    showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Predicted Probability', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Predicted Probability', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Count', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-033",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation for Stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-034",
   "metadata": {},
   "source": [
    "### 4.1 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = final_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display top 15 features\n",
    "print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*50)\n",
    "for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):\n",
    "    print(f\"{i:2}. {row['Feature']:<30} {row['Importance']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=top_features['Importance'],\n",
    "    y=top_features['Feature'],\n",
    "    orientation='h',\n",
    "    marker_color='steelblue',\n",
    "    text=top_features['Importance'].round(3),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Top {top_n} Feature Importances - Random Forest Model',\n",
    "    xaxis_title='Importance (Mean Decrease in Impurity)',\n",
    "    yaxis_title='Feature',\n",
    "    height=550,\n",
    "    margin=dict(l=200)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group features by category\n",
    "category_importance = {\n",
    "    'Academic Performance (GPA)': ['GPA_1', 'GPA_2', 'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA'],\n",
    "    'Course Completion': ['UNITS_COMPLETED_1', 'UNITS_COMPLETED_2', 'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2'],\n",
    "    'Academic Difficulty (DFW)': ['DFW_RATE_1', 'DFW_RATE_2', 'DFW_UNITS_1', 'DFW_UNITS_2'],\n",
    "    'Grade Points': ['GRADE_POINTS_1', 'GRADE_POINTS_2'],\n",
    "    'Demographics': [f for f in feature_names if any(x in f for x in ['RACE', 'GENDER', 'FIRST_GEN', 'COLLEGE'])]\n",
    "}\n",
    "\n",
    "category_totals = {}\n",
    "for category, features in category_importance.items():\n",
    "    total = importance_df[importance_df['Feature'].isin(features)]['Importance'].sum()\n",
    "    category_totals[category] = total\n",
    "\n",
    "cat_df = pd.DataFrame({\n",
    "    'Category': list(category_totals.keys()),\n",
    "    'Total Importance': list(category_totals.values())\n",
    "}).sort_values('Total Importance', ascending=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=cat_df['Total Importance'],\n",
    "    y=cat_df['Category'],\n",
    "    orientation='h',\n",
    "    marker_color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6'],\n",
    "    text=cat_df['Total Importance'].round(3),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance by Category',\n",
    "    xaxis_title='Total Importance',\n",
    "    yaxis_title='Feature Category',\n",
    "    height=400,\n",
    "    margin=dict(l=200)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-038",
   "metadata": {},
   "source": [
    "### 4.2 Creating Stakeholder-Friendly Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stakeholder-friendly feature descriptions\n",
    "feature_descriptions = {\n",
    "    'GPA_1': 'First semester GPA',\n",
    "    'GPA_2': 'Second semester GPA',\n",
    "    'DFW_RATE_1': 'Proportion of D/F/W grades in first semester',\n",
    "    'DFW_RATE_2': 'Proportion of D/F/W grades in second semester',\n",
    "    'HS_GPA': 'High school GPA',\n",
    "    'UNITS_COMPLETED_1': 'Credit hours completed in first semester',\n",
    "    'UNITS_COMPLETED_2': 'Credit hours completed in second semester',\n",
    "    'GRADE_POINTS_1': 'Total grade points earned in first semester',\n",
    "    'GRADE_POINTS_2': 'Total grade points earned in second semester'\n",
    "}\n",
    "\n",
    "print(\"STAKEHOLDER-FRIENDLY MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhat the model considers most important when predicting student departure:\\n\")\n",
    "\n",
    "for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "    feature = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    description = feature_descriptions.get(feature, feature)\n",
    "    \n",
    "    # Convert importance to percentage\n",
    "    pct = importance * 100 / importance_df['Importance'].sum()\n",
    "    \n",
    "    print(f\"{i}. {description}\")\n",
    "    print(f\"   Relative importance: {pct:.1f}%\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample prediction explanation\n",
    "def explain_prediction(model, X, feature_names, student_idx=0):\n",
    "    \"\"\"\n",
    "    Create a human-readable explanation for a single prediction.\n",
    "    \"\"\"\n",
    "    student_data = X.iloc[student_idx]\n",
    "    prob = model.predict_proba(X.iloc[[student_idx]])[0, 1]\n",
    "    \n",
    "    print(f\"PREDICTION EXPLANATION FOR STUDENT {student_idx}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPredicted Departure Probability: {prob:.1%}\")\n",
    "    \n",
    "    if prob >= 0.7:\n",
    "        risk_level = \"HIGH RISK\"\n",
    "    elif prob >= 0.4:\n",
    "        risk_level = \"MODERATE RISK\"\n",
    "    else:\n",
    "        risk_level = \"LOW RISK\"\n",
    "    \n",
    "    print(f\"Risk Level: {risk_level}\")\n",
    "    \n",
    "    print(\"\\nKey Student Characteristics:\")\n",
    "    key_features = ['GPA_1', 'GPA_2', 'DFW_RATE_1', 'DFW_RATE_2', 'HS_GPA']\n",
    "    for feature in key_features:\n",
    "        if feature in student_data.index:\n",
    "            value = student_data[feature]\n",
    "            desc = feature_descriptions.get(feature, feature)\n",
    "            print(f\"  - {desc}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Show example\n",
    "explain_prediction(final_model, X_test, feature_names, student_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-041",
   "metadata": {},
   "source": [
    "## 5. Preparing for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-042",
   "metadata": {},
   "source": [
    "### 5.1 Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model artifacts directory\n",
    "model_dir = '../../models/'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model using joblib (recommended for scikit-learn)\n",
    "model_filename = os.path.join(model_dir, 'student_departure_rf_model.joblib')\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"Model saved to: {model_filename}\")\n",
    "\n",
    "# Verify the saved model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "test_pred = loaded_model.predict_proba(X_test[:5])[:, 1]\n",
    "original_pred = final_model.predict_proba(X_test[:5])[:, 1]\n",
    "\n",
    "print(f\"\\nModel verification:\")\n",
    "print(f\"Predictions match: {np.allclose(test_pred, original_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names for consistency\n",
    "feature_file = os.path.join(model_dir, 'feature_names.json')\n",
    "with open(feature_file, 'w') as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(f\"Feature names saved to: {feature_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': 'Student Departure Prediction Model',\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'training_samples': len(X_train),\n",
    "    'n_features': len(feature_names),\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 12,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 5,\n",
    "        'max_features': 'sqrt',\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'roc_auc': round(metrics['ROC-AUC'], 4),\n",
    "        'f1_score': round(metrics['F1 Score'], 4),\n",
    "        'recall': round(metrics['Recall'], 4),\n",
    "        'precision': round(metrics['Precision'], 4),\n",
    "        'accuracy': round(metrics['Accuracy'], 4)\n",
    "    },\n",
    "    'recommended_threshold': {\n",
    "        'default': 0.5,\n",
    "        'high_recall': 0.35,\n",
    "        'balanced': float(optimal_f1_thresh),\n",
    "        'high_precision': 0.65\n",
    "    },\n",
    "    'feature_importance_top_5': importance_df.head(5).to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(model_dir, 'model_metadata.json')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved to: {metadata_file}\")\n",
    "print(\"\\nMetadata contents:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-046",
   "metadata": {},
   "source": [
    "### 5.2 Creating a Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDeparturePredictionPipeline:\n",
    "    \"\"\"\n",
    "    A complete prediction pipeline for student departure risk assessment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, feature_names_path, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to the saved model file\n",
    "        feature_names_path : str\n",
    "            Path to the feature names JSON file\n",
    "        threshold : float\n",
    "            Classification threshold (default: 0.5)\n",
    "        \"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        with open(feature_names_path, 'r') as f:\n",
    "            self.feature_names = json.load(f)\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def preprocess(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess input data to match training format.\n",
    "        \"\"\"\n",
    "        # Define expected features\n",
    "        numeric_features = [\n",
    "            'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA',\n",
    "            'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
    "            'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2',\n",
    "            'DFW_UNITS_1', 'DFW_UNITS_2',\n",
    "            'GPA_1', 'GPA_2',\n",
    "            'DFW_RATE_1', 'DFW_RATE_2',\n",
    "            'GRADE_POINTS_1', 'GRADE_POINTS_2'\n",
    "        ]\n",
    "        \n",
    "        categorical_features = ['RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'COLLEGE']\n",
    "        \n",
    "        # One-hot encode\n",
    "        df_encoded = pd.get_dummies(df[numeric_features + categorical_features],\n",
    "                                    columns=categorical_features, drop_first=True)\n",
    "        \n",
    "        # Align with training features\n",
    "        for feature in self.feature_names:\n",
    "            if feature not in df_encoded.columns:\n",
    "                df_encoded[feature] = 0\n",
    "        \n",
    "        # Reorder columns\n",
    "        df_encoded = df_encoded[self.feature_names]\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_encoded = df_encoded.fillna(df_encoded.median())\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Generate predictions for input data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with 'probabilities', 'predictions', and 'risk_levels'\n",
    "        \"\"\"\n",
    "        X = self.preprocess(df)\n",
    "        \n",
    "        probabilities = self.model.predict_proba(X)[:, 1]\n",
    "        predictions = (probabilities >= self.threshold).astype(int)\n",
    "        \n",
    "        # Assign risk levels\n",
    "        risk_levels = []\n",
    "        for prob in probabilities:\n",
    "            if prob >= 0.7:\n",
    "                risk_levels.append('High Risk')\n",
    "            elif prob >= 0.4:\n",
    "                risk_levels.append('Moderate Risk')\n",
    "            else:\n",
    "                risk_levels.append('Low Risk')\n",
    "        \n",
    "        return {\n",
    "            'probabilities': probabilities,\n",
    "            'predictions': predictions,\n",
    "            'risk_levels': risk_levels\n",
    "        }\n",
    "    \n",
    "    def generate_report(self, df, output_file=None):\n",
    "        \"\"\"\n",
    "        Generate a student risk assessment report.\n",
    "        \"\"\"\n",
    "        results = self.predict(df)\n",
    "        \n",
    "        report_df = df[['SID']].copy() if 'SID' in df.columns else df.iloc[:, :1].copy()\n",
    "        report_df['Departure_Probability'] = results['probabilities']\n",
    "        report_df['Predicted_Departure'] = results['predictions']\n",
    "        report_df['Risk_Level'] = results['risk_levels']\n",
    "        \n",
    "        if output_file:\n",
    "            report_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return report_df\n",
    "\n",
    "print(\"StudentDeparturePredictionPipeline class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "pipeline = StudentDeparturePredictionPipeline(\n",
    "    model_path=model_filename,\n",
    "    feature_names_path=feature_file,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "# Generate predictions for a few students\n",
    "sample_students = test_df.head(10)\n",
    "results = pipeline.predict(sample_students)\n",
    "\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(5, len(sample_students))):\n",
    "    sid = sample_students.iloc[i]['SID'] if 'SID' in sample_students.columns else f\"Student {i}\"\n",
    "    prob = results['probabilities'][i]\n",
    "    risk = results['risk_levels'][i]\n",
    "    actual = sample_students.iloc[i]['DEPARTED']\n",
    "    \n",
    "    print(f\"{sid}: Prob={prob:.2%}, Risk={risk}, Actual={'Departed' if actual else 'Retained'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-049",
   "metadata": {},
   "source": [
    "### 5.3 Model Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive model documentation\n",
    "documentation = f\"\"\"\n",
    "================================================================================\n",
    "                    STUDENT DEPARTURE PREDICTION MODEL\n",
    "                           Model Documentation\n",
    "================================================================================\n",
    "\n",
    "VERSION: 1.0.0\n",
    "DATE: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "1. MODEL OVERVIEW\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Purpose: Predict the probability that a student will depart (not return) by \n",
    "         their third semester based on academic and demographic factors.\n",
    "\n",
    "Model Type: Random Forest Classifier\n",
    "Algorithm: Ensemble of {final_model.n_estimators} decision trees with majority voting\n",
    "\n",
    "Target Variable: Student departure status (1 = Departed, 0 = Retained)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "2. INPUT FEATURES\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "The model uses {len(feature_names)} features across these categories:\n",
    "\n",
    "Academic Performance:\n",
    "  - GPA_1, GPA_2: First and second semester GPA\n",
    "  - HS_GPA, HS_MATH_GPA, HS_ENGL_GPA: High school performance\n",
    "  \n",
    "Course Completion:\n",
    "  - UNITS_ATTEMPTED_1, UNITS_ATTEMPTED_2: Credit hours attempted\n",
    "  - UNITS_COMPLETED_1, UNITS_COMPLETED_2: Credit hours completed\n",
    "  \n",
    "Academic Difficulty:\n",
    "  - DFW_RATE_1, DFW_RATE_2: Proportion of D/F/W grades\n",
    "  - DFW_UNITS_1, DFW_UNITS_2: Number of D/F/W credit hours\n",
    "  \n",
    "Demographics:\n",
    "  - RACE_ETHNICITY, GENDER, FIRST_GEN_STATUS, COLLEGE\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "3. PERFORMANCE METRICS\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Evaluated on held-out test set ({len(y_test):,} students):\n",
    "\n",
    "  ROC-AUC:      {metrics['ROC-AUC']:.4f}  (Discrimination ability)\n",
    "  F1 Score:     {metrics['F1 Score']:.4f}  (Balance of precision and recall)\n",
    "  Recall:       {metrics['Recall']:.4f}  (Sensitivity - at-risk students identified)\n",
    "  Precision:    {metrics['Precision']:.4f}  (Positive predictive value)\n",
    "  Accuracy:     {metrics['Accuracy']:.4f}  (Overall correctness)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "4. RECOMMENDED THRESHOLDS\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Choose threshold based on institutional priorities:\n",
    "\n",
    "  High Recall (0.35):      Identify most at-risk students\n",
    "                           Trade-off: More false positives\n",
    "                           \n",
    "  Balanced ({optimal_f1_thresh:.2f}):        Optimize F1 score\n",
    "                           Trade-off: Good balance\n",
    "                           \n",
    "  High Precision (0.65):   Minimize false alarms\n",
    "                           Trade-off: May miss some students\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "5. RISK LEVEL DEFINITIONS\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "  HIGH RISK (>= 70%):      Immediate intervention recommended\n",
    "  MODERATE RISK (40-70%):  Proactive outreach recommended\n",
    "  LOW RISK (< 40%):        Standard support sufficient\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "6. LIMITATIONS AND CONSIDERATIONS\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "- Model is trained on historical data and may not capture recent changes\n",
    "- Predictions are probabilities, not certainties\n",
    "- Should be used as one input among many in advising decisions\n",
    "- Regular retraining recommended (annually at minimum)\n",
    "- Monitor for demographic disparities in predictions\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "7. USAGE\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "from pipeline import StudentDeparturePredictionPipeline\n",
    "\n",
    "pipeline = StudentDeparturePredictionPipeline(\n",
    "    model_path='models/student_departure_rf_model.joblib',\n",
    "    feature_names_path='models/feature_names.json',\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "results = pipeline.predict(student_data)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(documentation)\n",
    "\n",
    "# Save documentation\n",
    "doc_file = os.path.join(model_dir, 'model_documentation.txt')\n",
    "with open(doc_file, 'w') as f:\n",
    "    f.write(documentation)\n",
    "print(f\"Documentation saved to: {doc_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-051",
   "metadata": {},
   "source": [
    "## 6. Deployment Considerations for Higher Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-052",
   "metadata": {},
   "source": [
    "### 6.1 Integration Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-053",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_strategies = {\n",
    "    'Strategy': [\n",
    "        'Batch Processing',\n",
    "        'Real-time API',\n",
    "        'Dashboard Integration',\n",
    "        'SIS Integration',\n",
    "        'Early Alert System'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Run predictions weekly/monthly on all students',\n",
    "        'REST API for on-demand predictions',\n",
    "        'Integrate into advising dashboards (Tableau, Power BI)',\n",
    "        'Connect to Student Information System',\n",
    "        'Trigger alerts for high-risk students'\n",
    "    ],\n",
    "    'Complexity': ['Low', 'Medium', 'Medium', 'High', 'Medium'],\n",
    "    'Best For': [\n",
    "        'Small institutions, limited IT',\n",
    "        'Custom applications, flexibility',\n",
    "        'Visual analytics, reporting',\n",
    "        'Seamless workflow integration',\n",
    "        'Proactive intervention'\n",
    "    ]\n",
    "}\n",
    "\n",
    "integration_df = pd.DataFrame(integration_strategies)\n",
    "\n",
    "print(\"INTEGRATION STRATEGIES FOR HIGHER EDUCATION\")\n",
    "print(\"=\"*90)\n",
    "for _, row in integration_df.iterrows():\n",
    "    print(f\"\\n{row['Strategy']} (Complexity: {row['Complexity']})\")\n",
    "    print(f\"  Description: {row['Description']}\")\n",
    "    print(f\"  Best for: {row['Best For']}\")\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-054",
   "metadata": {},
   "source": [
    "### 6.2 Monitoring and Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-055",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_checklist = \"\"\"\n",
    "MODEL MONITORING AND MAINTENANCE CHECKLIST\n",
    "================================================================================\n",
    "\n",
    "REGULAR MONITORING (Monthly)\n",
    "[ ] Check prediction distribution - has it shifted significantly?\n",
    "[ ] Review feature distributions for input drift\n",
    "[ ] Monitor API response times (if applicable)\n",
    "[ ] Track number of predictions made\n",
    "\n",
    "PERFORMANCE VALIDATION (Quarterly)\n",
    "[ ] Calculate actual vs predicted outcomes for past predictions\n",
    "[ ] Update performance metrics (AUC, precision, recall)\n",
    "[ ] Check for performance degradation\n",
    "[ ] Analyze performance by demographic group\n",
    "\n",
    "MODEL RETRAINING (Annually or as needed)\n",
    "[ ] Collect new training data from recent cohorts\n",
    "[ ] Retrain model with updated data\n",
    "[ ] Validate new model against holdout set\n",
    "[ ] Compare performance to previous version\n",
    "[ ] Document changes and update version number\n",
    "\n",
    "ALERT CONDITIONS (Investigate immediately)\n",
    "- Performance drops more than 5% from baseline\n",
    "- Prediction distribution shifts significantly\n",
    "- Feature values outside expected ranges\n",
    "- Significant disparities across demographic groups\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(monitoring_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-056",
   "metadata": {},
   "source": [
    "### 6.3 Ethical Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-057",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethical_considerations = \"\"\"\n",
    "ETHICAL CONSIDERATIONS FOR STUDENT DEPARTURE PREDICTION\n",
    "================================================================================\n",
    "\n",
    "1. TRANSPARENCY\n",
    "   - Students should be informed that predictive analytics are used\n",
    "   - Explain what data is collected and how it's used\n",
    "   - Provide opt-out options where feasible\n",
    "\n",
    "2. FAIRNESS\n",
    "   - Regularly audit model for demographic disparities\n",
    "   - Ensure interventions don't disadvantage any group\n",
    "   - Consider removing demographic features if they cause bias\n",
    "\n",
    "3. PRIVACY\n",
    "   - Limit access to predictions on a need-to-know basis\n",
    "   - Secure storage and transmission of student data\n",
    "   - Comply with FERPA and institutional policies\n",
    "\n",
    "4. HUMAN OVERSIGHT\n",
    "   - Predictions should inform, not replace, human judgment\n",
    "   - Advisors should have final decision-making authority\n",
    "   - Regular review by diverse stakeholders\n",
    "\n",
    "5. AVOIDING HARM\n",
    "   - Risk labels should not become self-fulfilling prophecies\n",
    "   - Interventions should be supportive, not punitive\n",
    "   - Consider unintended consequences of classification\n",
    "\n",
    "6. ACCOUNTABILITY\n",
    "   - Clear ownership of model decisions\n",
    "   - Process for students to appeal or dispute predictions\n",
    "   - Document all model changes and their rationale\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(ethical_considerations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model fairness across demographic groups\n",
    "print(\"MODEL FAIRNESS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add predictions to test data for analysis\n",
    "test_df_analysis = test_df.copy()\n",
    "test_df_analysis['predicted_prob'] = y_prob\n",
    "test_df_analysis['predicted_class'] = y_pred\n",
    "\n",
    "# Check performance by demographic groups\n",
    "for group_col in ['GENDER', 'FIRST_GEN_STATUS']:\n",
    "    if group_col in test_df_analysis.columns:\n",
    "        print(f\"\\nPerformance by {group_col}:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for group in test_df_analysis[group_col].unique():\n",
    "            mask = test_df_analysis[group_col] == group\n",
    "            group_y_true = test_df_analysis.loc[mask, 'DEPARTED']\n",
    "            group_y_prob = test_df_analysis.loc[mask, 'predicted_prob']\n",
    "            group_y_pred = test_df_analysis.loc[mask, 'predicted_class']\n",
    "            \n",
    "            if len(group_y_true) > 0 and group_y_true.nunique() > 1:\n",
    "                auc = roc_auc_score(group_y_true, group_y_prob)\n",
    "                recall = recall_score(group_y_true, group_y_pred)\n",
    "                \n",
    "                print(f\"  {group}: n={len(group_y_true):,}, AUC={auc:.3f}, Recall={recall:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-059",
   "metadata": {},
   "source": [
    "## 7. Summary and Course Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"                    COURSE 3 SUMMARY: MODEL COMPARISON AND SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "MODELS EXPLORED:\n",
    "----------------\n",
    "Module 1: Regularized Logistic Regression (L1, L2, ElasticNet)\n",
    "Module 2: Decision Trees\n",
    "Module 3: Random Forests\n",
    "Module 4: Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "Module 5: Neural Networks (MLPClassifier)\n",
    "Module 6: Model Comparison and Final Selection\n",
    "\n",
    "SELECTED MODEL:\n",
    "---------------\n",
    "Random Forest Classifier\n",
    "- Best balance of performance and interpretability\n",
    "- Robust to overfitting\n",
    "- Feature importance for stakeholder explanations\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "FINAL MODEL PERFORMANCE:\n",
    "------------------------\n",
    "ROC-AUC:      {metrics['ROC-AUC']:.4f}\n",
    "F1 Score:     {metrics['F1 Score']:.4f}\n",
    "Recall:       {metrics['Recall']:.4f}\n",
    "Precision:    {metrics['Precision']:.4f}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "DEPLOYMENT ARTIFACTS CREATED:\n",
    "-----------------------------\n",
    "1. Trained model (joblib format)\n",
    "2. Feature names (JSON format)\n",
    "3. Model metadata (JSON format)\n",
    "4. Model documentation (text format)\n",
    "5. Prediction pipeline class\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "KEY RECOMMENDATIONS:\n",
    "--------------------\n",
    "1. Use High Recall threshold (0.35) if identifying all at-risk students is priority\n",
    "2. Use Balanced threshold (~0.45) for general use\n",
    "3. Monitor model performance quarterly\n",
    "4. Retrain annually with new cohort data\n",
    "5. Regularly audit for fairness across demographic groups\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-061",
   "metadata": {},
   "source": [
    "### Key Takeaways from Course 3\n",
    "\n",
    "| Topic | Key Learning |\n",
    "|:------|:-------------|\n",
    "| **Model Selection** | No single model is best for all situations - consider interpretability, performance, and deployment requirements |\n",
    "| **Regularization** | L1/L2 regularization prevents overfitting and can perform feature selection |\n",
    "| **Ensemble Methods** | Random Forests and Gradient Boosting often achieve best performance by combining multiple models |\n",
    "| **Neural Networks** | Powerful for complex patterns but require more data and tuning |\n",
    "| **Threshold Tuning** | Adjust classification threshold based on institutional priorities (recall vs precision) |\n",
    "| **Deployment** | Production models need monitoring, documentation, and maintenance plans |\n",
    "| **Ethics** | Consider fairness, transparency, and potential unintended consequences |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With your model trained, validated, and documented, you are ready to:\n",
    "\n",
    "1. **Deploy** the model in your institution's systems\n",
    "2. **Monitor** performance and fairness over time\n",
    "3. **Iterate** by collecting feedback and improving the model\n",
    "4. **Expand** to other prediction tasks (e.g., graduation, major selection)\n",
    "\n",
    "**Congratulations on completing Course 3!**\n",
    "\n",
    "You now have the skills to build, compare, and deploy machine learning models for student success prediction in higher education contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved artifacts\n",
    "print(\"\\nSAVED MODEL ARTIFACTS:\")\n",
    "print(\"=\"*50)\n",
    "for filename in os.listdir(model_dir):\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    size = os.path.getsize(filepath) / 1024  # Size in KB\n",
    "    print(f\"  {filename}: {size:.1f} KB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nCourse 3, Module 6 complete!\")"
   ]
  }
 ]
}