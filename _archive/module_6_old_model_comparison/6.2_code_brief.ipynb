{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.2 Final Model Selection and Deployment - Code Brief\n",
        "\n",
        "Condensed reference for model selection, validation, and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.calibration import calibration_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final model with optimized parameters\n",
        "final_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=12,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    oob_score=True\n",
        ")\n",
        "\n",
        "final_model.fit(X_train, y_train)\n",
        "print(f\"OOB Score: {final_model.oob_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_prob = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision': precision_score(y_test, y_pred),\n",
        "    'Recall': recall_score(y_test, y_pred),\n",
        "    'F1 Score': f1_score(y_test, y_pred),\n",
        "    'ROC-AUC': roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics at different thresholds\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "threshold_metrics = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
        "    threshold_metrics.append({\n",
        "        'Threshold': thresh,\n",
        "        'Precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred_thresh, zero_division=0),\n",
        "        'F1 Score': f1_score(y_test, y_pred_thresh, zero_division=0)\n",
        "    })\n",
        "\n",
        "thresh_df = pd.DataFrame(threshold_metrics)\n",
        "optimal_f1_thresh = thresh_df.loc[thresh_df['F1 Score'].idxmax(), 'Threshold']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': final_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(importance_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Serialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_dir = '../../models/'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(final_model, os.path.join(model_dir, 'final_model.joblib'))\n",
        "\n",
        "# Save feature names\n",
        "with open(os.path.join(model_dir, 'feature_names.json'), 'w') as f:\n",
        "    json.dump(feature_names, f)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'model_type': 'RandomForestClassifier',\n",
        "    'version': '1.0.0',\n",
        "    'created_date': datetime.now().isoformat(),\n",
        "    'n_features': len(feature_names),\n",
        "    'performance_metrics': metrics,\n",
        "    'recommended_thresholds': {\n",
        "        'default': 0.5,\n",
        "        'high_recall': 0.35,\n",
        "        'balanced': float(optimal_f1_thresh)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(model_dir, 'metadata.json'), 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PredictionPipeline:\n",
        "    def __init__(self, model_path, feature_names_path, threshold=0.5):\n",
        "        self.model = joblib.load(model_path)\n",
        "        with open(feature_names_path, 'r') as f:\n",
        "            self.feature_names = json.load(f)\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def predict(self, X):\n",
        "        probabilities = self.model.predict_proba(X)[:, 1]\n",
        "        predictions = (probabilities >= self.threshold).astype(int)\n",
        "        \n",
        "        risk_levels = []\n",
        "        for prob in probabilities:\n",
        "            if prob >= 0.7:\n",
        "                risk_levels.append('High Risk')\n",
        "            elif prob >= 0.4:\n",
        "                risk_levels.append('Moderate Risk')\n",
        "            else:\n",
        "                risk_levels.append('Low Risk')\n",
        "        \n",
        "        return {\n",
        "            'probabilities': probabilities,\n",
        "            'predictions': predictions,\n",
        "            'risk_levels': risk_levels\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Use Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load saved model\n",
        "loaded_model = joblib.load(os.path.join(model_dir, 'final_model.joblib'))\n",
        "\n",
        "# Verify predictions match\n",
        "test_pred = loaded_model.predict_proba(X_test[:5])[:, 1]\n",
        "original_pred = final_model.predict_proba(X_test[:5])[:, 1]\n",
        "print(f\"Predictions match: {np.allclose(test_pred, original_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts\n",
        "\n",
        "| Threshold | Priority | Trade-off |\n",
        "|:----------|:---------|:----------|\n",
        "| 0.35 | High Recall | More false alarms |\n",
        "| 0.45-0.50 | Balanced | Good F1 score |\n",
        "| 0.65+ | High Precision | May miss students |\n",
        "\n",
        "## Risk Levels\n",
        "\n",
        "| Level | Probability | Action |\n",
        "|:------|:------------|:-------|\n",
        "| High | >= 70% | Immediate intervention |\n",
        "| Moderate | 40-70% | Proactive outreach |\n",
        "| Low | < 40% | Standard support |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
