{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 2: Equity Analysis - Model Performance by Student Subgroups\n",
    "\n",
    "# Student Workbook\n",
    "\n",
    "Welcome to your Capstone Project workbook! This notebook provides a structured outline for your equity analysis. Your task is to fill in the missing code where indicated (replace `...` with appropriate code) to complete the analysis. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Equity in Predictive Analytics\n",
    "\n",
    "As universities increasingly adopt machine learning models for student success initiatives, it is critical to ensure these models do not perpetuate or amplify existing inequities.\n",
    "\n",
    "This capstone project examines model performance across three key demographic dimensions:\n",
    "1. **Race/Ethnicity**\n",
    "2. **First Generation Status**\n",
    "3. **Gender**\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will be able to:\n",
    "1. Evaluate model performance separately for demographic subgroups\n",
    "2. Identify potential sources of algorithmic bias\n",
    "3. Apply fairness metrics to machine learning models\n",
    "4. Develop policy recommendations for equitable model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Import Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_location = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/data/'\n",
    "df = ...                    # Load the student academics data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "...                         # Display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Rare Classes in RACE_ETHNICITY\n",
    "df['RACE_ETHNICITY'] = ...     # Replace rare categories with 'Other'\n",
    "\n",
    "# Address Rare Classes in GENDER\n",
    "df = ...                       # Remove Nonbinary rows\n",
    "df['GENDER'] = ...             # Clean and standardize\n",
    "\n",
    "# Drop noninformative features\n",
    "...                            # Drop SEM_1_STATUS and SEM_2_STATUS\n",
    "\n",
    "# Remove duplicates and handle missing values\n",
    "...                            # Remove duplicate rows\n",
    "...                            # Drop columns with >50% missing\n",
    "\n",
    "# Create binary target\n",
    "df['DEPARTED'] = ...           # Create target variable\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Examine Demographic Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display demographic distributions\n",
    "print(\"DEMOGRAPHIC DISTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nRace/Ethnicity:\")\n",
    "print(df['RACE_ETHNICITY'].value_counts())\n",
    "\n",
    "print(\"\\nFirst Generation Status:\")\n",
    "print(df['FIRST_GEN_STATUS'].value_counts())\n",
    "\n",
    "print(\"\\nGender:\")\n",
    "print(df['GENDER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize departure rates by demographic groups\n",
    "# Your code to create a visualization showing departure rates across groups\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Prepare Features and Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering - create DFW rates and grade points\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['DFW_RATE_1'] = ...     # Calculate DFW rate for semester 1\n",
    "    df['DFW_RATE_2'] = ...     # Calculate DFW rate for semester 2\n",
    "    df['GRADE_POINTS_1'] = ... # Calculate grade points for semester 1\n",
    "    df['GRADE_POINTS_2'] = ... # Calculate grade points for semester 2\n",
    "    return df\n",
    "\n",
    "df = create_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (excluding demographic features from model inputs)\n",
    "numeric_features = ...         # List of numeric feature columns\n",
    "\n",
    "categorical_features = ...     # List of non-demographic categorical features (e.g., COLLEGE)\n",
    "\n",
    "# Store demographic columns for later analysis\n",
    "demographic_cols = ['RACE_ETHNICITY', 'FIRST_GEN_STATUS', 'GENDER']\n",
    "\n",
    "target = 'DEPARTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in numeric features\n",
    "for col in numeric_features:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Train/Test split\n",
    "train_df, test_df = ...        # Split with 80/20 and stratification\n",
    "\n",
    "print(f\"Training set: {len(train_df):,} students\")\n",
    "print(f\"Testing set: {len(test_df):,} students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices (one-hot encode categorical features)\n",
    "train_encoded = ...            # One-hot encode training features\n",
    "test_encoded = ...             # One-hot encode test features\n",
    "\n",
    "# Align columns\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = ...                  # Training features\n",
    "y_train = ...                  # Training target\n",
    "X_test = ...                   # Test features\n",
    "y_test = ...                   # Test target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = ...           # Fit and transform\n",
    "X_test_scaled = ...            # Transform only\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Train Best-Performing Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "rf_model = ...                 # Create RandomForestClassifier with balanced class weights\n",
    "...                            # Fit on training data\n",
    "\n",
    "# Get predictions\n",
    "y_pred = ...                   # Get class predictions\n",
    "y_prob = ...                   # Get probability of positive class\n",
    "\n",
    "print(f\"\\nOverall Model Performance:\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fairness Analysis by Subgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Define Fairness Metrics Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subgroup_metrics(y_true, y_pred, y_prob, group_labels, group_name):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for each subgroup.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for group in group_labels.unique():\n",
    "        mask = group_labels == group\n",
    "        n = mask.sum()\n",
    "        \n",
    "        if n < 10:  # Skip groups with too few samples\n",
    "            continue\n",
    "            \n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        y_prob_group = y_prob[mask]\n",
    "        \n",
    "        # Calculate confusion matrix components\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group, labels=[0, 1]).ravel()\n",
    "        \n",
    "        metrics = {\n",
    "            'Group': group,\n",
    "            'N': n,\n",
    "            'Base Rate': ...           # Calculate actual departure rate\n",
    "            'Positive Rate': ...       # Calculate predicted departure rate\n",
    "            'Accuracy': ...            # Calculate accuracy\n",
    "            'Precision': ...           # Calculate precision\n",
    "            'Recall': ...              # Calculate recall\n",
    "            'F1 Score': ...            # Calculate F1 score\n",
    "            'ROC-AUC': ...             # Calculate ROC-AUC (handle single-class case)\n",
    "            'FPR': ...                 # Calculate False Positive Rate\n",
    "            'FNR': ...                 # Calculate False Negative Rate\n",
    "        }\n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Analyze Performance by Race/Ethnicity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set demographic labels\n",
    "test_race = ...                # Extract race/ethnicity from test_df\n",
    "\n",
    "# Calculate metrics by race/ethnicity\n",
    "race_metrics = calculate_subgroup_metrics(\n",
    "    y_test.values, y_pred, y_prob, \n",
    "    pd.Series(test_race), 'Race/Ethnicity'\n",
    ")\n",
    "\n",
    "print(\"MODEL PERFORMANCE BY RACE/ETHNICITY\")\n",
    "print(\"=\"*100)\n",
    "print(race_metrics.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize racial equity metrics\n",
    "# Your code to create visualizations\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8: Analyze Performance by First Generation Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set first gen labels\n",
    "test_firstgen = ...            # Extract first gen status from test_df\n",
    "\n",
    "# Calculate metrics by first gen status\n",
    "fg_metrics = calculate_subgroup_metrics(\n",
    "    y_test.values, y_pred, y_prob, \n",
    "    pd.Series(test_firstgen), 'First Gen Status'\n",
    ")\n",
    "\n",
    "print(\"MODEL PERFORMANCE BY FIRST GENERATION STATUS\")\n",
    "print(\"=\"*100)\n",
    "print(fg_metrics.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first gen equity metrics\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 9: Analyze Performance by Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set gender labels\n",
    "test_gender = ...              # Extract gender from test_df\n",
    "\n",
    "# Calculate metrics by gender\n",
    "gender_metrics = calculate_subgroup_metrics(\n",
    "    y_test.values, y_pred, y_prob, \n",
    "    pd.Series(test_gender), 'Gender'\n",
    ")\n",
    "\n",
    "print(\"MODEL PERFORMANCE BY GENDER\")\n",
    "print(\"=\"*100)\n",
    "print(gender_metrics.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gender equity metrics\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fairness Disparity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 10: Calculate Disparity Ratios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disparity_ratios(metrics_df, metric_name, reference_group=None):\n",
    "    \"\"\"\n",
    "    Calculate disparity ratios relative to a reference group.\n",
    "    A ratio of 1.0 indicates parity.\n",
    "    \"\"\"\n",
    "    if reference_group is None:\n",
    "        # Use group with largest N as reference\n",
    "        reference_group = metrics_df.loc[metrics_df['N'].idxmax(), 'Group']\n",
    "    \n",
    "    reference_value = ...      # Get reference group's metric value\n",
    "    \n",
    "    disparity = metrics_df.copy()\n",
    "    disparity[f'{metric_name} Ratio'] = ...  # Calculate ratio for each group\n",
    "    \n",
    "    return disparity[['Group', 'N', metric_name, f'{metric_name} Ratio']], reference_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disparity ratios for key metrics\n",
    "print(\"FAIRNESS DISPARITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ROC-AUC disparity by race\n",
    "auc_disparity, ref = calculate_disparity_ratios(race_metrics, 'ROC-AUC')\n",
    "print(f\"\\nROC-AUC Disparity by Race/Ethnicity (Reference: {ref})\")\n",
    "print(auc_disparity.round(4).to_string(index=False))\n",
    "\n",
    "# FPR disparity by race\n",
    "fpr_disparity, ref = calculate_disparity_ratios(race_metrics, 'FPR')\n",
    "print(f\"\\nFalse Positive Rate Disparity by Race/Ethnicity (Reference: {ref})\")\n",
    "print(fpr_disparity.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 11: Create Comprehensive Fairness Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive fairness dashboard visualization\n",
    "# Your code to create a multi-panel visualization showing:\n",
    "# - ROC-AUC by each demographic dimension\n",
    "# - Error rates (FPR, FNR) by each demographic dimension\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Subgroup-Specific Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 12: Train Separate Models for Key Subgroups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subgroup_model(train_df, test_df, subgroup_col, subgroup_val, X_cols, target):\n",
    "    \"\"\"\n",
    "    Train a model specifically for a demographic subgroup.\n",
    "    \"\"\"\n",
    "    # Filter to subgroup\n",
    "    train_sub = ...            # Filter training data to subgroup\n",
    "    test_sub = ...             # Filter test data to subgroup\n",
    "    \n",
    "    if len(train_sub) < 100 or len(test_sub) < 20:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Prepare features\n",
    "    X_train_sub = ...          # Prepare training features\n",
    "    y_train_sub = ...          # Prepare training target\n",
    "    X_test_sub = ...           # Prepare test features\n",
    "    y_test_sub = ...           # Prepare test target\n",
    "    \n",
    "    # Train model\n",
    "    model = ...                # Create and train RandomForestClassifier\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_sub = ...           # Get predictions\n",
    "    y_prob_sub = ...           # Get probabilities\n",
    "    \n",
    "    metrics = {\n",
    "        'Subgroup': subgroup_val,\n",
    "        'N_train': len(train_sub),\n",
    "        'N_test': len(test_sub),\n",
    "        'Accuracy': accuracy_score(y_test_sub, y_pred_sub),\n",
    "        'F1 Score': f1_score(y_test_sub, y_pred_sub),\n",
    "        'ROC-AUC': roc_auc_score(y_test_sub, y_prob_sub) if len(np.unique(y_test_sub)) > 1 else np.nan\n",
    "    }\n",
    "    \n",
    "    return model, metrics, (y_test_sub, y_pred_sub, y_prob_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for first gen vs continuing gen\n",
    "print(\"Training subgroup-specific models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_cols = numeric_features\n",
    "subgroup_results = []\n",
    "\n",
    "for fg_status in ['First Generation', 'Continuing Generation']:\n",
    "    model, metrics, _ = train_subgroup_model(\n",
    "        train_df, test_df, 'FIRST_GEN_STATUS', fg_status, X_cols, target\n",
    "    )\n",
    "    if metrics:\n",
    "        subgroup_results.append(metrics)\n",
    "        print(f\"\\n{fg_status}:\")\n",
    "        print(f\"  Training samples: {metrics['N_train']:,}\")\n",
    "        print(f\"  Test samples: {metrics['N_test']:,}\")\n",
    "        print(f\"  ROC-AUC: {metrics['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare subgroup-specific models vs global model\n",
    "print(\"\\nCOMPARISON: SUBGROUP MODELS vs GLOBAL MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Your code to compare performance\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 13: Generate Equity Report Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"EQUITY ANALYSIS EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Your code to generate the executive summary\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 14: Produce Comprehensive Policy Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable: Equity Analysis and Policy Recommendations Report\n",
    "\n",
    "Using the analyses above, write a comprehensive report that addresses the following:\n",
    "\n",
    "1. **Fairness Metrics Summary**: Create tables showing model performance metrics for each demographic subgroup.\n",
    "\n",
    "2. **Disparity Analysis**: Calculate and interpret disparity ratios for key metrics.\n",
    "\n",
    "3. **Bias Identification**: Identify which groups experience higher false positive or false negative rates.\n",
    "\n",
    "4. **Root Cause Analysis**: Discuss potential reasons for performance disparities.\n",
    "\n",
    "5. **Policy Recommendations**: Provide specific recommendations for equitable model deployment.\n",
    "\n",
    "6. **Ethical Considerations**: Discuss the ethical implications of using predictive models for student success.\n",
    "\n",
    "> **Rubric**: Your report should be 3-4 pages and include:\n",
    "> - Summary tables of performance metrics by demographic group\n",
    "> - At least 3 visualizations from your fairness analysis\n",
    "> - Specific, actionable policy recommendations\n",
    "> - Discussion of ethical considerations and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Report (Write Below)\n",
    "\n",
    "*[Write your comprehensive equity analysis and policy recommendations report here]*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
