{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 1: Predicting Third Semester Retention - Model Tournament\n",
    "\n",
    "## Objective\n",
    "This notebook walks you through the UPAD cycle (Understand, Prepare, Analyze, Deploy) to:\n",
    "- Compare ALL classification models from Course 3 on the student departure prediction task\n",
    "- Build regularized logistic regression, decision tree, random forest, boosted model, and neural network classifiers\n",
    "- Create comprehensive comparison tables and select the best model\n",
    "- Write an interpretation report for university stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A university's Student Success Center is seeking to implement an early warning system to identify students at risk of not returning for their third semester. The center has tasked the institutional research team with developing and comparing multiple machine learning models to determine which approach will be most effective for their campus.\n",
    "\n",
    "This \"model tournament\" approach is important because different models have different strengths:\n",
    "- **Regularized Logistic Regression**: Highly interpretable, provides coefficients that show factor importance\n",
    "- **Decision Trees**: Create visual, rule-based explanations that advisors can easily understand\n",
    "- **Random Forests**: Ensemble methods that reduce overfitting and provide robust predictions\n",
    "- **Gradient Boosting**: State-of-the-art performance for tabular data\n",
    "- **Neural Networks**: Flexible models that can capture complex patterns\n",
    "\n",
    "The goal is to systematically evaluate each model family and provide a recommendation for which model should be deployed in the early warning system.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will be able to:\n",
    "1. Build and tune five different model families for classification\n",
    "2. Compare models across multiple performance metrics\n",
    "3. Consider trade-offs between accuracy, interpretability, and computational cost\n",
    "4. Communicate findings to non-technical stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Import Libraries and Data**\n",
    "\n",
    "We need to import all the libraries required for our five model families plus evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models - All five families\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay,\n",
    "    brier_score_loss, log_loss, make_scorer\n",
    ")\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_location = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/data/'\n",
    "df = pd.read_csv(f'{data_location}student_academics_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Quality - Handle Rare Categories and Missing Values**\n",
    "\n",
    "Following the data preparation steps from our tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Rare Classes in RACE_ETHNICITY\n",
    "df['RACE_ETHNICITY'] = df['RACE_ETHNICITY'].replace(\n",
    "    ['Unknown', 'Native Hawaiian or Other Pacific Islander', 'American Indian or Alaska Native'], \n",
    "    'Other'\n",
    ")\n",
    "print(\"Race/Ethnicity categories:\")\n",
    "print(df['RACE_ETHNICITY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Rare Classes in GENDER\n",
    "df = df[df['GENDER'] != 'Nonbinary']\n",
    "df['GENDER'] = df['GENDER'].str.strip().str.capitalize()\n",
    "print(\"Gender categories:\")\n",
    "print(df['GENDER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop noninformative features\n",
    "df.drop(['SEM_1_STATUS', 'SEM_2_STATUS'], axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "columns_to_drop = missing_values_count[missing_values_count / total_rows > 0.5].index.tolist()\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")\n",
    "print(f\"Columns dropped due to missingness: {columns_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Create Target Variable and Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target: 1 = Departed (not enrolled), 0 = Enrolled\n",
    "df['DEPARTED'] = (df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(df['DEPARTED'].value_counts())\n",
    "print(f\"\\nDeparture rate: {df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE, stratify=df['DEPARTED'])\n",
    "\n",
    "print(f\"Training set: {train_df.shape[0]:,} students\")\n",
    "print(f\"Testing set: {test_df.shape[0]:,} students\")\n",
    "print(f\"\\nDeparture rate (Train): {train_df['DEPARTED'].mean():.2%}\")\n",
    "print(f\"Departure rate (Test): {test_df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Handle Missing Values and Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df_train, df_test):\n",
    "    \"\"\"Impute missing values using train statistics to prevent data leakage.\"\"\"\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        if df_train[col].isnull().any():\n",
    "            if df_train[col].dtype in ['int64', 'float64']:\n",
    "                median_val = df_train[col].median()\n",
    "                df_train[col] = df_train[col].fillna(median_val)\n",
    "                df_test[col] = df_test[col].fillna(median_val)\n",
    "            else:\n",
    "                mode_val = df_train[col].mode()[0]\n",
    "                df_train[col] = df_train[col].fillna(mode_val)\n",
    "                df_test[col] = df_test[col].fillna(mode_val)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "train_df, test_df = impute_missing_values(train_df, test_df)\n",
    "print(\"Missing values imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create DFW rates and grade points\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # DFW Rate (proportion of attempted units not completed)\n",
    "    df['DFW_RATE_1'] = ((df['UNITS_ATTEMPTED_1'] - df['UNITS_COMPLETED_1']).clip(lower=0) \n",
    "                        / df['UNITS_ATTEMPTED_1'].replace(0, 1))\n",
    "    df['DFW_RATE_2'] = ((df['UNITS_ATTEMPTED_2'] - df['UNITS_COMPLETED_2']).clip(lower=0) \n",
    "                        / df['UNITS_ATTEMPTED_2'].replace(0, 1))\n",
    "    \n",
    "    # Grade Points\n",
    "    df['GRADE_POINTS_1'] = df['UNITS_ATTEMPTED_1'] * df['GPA_1']\n",
    "    df['GRADE_POINTS_2'] = df['UNITS_ATTEMPTED_2'] * df['GPA_2']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_features(train_df)\n",
    "test_df = create_features(test_df)\n",
    "print(\"Features created successfully.\")\n",
    "print(f\"New columns: DFW_RATE_1, DFW_RATE_2, GRADE_POINTS_1, GRADE_POINTS_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Define Feature Sets and Prepare Data for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories\n",
    "numeric_features = [\n",
    "    'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
    "    'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2',\n",
    "    'DFW_UNITS_1', 'DFW_UNITS_2',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'GRADE_POINTS_1', 'GRADE_POINTS_2'\n",
    "]\n",
    "\n",
    "categorical_features = ['RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'COLLEGE']\n",
    "\n",
    "target = 'DEPARTED'\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df[numeric_features + categorical_features], \n",
    "                               columns=categorical_features, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df[numeric_features + categorical_features], \n",
    "                              columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align columns between train and test\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Handle any remaining missing values\n",
    "train_encoded = train_encoded.fillna(train_encoded.median())\n",
    "test_encoded = test_encoded.fillna(test_encoded.median())\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_encoded\n",
    "y_train = train_df[target]\n",
    "X_test = test_encoded\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X_train.columns)} total):\")\n",
    "print(X_train.columns.tolist()[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for models that require it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Scaled mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Scaled std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize departure rates by key factors\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    'Departure Rate by First Gen Status',\n",
    "    'Departure Rate by College',\n",
    "    'GPA Distribution by Departure Status',\n",
    "    'DFW Rate Distribution by Departure Status'\n",
    "))\n",
    "\n",
    "# Departure by First Gen\n",
    "fg_rates = train_df.groupby('FIRST_GEN_STATUS')['DEPARTED'].mean().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=fg_rates['FIRST_GEN_STATUS'], y=fg_rates['DEPARTED'], \n",
    "           marker_color=['steelblue', 'coral', 'gray'][:len(fg_rates)]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Departure by College\n",
    "college_rates = train_df.groupby('COLLEGE')['DEPARTED'].mean().reset_index().sort_values('DEPARTED', ascending=False)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=college_rates['COLLEGE'], y=college_rates['DEPARTED'], marker_color='teal'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# GPA by Departure\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_df[train_df['DEPARTED']==0]['GPA_1'], name='Enrolled', opacity=0.7),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_df[train_df['DEPARTED']==1]['GPA_1'], name='Departed', opacity=0.7),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# DFW Rate by Departure\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_df[train_df['DEPARTED']==0]['DFW_RATE_1'], name='Enrolled', opacity=0.7, showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_df[train_df['DEPARTED']==1]['DFW_RATE_1'], name='Departed', opacity=0.7, showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Exploratory Data Analysis: Factors Related to Student Departure\")\n",
    "fig.update_xaxes(tickangle=-45, row=1, col=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tournament: Building and Comparing All Five Model Families\n",
    "\n",
    "We will now build one model from each of the five families covered in Course 3:\n",
    "1. Regularized Logistic Regression (L2)\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. Gradient Boosting\n",
    "5. Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all models and results\n",
    "models = {}\n",
    "training_times = {}\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Build Regularized Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularized Logistic Regression\n",
    "print(\"Training L2 Regularized Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_l2 = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.1,  # Inverse of regularization strength\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "lr_l2.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Logistic Regression (L2)'] = time.time() - start_time\n",
    "models['Logistic Regression (L2)'] = ('scaled', lr_l2)\n",
    "\n",
    "print(f\"Training completed in {training_times['Logistic Regression (L2)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8: Build Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with tuned hyperparameters\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "dt.fit(X_train, y_train)  # No scaling needed for tree-based models\n",
    "\n",
    "training_times['Decision Tree'] = time.time() - start_time\n",
    "models['Decision Tree'] = ('unscaled', dt)\n",
    "\n",
    "print(f\"Training completed in {training_times['Decision Tree']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 9: Build Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with tuned hyperparameters\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "training_times['Random Forest'] = time.time() - start_time\n",
    "models['Random Forest'] = ('unscaled', rf)\n",
    "\n",
    "print(f\"Training completed in {training_times['Random Forest']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 10: Build Gradient Boosting Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "print(\"Training Gradient Boosting Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "training_times['Gradient Boosting'] = time.time() - start_time\n",
    "models['Gradient Boosting'] = ('unscaled', gb)\n",
    "\n",
    "print(f\"Training completed in {training_times['Gradient Boosting']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 11: Build Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (MLP)\n",
    "print(\"Training Neural Network (MLP) Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32, 16),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,  # L2 regularization\n",
    "    batch_size=32,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_times['Neural Network'] = time.time() - start_time\n",
    "models['Neural Network'] = ('scaled', nn)\n",
    "\n",
    "print(f\"Training completed in {training_times['Neural Network']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of trained models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<30} {'Training Time (s)':<20}\")\n",
    "print(\"-\"*60)\n",
    "for model_name, train_time in sorted(training_times.items(), key=lambda x: x[1]):\n",
    "    print(f\"{model_name:<30} {train_time:>15.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total models trained: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 12: Evaluate All Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, scaled=False, X_test_scaled=None):\n",
    "    \"\"\"Comprehensive model evaluation returning multiple metrics.\"\"\"\n",
    "    # Select appropriate test set\n",
    "    X_eval = X_test_scaled if scaled else X_test\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob),\n",
    "        'Avg Precision': average_precision_score(y_test, y_prob),\n",
    "        'Brier Score': brier_score_loss(y_test, y_prob),\n",
    "        'Log Loss': log_loss(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "all_results = []\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for model_name, (scale_type, model) in models.items():\n",
    "    scaled = (scale_type == 'scaled')\n",
    "    metrics, y_pred, y_prob = evaluate_model(\n",
    "        model, X_test, y_test, model_name, \n",
    "        scaled=scaled, \n",
    "        X_test_scaled=X_test_scaled\n",
    "    )\n",
    "    all_results.append(metrics)\n",
    "    predictions[model_name] = y_pred\n",
    "    probabilities[model_name] = y_prob\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.set_index('Model')\n",
    "results_df['Training Time (s)'] = results_df.index.map(training_times)\n",
    "\n",
    "print(\"Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results table\n",
    "print(\"=\"*100)\n",
    "print(\"MODEL TOURNAMENT RESULTS - PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "display_cols = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'Avg Precision', 'Training Time (s)']\n",
    "print(results_df[display_cols].round(4).to_string())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 13: Visualize Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=results_df.index,\n",
    "        y=results_df[metric],\n",
    "        marker_color=colors[i % len(colors)]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Tournament: Performance Comparison Across Metrics',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    xaxis_tickangle=-30\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison\n",
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, (model_name, y_prob) in enumerate(probabilities.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (AUC={auc:.3f})',\n",
    "        line=dict(color=colors[i % len(colors)], width=2)\n",
    "    ))\n",
    "\n",
    "# Add diagonal reference line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random Classifier',\n",
    "    line=dict(color='gray', dash='dash', width=1)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curve Comparison - All Models',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=600,\n",
    "    legend=dict(x=0.55, y=0.05),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    yaxis=dict(scaleanchor='x', scaleratio=1)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve Comparison\n",
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, (model_name, y_prob) in enumerate(probabilities.items()):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    ap = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=recall, y=precision,\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (AP={ap:.3f})',\n",
    "        line=dict(color=colors[i % len(colors)], width=2)\n",
    "    ))\n",
    "\n",
    "# Add baseline\n",
    "prevalence = y_test.mean()\n",
    "fig.add_hline(y=prevalence, line_dash='dash', line_color='gray',\n",
    "              annotation_text=f'Baseline (prevalence={prevalence:.2%})')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Precision-Recall Curve Comparison - All Models',\n",
    "    xaxis_title='Recall (True Positive Rate)',\n",
    "    yaxis_title='Precision (Positive Predictive Value)',\n",
    "    height=600,\n",
    "    legend=dict(x=0.55, y=0.95)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 14: Confusion Matrices for All Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "    if i < len(axes):\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                   xticklabels=['Enrolled', 'Departed'],\n",
    "                   yticklabels=['Enrolled', 'Departed'])\n",
    "        axes[i].set_title(f'{model_name}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "\n",
    "# Hide empty subplot\n",
    "if len(predictions) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Confusion Matrices: All Models', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 15: Feature Importance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from tree-based models\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': models['Random Forest'][1].feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': models['Gradient Boosting'][1].feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "# Logistic Regression coefficients (absolute values)\n",
    "lr_coefs = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': np.abs(models['Logistic Regression (L2)'][1].coef_[0])\n",
    "}).sort_values('Importance', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances side by side\n",
    "fig = make_subplots(rows=1, cols=3, \n",
    "                    subplot_titles=('Logistic Regression', 'Random Forest', 'Gradient Boosting'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=lr_coefs['Importance'], y=lr_coefs['Feature'], orientation='h', marker_color='steelblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=rf_importance['Importance'], y=rf_importance['Feature'], orientation='h', marker_color='forestgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=gb_importance['Importance'], y=gb_importance['Feature'], orientation='h', marker_color='coral'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance Comparison Across Models',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 16: Model Selection - Determine Tournament Winner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models by different criteria\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL TOURNAMENT - FINAL RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best by each metric\n",
    "print(\"\\nBest Model by Each Metric:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Best ROC-AUC: {results_df['ROC-AUC'].idxmax()} ({results_df['ROC-AUC'].max():.4f})\")\n",
    "print(f\"Best F1 Score: {results_df['F1 Score'].idxmax()} ({results_df['F1 Score'].max():.4f})\")\n",
    "print(f\"Best Recall: {results_df['Recall'].idxmax()} ({results_df['Recall'].max():.4f})\")\n",
    "print(f\"Best Precision: {results_df['Precision'].idxmax()} ({results_df['Precision'].max():.4f})\")\n",
    "print(f\"Fastest Training: {min(training_times, key=training_times.get)} ({min(training_times.values()):.3f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive ranking table\n",
    "ranking_df = results_df[['ROC-AUC', 'F1 Score', 'Recall', 'Precision']].copy()\n",
    "\n",
    "# Rank each metric (1 = best)\n",
    "for col in ranking_df.columns:\n",
    "    ranking_df[f'{col} Rank'] = ranking_df[col].rank(ascending=False).astype(int)\n",
    "\n",
    "# Calculate average rank\n",
    "rank_cols = [c for c in ranking_df.columns if 'Rank' in c]\n",
    "ranking_df['Average Rank'] = ranking_df[rank_cols].mean(axis=1)\n",
    "\n",
    "# Sort by average rank\n",
    "ranking_df = ranking_df.sort_values('Average Rank')\n",
    "\n",
    "print(\"\\nOVERALL MODEL RANKINGS (Lower Average Rank = Better):\")\n",
    "print(\"=\"*80)\n",
    "print(ranking_df[rank_cols + ['Average Rank']].to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "tournament_winner = ranking_df['Average Rank'].idxmin()\n",
    "print(f\"\\n*** TOURNAMENT WINNER: {tournament_winner} ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 17: Create Stakeholder Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for the report\n",
    "best_model_name = tournament_winner\n",
    "best_model_metrics = results_df.loc[best_model_name]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY: EARLY WARNING SYSTEM MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "OBJECTIVE:\n",
    "Develop a predictive model to identify students at risk of not returning \n",
    "for their third semester.\n",
    "\n",
    "APPROACH:\n",
    "Evaluated five different machine learning model families:\n",
    "- Regularized Logistic Regression (interpretable, coefficient-based)\n",
    "- Decision Tree (visual, rule-based)\n",
    "- Random Forest (ensemble of trees)\n",
    "- Gradient Boosting (advanced ensemble)\n",
    "- Neural Network (deep learning)\n",
    "\n",
    "DATASET:\n",
    "- Total students: {len(df):,}\n",
    "- Training set: {len(train_df):,} students\n",
    "- Testing set: {len(test_df):,} students\n",
    "- Departure rate: {df['DEPARTED'].mean():.1%}\n",
    "\n",
    "RECOMMENDED MODEL: {best_model_name}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "- ROC-AUC: {best_model_metrics['ROC-AUC']:.2%}\n",
    "- F1 Score: {best_model_metrics['F1 Score']:.2%}\n",
    "- Recall: {best_model_metrics['Recall']:.2%}\n",
    "- Precision: {best_model_metrics['Precision']:.2%}\n",
    "\n",
    "INTERPRETATION:\n",
    "- ROC-AUC of {best_model_metrics['ROC-AUC']:.1%} means the model can distinguish \n",
    "  between students who will depart and those who will stay {best_model_metrics['ROC-AUC']*100:.0f}% \n",
    "  of the time.\n",
    "- Recall of {best_model_metrics['Recall']:.1%} means the model correctly identifies \n",
    "  {best_model_metrics['Recall']*100:.0f}% of students who actually depart.\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 18: Produce a Comprehensive Report on Your Findings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable: Written Report for Stakeholders\n",
    "\n",
    "Using the analyses above, write a comprehensive report that addresses the following:\n",
    "\n",
    "1. **Model Comparison Summary**: Create a table comparing all five models across key metrics. Which model performed best overall? Were there trade-offs between different metrics?\n",
    "\n",
    "2. **Feature Importance Analysis**: What factors are most predictive of student departure? Do different models agree on the most important features? What does this tell us about the drivers of student attrition?\n",
    "\n",
    "3. **Model Selection Rationale**: Beyond raw performance, discuss why you would recommend a particular model. Consider:\n",
    "   - Interpretability (can advisors understand and explain predictions?)\n",
    "   - Training time and computational resources\n",
    "   - Maintenance burden\n",
    "   - Regulatory compliance (can decisions be audited?)\n",
    "\n",
    "4. **Implementation Recommendations**: How should the selected model be deployed? Consider:\n",
    "   - Threshold selection (when should a student be flagged as \"at-risk\"?)\n",
    "   - Intervention strategies based on risk scores\n",
    "   - Monitoring and retraining schedule\n",
    "\n",
    "5. **Limitations and Ethical Considerations**: What are the limitations of this analysis? What ethical concerns should be considered when deploying predictive models for student success?\n",
    "\n",
    "> **Rubric**: Your report should be 2-3 pages and include:\n",
    "> - Clear summary table of model performance\n",
    "> - At least 2 visualizations from your analysis\n",
    "> - Specific recommendations for implementation\n",
    "> - Discussion of limitations and ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Report (Write Below)\n",
    "\n",
    "*[Write your comprehensive stakeholder report here]*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
