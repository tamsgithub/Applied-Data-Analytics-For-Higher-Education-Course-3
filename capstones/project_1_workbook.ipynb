{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 1: Predicting Third Semester Retention - Model Tournament\n",
    "\n",
    "# Student Workbook\n",
    "\n",
    "Welcome to your Capstone Project workbook! This notebook provides a structured outline for your project. Your task is to fill in the missing code where indicated (replace `...` with appropriate code) to complete the steps and analysis. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A university's Student Success Center is seeking to implement an early warning system to identify students at risk of not returning for their third semester. Your task is to build and compare five different machine learning models to determine which approach will be most effective.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will be able to:\n",
    "1. Build and tune five different model families for classification\n",
    "2. Compare models across multiple performance metrics\n",
    "3. Consider trade-offs between accuracy, interpretability, and computational cost\n",
    "4. Communicate findings to non-technical stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Import Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models - All five families\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay,\n",
    "    brier_score_loss, log_loss, make_scorer\n",
    ")\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_location = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/data/'\n",
    "df = ...                    # Load the student academics data into a pandas DataFrame\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "...                         # Display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Quality - Handle Rare Categories and Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Rare Classes in RACE_ETHNICITY\n",
    "df['RACE_ETHNICITY'] = ...     # Replace rare categories with 'Other'\n",
    "print(\"Race/Ethnicity categories:\")\n",
    "print(df['RACE_ETHNICITY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Rare Classes in GENDER\n",
    "df = ...                       # Remove rows where gender is 'Nonbinary'\n",
    "df['GENDER'] = ...             # Clean and standardize the 'GENDER' column\n",
    "print(\"Gender categories:\")\n",
    "print(df['GENDER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop noninformative features\n",
    "df.drop(['SEM_1_STATUS', 'SEM_2_STATUS'], axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "...                            # Drop duplicate rows\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "missing_values_count = ...     # Count missing values per column\n",
    "total_rows = ...               # Get total number of rows\n",
    "columns_to_drop = ...          # Identify columns with >50% missing\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Create Target Variable and Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target: 1 = Departed (not enrolled), 0 = Enrolled\n",
    "df['DEPARTED'] = ...           # Create the binary target variable\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(df['DEPARTED'].value_counts())\n",
    "print(f\"\\nDeparture rate: {df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_df, test_df = ...        # Use train_test_split with 80/20 split and stratification\n",
    "\n",
    "print(f\"Training set: {train_df.shape[0]:,} students\")\n",
    "print(f\"Testing set: {test_df.shape[0]:,} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Handle Missing Values and Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df_train, df_test):\n",
    "    \"\"\"Impute missing values using train statistics to prevent data leakage.\"\"\"\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        if df_train[col].isnull().any():\n",
    "            if df_train[col].dtype in ['int64', 'float64']:\n",
    "                median_val = df_train[col].median()\n",
    "                df_train[col] = df_train[col].fillna(median_val)\n",
    "                df_test[col] = df_test[col].fillna(median_val)\n",
    "            else:\n",
    "                mode_val = df_train[col].mode()[0]\n",
    "                df_train[col] = df_train[col].fillna(mode_val)\n",
    "                df_test[col] = df_test[col].fillna(mode_val)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "train_df, test_df = ...        # Call the impute function\n",
    "print(\"Missing values imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create DFW rates and grade points\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # DFW Rate (proportion of attempted units not completed)\n",
    "    df['DFW_RATE_1'] = ...     # Calculate DFW rate for semester 1\n",
    "    df['DFW_RATE_2'] = ...     # Calculate DFW rate for semester 2\n",
    "    \n",
    "    # Grade Points\n",
    "    df['GRADE_POINTS_1'] = ... # Calculate grade points for semester 1\n",
    "    df['GRADE_POINTS_2'] = ... # Calculate grade points for semester 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_features(train_df)\n",
    "test_df = create_features(test_df)\n",
    "print(\"Features created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Define Feature Sets and Prepare Data for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories\n",
    "numeric_features = ...         # List of numeric feature column names\n",
    "\n",
    "categorical_features = ...     # List of categorical feature column names\n",
    "\n",
    "target = 'DEPARTED'\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "train_encoded = ...            # Use pd.get_dummies to encode training data\n",
    "test_encoded = ...             # Use pd.get_dummies to encode test data\n",
    "\n",
    "# Align columns between train and test\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Handle any remaining missing values\n",
    "train_encoded = train_encoded.fillna(train_encoded.median())\n",
    "test_encoded = test_encoded.fillna(test_encoded.median())\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = ...                  # Features for training\n",
    "y_train = ...                  # Target for training\n",
    "X_test = ...                   # Features for testing\n",
    "y_test = ...                   # Target for testing\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for models that require it\n",
    "scaler = ...                   # Create a StandardScaler\n",
    "X_train_scaled = ...           # Fit and transform training data\n",
    "X_test_scaled = ...            # Transform test data\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create at least one visualization comparing departure rates\n",
    "# across different student groups\n",
    "\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tournament: Building and Comparing All Five Model Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all models and results\n",
    "models = {}\n",
    "training_times = {}\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Build Regularized Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularized Logistic Regression\n",
    "print(\"Training L2 Regularized Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_l2 = ...                    # Create LogisticRegression with L2 penalty\n",
    "...                            # Fit on scaled training data\n",
    "\n",
    "training_times['Logistic Regression (L2)'] = time.time() - start_time\n",
    "models['Logistic Regression (L2)'] = ('scaled', lr_l2)\n",
    "\n",
    "print(f\"Training completed in {training_times['Logistic Regression (L2)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8: Build Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with tuned hyperparameters\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt = ...                       # Create DecisionTreeClassifier\n",
    "...                            # Fit on unscaled training data\n",
    "\n",
    "training_times['Decision Tree'] = time.time() - start_time\n",
    "models['Decision Tree'] = ('unscaled', dt)\n",
    "\n",
    "print(f\"Training completed in {training_times['Decision Tree']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 9: Build Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with tuned hyperparameters\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf = ...                       # Create RandomForestClassifier\n",
    "...                            # Fit on unscaled training data\n",
    "\n",
    "training_times['Random Forest'] = time.time() - start_time\n",
    "models['Random Forest'] = ('unscaled', rf)\n",
    "\n",
    "print(f\"Training completed in {training_times['Random Forest']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 10: Build Gradient Boosting Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "print(\"Training Gradient Boosting Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gb = ...                       # Create GradientBoostingClassifier\n",
    "...                            # Fit on unscaled training data\n",
    "\n",
    "training_times['Gradient Boosting'] = time.time() - start_time\n",
    "models['Gradient Boosting'] = ('unscaled', gb)\n",
    "\n",
    "print(f\"Training completed in {training_times['Gradient Boosting']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 11: Build Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (MLP)\n",
    "print(\"Training Neural Network (MLP) Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nn = ...                       # Create MLPClassifier\n",
    "...                            # Fit on scaled training data\n",
    "\n",
    "training_times['Neural Network'] = time.time() - start_time\n",
    "models['Neural Network'] = ('scaled', nn)\n",
    "\n",
    "print(f\"Training completed in {training_times['Neural Network']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of trained models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for model_name, train_time in sorted(training_times.items(), key=lambda x: x[1]):\n",
    "    print(f\"{model_name:<30} {train_time:>15.3f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 12: Evaluate All Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, scaled=False, X_test_scaled=None):\n",
    "    \"\"\"Comprehensive model evaluation returning multiple metrics.\"\"\"\n",
    "    # Select appropriate test set\n",
    "    X_eval = X_test_scaled if scaled else X_test\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = ...               # Get class predictions\n",
    "    y_prob = ...               # Get probability of positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': ...        # Calculate accuracy\n",
    "        'Precision': ...       # Calculate precision\n",
    "        'Recall': ...          # Calculate recall\n",
    "        'F1 Score': ...        # Calculate F1 score\n",
    "        'ROC-AUC': ...         # Calculate ROC-AUC\n",
    "        'Avg Precision': ...   # Calculate average precision\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "all_results = []\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for model_name, (scale_type, model) in models.items():\n",
    "    scaled = (scale_type == 'scaled')\n",
    "    metrics, y_pred, y_prob = evaluate_model(\n",
    "        model, X_test, y_test, model_name, \n",
    "        scaled=scaled, \n",
    "        X_test_scaled=X_test_scaled\n",
    "    )\n",
    "    all_results.append(metrics)\n",
    "    predictions[model_name] = y_pred\n",
    "    probabilities[model_name] = y_prob\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.set_index('Model')\n",
    "results_df['Training Time (s)'] = results_df.index.map(training_times)\n",
    "\n",
    "print(\"Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results table\n",
    "print(\"=\"*100)\n",
    "print(\"MODEL TOURNAMENT RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.round(4).to_string())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 13: Visualize Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart comparing model performance across metrics\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC Curve Comparison\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Precision-Recall Curve Comparison\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 14: Confusion Matrices for All Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 15: Feature Importance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare feature importances from different models\n",
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 16: Model Selection - Determine Tournament Winner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models and determine the winner\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL TOURNAMENT - FINAL RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Your code to rank models by different criteria\n",
    "...\n",
    "\n",
    "# Determine and print the tournament winner\n",
    "tournament_winner = ...        # Determine the best model\n",
    "print(f\"\\n*** TOURNAMENT WINNER: {tournament_winner} ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 17: Create Stakeholder Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for the report\n",
    "print(\"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY: EARLY WARNING SYSTEM MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Your code to generate the executive summary\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 18: Produce a Comprehensive Report on Your Findings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable: Written Report for Stakeholders\n",
    "\n",
    "Using the analyses above, write a comprehensive report that addresses the following:\n",
    "\n",
    "1. **Model Comparison Summary**: Create a table comparing all five models across key metrics. Which model performed best overall? Were there trade-offs between different metrics?\n",
    "\n",
    "2. **Feature Importance Analysis**: What factors are most predictive of student departure? Do different models agree on the most important features?\n",
    "\n",
    "3. **Model Selection Rationale**: Beyond raw performance, discuss why you would recommend a particular model. Consider interpretability, training time, and maintenance burden.\n",
    "\n",
    "4. **Implementation Recommendations**: How should the selected model be deployed? Consider threshold selection, intervention strategies, and monitoring.\n",
    "\n",
    "5. **Limitations and Ethical Considerations**: What are the limitations of this analysis? What ethical concerns should be considered?\n",
    "\n",
    "> **Rubric**: Your report should be 2-3 pages and include:\n",
    "> - Clear summary table of model performance\n",
    "> - At least 2 visualizations from your analysis\n",
    "> - Specific recommendations for implementation\n",
    "> - Discussion of limitations and ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Report (Write Below)\n",
    "\n",
    "*[Write your comprehensive stakeholder report here]*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
