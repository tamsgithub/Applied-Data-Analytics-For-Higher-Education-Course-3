{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 3: Early Warning System - Combining Multiple Models\n",
    "\n",
    "## Objective\n",
    "This notebook walks you through the UPAD cycle (Understand, Prepare, Analyze, Deploy) to:\n",
    "- Build an ensemble of models from different model families\n",
    "- Implement a stacking classifier to combine model predictions\n",
    "- Create risk scores and early warning tiers (High/Medium/Low risk)\n",
    "- Develop student intervention recommendations\n",
    "- Create deployment documentation for institutional use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Production-Ready Early Warning System\n",
    "\n",
    "Universities increasingly rely on early warning systems (EWS) to identify at-risk students before they leave. An effective EWS should:\n",
    "\n",
    "1. **Combine multiple signals**: Different models may capture different patterns in student behavior\n",
    "2. **Provide calibrated risk scores**: Not just binary predictions, but meaningful probability estimates\n",
    "3. **Enable tiered interventions**: Different risk levels warrant different intervention intensities\n",
    "4. **Be actionable**: Predictions should lead to specific intervention recommendations\n",
    "\n",
    "### Ensemble Learning Approach\n",
    "\n",
    "In this project, we will implement **stacking** (stacked generalization), which combines predictions from multiple base models using a meta-learner. The approach:\n",
    "\n",
    "1. **Base Models**: Train diverse models (logistic regression, random forest, gradient boosting, neural network)\n",
    "2. **Meta-Learner**: Train a second-level model on the base model predictions\n",
    "3. **Final Prediction**: The meta-learner makes the final prediction\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will be able to:\n",
    "1. Implement ensemble methods including stacking classifiers\n",
    "2. Create calibrated risk scores and probability estimates\n",
    "3. Design tiered intervention frameworks based on risk levels\n",
    "4. Prepare deployment documentation for institutional use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Import Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# Base Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_location = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/data/'\n",
    "df = pd.read_csv(f'{data_location}student_academics_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data cleaning\n",
    "df['RACE_ETHNICITY'] = df['RACE_ETHNICITY'].replace(\n",
    "    ['Unknown', 'Native Hawaiian or Other Pacific Islander', 'American Indian or Alaska Native'], \n",
    "    'Other'\n",
    ")\n",
    "\n",
    "df = df[df['GENDER'] != 'Nonbinary']\n",
    "df['GENDER'] = df['GENDER'].str.strip().str.capitalize()\n",
    "\n",
    "df.drop(['SEM_1_STATUS', 'SEM_2_STATUS'], axis=1, inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop columns with >50% missing\n",
    "missing_pct = df.isnull().sum() / len(df)\n",
    "cols_to_drop = missing_pct[missing_pct > 0.5].index.tolist()\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Create target\n",
    "df['DEPARTED'] = (df['SEM_3_STATUS'] != 'E').astype(int)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")\n",
    "print(f\"Departure rate: {df['DEPARTED'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # DFW Rates\n",
    "    df['DFW_RATE_1'] = ((df['UNITS_ATTEMPTED_1'] - df['UNITS_COMPLETED_1']).clip(lower=0) \n",
    "                        / df['UNITS_ATTEMPTED_1'].replace(0, 1))\n",
    "    df['DFW_RATE_2'] = ((df['UNITS_ATTEMPTED_2'] - df['UNITS_COMPLETED_2']).clip(lower=0) \n",
    "                        / df['UNITS_ATTEMPTED_2'].replace(0, 1))\n",
    "    \n",
    "    # Grade Points\n",
    "    df['GRADE_POINTS_1'] = df['UNITS_ATTEMPTED_1'] * df['GPA_1']\n",
    "    df['GRADE_POINTS_2'] = df['UNITS_ATTEMPTED_2'] * df['GPA_2']\n",
    "    \n",
    "    # GPA Trend (change from sem 1 to sem 2)\n",
    "    df['GPA_TREND'] = df['GPA_2'] - df['GPA_1']\n",
    "    \n",
    "    # Cumulative metrics\n",
    "    df['TOTAL_UNITS_ATTEMPTED'] = df['UNITS_ATTEMPTED_1'] + df['UNITS_ATTEMPTED_2']\n",
    "    df['TOTAL_UNITS_COMPLETED'] = df['UNITS_COMPLETED_1'] + df['UNITS_COMPLETED_2']\n",
    "    df['OVERALL_COMPLETION_RATE'] = df['TOTAL_UNITS_COMPLETED'] / df['TOTAL_UNITS_ATTEMPTED'].replace(0, 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_features(df)\n",
    "print(\"Features created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Prepare Data for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = [\n",
    "    'HS_GPA', 'HS_MATH_GPA', 'HS_ENGL_GPA',\n",
    "    'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
    "    'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2',\n",
    "    'DFW_UNITS_1', 'DFW_UNITS_2',\n",
    "    'GPA_1', 'GPA_2',\n",
    "    'DFW_RATE_1', 'DFW_RATE_2',\n",
    "    'GRADE_POINTS_1', 'GRADE_POINTS_2',\n",
    "    'GPA_TREND', 'OVERALL_COMPLETION_RATE'\n",
    "]\n",
    "\n",
    "categorical_features = ['RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'COLLEGE']\n",
    "\n",
    "target = 'DEPARTED'\n",
    "\n",
    "# Handle missing values\n",
    "for col in numeric_features:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test split (60/20/20)\n",
    "# First split off test set\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=RANDOM_STATE, stratify=df[target]\n",
    ")\n",
    "\n",
    "# Then split training into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.25, random_state=RANDOM_STATE, stratify=train_val_df[target]\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df):,} students ({len(train_df)/len(df):.1%})\")\n",
    "print(f\"Validation set: {len(val_df):,} students ({len(val_df)/len(df):.1%})\")\n",
    "print(f\"Test set: {len(test_df):,} students ({len(test_df)/len(df):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices\n",
    "all_features = numeric_features + categorical_features\n",
    "available_features = [f for f in all_features if f in train_df.columns]\n",
    "\n",
    "train_encoded = pd.get_dummies(train_df[available_features], columns=categorical_features, drop_first=True)\n",
    "val_encoded = pd.get_dummies(val_df[available_features], columns=categorical_features, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df[available_features], columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "train_encoded, val_encoded = train_encoded.align(val_encoded, join='left', axis=1, fill_value=0)\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Fill NaN\n",
    "train_encoded = train_encoded.fillna(0)\n",
    "val_encoded = val_encoded.fillna(0)\n",
    "test_encoded = test_encoded.fillna(0)\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_encoded\n",
    "y_train = train_df[target]\n",
    "X_val = val_encoded\n",
    "y_val = val_df[target]\n",
    "X_test = test_encoded\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Train Individual Base Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        penalty='l2', C=0.1, solver='lbfgs', max_iter=1000,\n",
    "        class_weight='balanced', random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=10, min_samples_split=10,\n",
    "        min_samples_leaf=5, class_weight='balanced',\n",
    "        n_jobs=-1, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=5,\n",
    "        min_samples_split=10, subsample=0.8,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32), activation='relu',\n",
    "        solver='adam', alpha=0.01, max_iter=300,\n",
    "        early_stopping=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "}\n",
    "\n",
    "# Models that need scaled data\n",
    "scaled_models = ['Logistic Regression', 'Neural Network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each base model\n",
    "base_results = []\n",
    "trained_models = {}\n",
    "base_predictions = {}\n",
    "base_probabilities = {}\n",
    "\n",
    "print(\"Training Base Models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Select appropriate data\n",
    "    if name in scaled_models:\n",
    "        X_tr, X_v = X_train_scaled, X_val_scaled\n",
    "    else:\n",
    "        X_tr, X_v = X_train, X_val\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_tr, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predict on validation\n",
    "    y_pred = model.predict(X_v)\n",
    "    y_prob = model.predict_proba(X_v)[:, 1]\n",
    "    \n",
    "    base_predictions[name] = y_pred\n",
    "    base_probabilities[name] = y_prob\n",
    "    \n",
    "    # Evaluate\n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_val, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_val, y_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_val, y_prob),\n",
    "        'Brier Score': brier_score_loss(y_val, y_prob)\n",
    "    }\n",
    "    base_results.append(results)\n",
    "    \n",
    "    print(f\"  ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
    "    print(f\"  F1 Score: {results['F1 Score']:.4f}\")\n",
    "\n",
    "base_results_df = pd.DataFrame(base_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nBase Model Results (Validation Set):\")\n",
    "print(base_results_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Implement Voting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Voting Classifier (soft voting uses probability averages)\n",
    "print(\"Building Voting Classifier...\")\n",
    "\n",
    "# For voting classifier, we need all models to use the same input\n",
    "# We'll use scaled data and models that can handle it\n",
    "voting_estimators = [\n",
    "    ('lr', LogisticRegression(penalty='l2', C=0.1, max_iter=1000,\n",
    "                              class_weight='balanced', random_state=RANDOM_STATE)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=10,\n",
    "                                   class_weight='balanced', n_jobs=-1,\n",
    "                                   random_state=RANDOM_STATE)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                       max_depth=5, random_state=RANDOM_STATE)),\n",
    "    ('nn', MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300,\n",
    "                         early_stopping=True, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft',  # Use probability averaging\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "voting_pred = voting_clf.predict(X_val_scaled)\n",
    "voting_prob = voting_clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(f\"\\nVoting Classifier (Validation):\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_val, voting_prob):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_val, voting_pred):.4f}\")\n",
    "print(f\"  Brier Score: {brier_score_loss(y_val, voting_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Implement Stacking Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stacking Classifier\n",
    "print(\"Building Stacking Classifier...\")\n",
    "\n",
    "# Base estimators for stacking\n",
    "stacking_estimators = [\n",
    "    ('lr', LogisticRegression(penalty='l2', C=0.1, max_iter=1000,\n",
    "                              class_weight='balanced', random_state=RANDOM_STATE)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10,\n",
    "                                   class_weight='balanced', n_jobs=-1,\n",
    "                                   random_state=RANDOM_STATE)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=80, learning_rate=0.1,\n",
    "                                       max_depth=5, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Meta-learner (final estimator)\n",
    "meta_learner = LogisticRegression(\n",
    "    penalty='l2', C=1.0, max_iter=1000, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=stacking_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,  # Cross-validation for generating meta-features\n",
    "    stack_method='predict_proba',  # Use probabilities as meta-features\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "stacking_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "stacking_pred = stacking_clf.predict(X_val_scaled)\n",
    "stacking_prob = stacking_clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(f\"\\nStacking Classifier (Validation):\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_val, stacking_prob):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_val, stacking_pred):.4f}\")\n",
    "print(f\"  Brier Score: {brier_score_loss(y_val, stacking_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8: Compare All Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ensemble results to comparison\n",
    "ensemble_results = [\n",
    "    {\n",
    "        'Model': 'Voting Ensemble',\n",
    "        'Accuracy': accuracy_score(y_val, voting_pred),\n",
    "        'Precision': precision_score(y_val, voting_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_val, voting_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_val, voting_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_val, voting_prob),\n",
    "        'Brier Score': brier_score_loss(y_val, voting_prob)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Stacking Ensemble',\n",
    "        'Accuracy': accuracy_score(y_val, stacking_pred),\n",
    "        'Precision': precision_score(y_val, stacking_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_val, stacking_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_val, stacking_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_val, stacking_prob),\n",
    "        'Brier Score': brier_score_loss(y_val, stacking_prob)\n",
    "    }\n",
    "]\n",
    "\n",
    "all_results_df = pd.concat([base_results_df, pd.DataFrame(ensemble_results)], ignore_index=True)\n",
    "all_results_df = all_results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPLETE MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\"*100)\n",
    "print(all_results_df.round(4).to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=all_results_df['Model'],\n",
    "        y=all_results_df[metric],\n",
    "        marker_color=colors[i % len(colors)]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Base Models vs Ensembles',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    xaxis_tickangle=-30,\n",
    "    legend=dict(orientation='h', y=1.1)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Risk Scores and Early Warning Tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 9: Select Best Model and Calibrate Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (using stacking ensemble as our production model)\n",
    "best_model = stacking_clf\n",
    "best_model_name = 'Stacking Ensemble'\n",
    "\n",
    "print(f\"Selected model for production: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probabilities using isotonic regression\n",
    "print(\"Calibrating probabilities...\")\n",
    "\n",
    "# We'll use CalibratedClassifierCV to improve probability estimates\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    best_model, method='isotonic', cv='prefit'\n",
    ")\n",
    "\n",
    "# Fit calibration on validation set\n",
    "calibrated_model.fit(X_val_scaled, y_val)\n",
    "\n",
    "# Get calibrated probabilities on test set\n",
    "calibrated_prob = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Compare calibration\n",
    "uncalibrated_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"\\nCalibration Results (Test Set):\")\n",
    "print(f\"Uncalibrated Brier Score: {brier_score_loss(y_test, uncalibrated_prob):.4f}\")\n",
    "print(f\"Calibrated Brier Score: {brier_score_loss(y_test, calibrated_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize calibration curves\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Calibration Curve', 'Probability Distribution'))\n",
    "\n",
    "# Calibration curve\n",
    "fraction_positives_uncal, mean_predicted_uncal = calibration_curve(y_test, uncalibrated_prob, n_bins=10)\n",
    "fraction_positives_cal, mean_predicted_cal = calibration_curve(y_test, calibrated_prob, n_bins=10)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Perfectly Calibrated',\n",
    "               line=dict(dash='dash', color='gray')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=mean_predicted_uncal, y=fraction_positives_uncal, mode='lines+markers',\n",
    "               name='Uncalibrated', line=dict(color='coral')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=mean_predicted_cal, y=fraction_positives_cal, mode='lines+markers',\n",
    "               name='Calibrated', line=dict(color='steelblue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Probability distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=calibrated_prob, nbinsx=50, name='Risk Score Distribution',\n",
    "                 marker_color='steelblue', opacity=0.7),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Model Calibration Analysis\")\n",
    "fig.update_xaxes(title_text=\"Mean Predicted Probability\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Fraction of Positives\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Risk Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 10: Define Risk Tiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define risk tiers based on probability thresholds\n",
    "def assign_risk_tier(probability):\n",
    "    \"\"\"\n",
    "    Assign risk tier based on departure probability.\n",
    "    Thresholds are set to balance intervention resources with identification accuracy.\n",
    "    \"\"\"\n",
    "    if probability >= 0.5:\n",
    "        return 'High Risk'\n",
    "    elif probability >= 0.25:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'Low Risk'\n",
    "\n",
    "# Apply to test set\n",
    "test_results = test_df.copy()\n",
    "test_results['Risk_Score'] = calibrated_prob\n",
    "test_results['Risk_Tier'] = [assign_risk_tier(p) for p in calibrated_prob]\n",
    "test_results['Predicted_Departure'] = (calibrated_prob >= 0.5).astype(int)\n",
    "\n",
    "# Summarize risk tier distribution\n",
    "tier_summary = test_results.groupby('Risk_Tier').agg({\n",
    "    'SID': 'count',\n",
    "    'DEPARTED': ['sum', 'mean'],\n",
    "    'Risk_Score': 'mean'\n",
    "}).round(4)\n",
    "tier_summary.columns = ['Count', 'Actual Departures', 'Departure Rate', 'Avg Risk Score']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RISK TIER SUMMARY (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(tier_summary.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk tiers\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=('Students by Risk Tier', 'Departure Rate by Risk Tier'))\n",
    "\n",
    "tier_counts = test_results['Risk_Tier'].value_counts()\n",
    "tier_colors = {'High Risk': 'firebrick', 'Medium Risk': 'orange', 'Low Risk': 'forestgreen'}\n",
    "\n",
    "# Count by tier\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['High Risk', 'Medium Risk', 'Low Risk'],\n",
    "           y=[tier_counts.get('High Risk', 0), tier_counts.get('Medium Risk', 0), tier_counts.get('Low Risk', 0)],\n",
    "           marker_color=['firebrick', 'orange', 'forestgreen']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Departure rate by tier\n",
    "tier_rates = test_results.groupby('Risk_Tier')['DEPARTED'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['High Risk', 'Medium Risk', 'Low Risk'],\n",
    "           y=[tier_rates.get('High Risk', 0), tier_rates.get('Medium Risk', 0), tier_rates.get('Low Risk', 0)],\n",
    "           marker_color=['firebrick', 'orange', 'forestgreen'],\n",
    "           text=[f\"{tier_rates.get(t, 0):.1%}\" for t in ['High Risk', 'Medium Risk', 'Low Risk']],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Early Warning System: Risk Tier Analysis\", showlegend=False)\n",
    "fig.update_yaxes(title_text=\"Number of Students\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Actual Departure Rate\", tickformat='.0%', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Develop Intervention Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 11: Define Intervention Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intervention recommendations based on risk tier and risk factors\n",
    "intervention_framework = {\n",
    "    'High Risk': {\n",
    "        'Priority': 'Immediate',\n",
    "        'Interventions': [\n",
    "            'Mandatory meeting with academic advisor within 1 week',\n",
    "            'Enrollment in academic success course',\n",
    "            'Peer mentoring program assignment',\n",
    "            'Weekly check-ins with success coach',\n",
    "            'Financial aid review and emergency funding eligibility check'\n",
    "        ],\n",
    "        'Frequency': 'Weekly monitoring',\n",
    "        'Escalation': 'Refer to Dean of Students if no engagement within 2 weeks'\n",
    "    },\n",
    "    'Medium Risk': {\n",
    "        'Priority': 'Proactive',\n",
    "        'Interventions': [\n",
    "            'Outreach email with support resources',\n",
    "            'Invitation to academic success workshop',\n",
    "            'Optional tutoring center referral',\n",
    "            'Study group matching',\n",
    "            'Career counseling appointment offer'\n",
    "        ],\n",
    "        'Frequency': 'Bi-weekly monitoring',\n",
    "        'Escalation': 'Upgrade to High Risk if GPA drops or engagement decreases'\n",
    "    },\n",
    "    'Low Risk': {\n",
    "        'Priority': 'Standard',\n",
    "        'Interventions': [\n",
    "            'Automated wellness check-in survey',\n",
    "            'General campus resource newsletter',\n",
    "            'Leadership and involvement opportunities'\n",
    "        ],\n",
    "        'Frequency': 'Monthly monitoring',\n",
    "        'Escalation': 'Upgrade if academic performance declines'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display intervention framework\n",
    "print(\"=\"*80)\n",
    "print(\"INTERVENTION FRAMEWORK\")\n",
    "print(\"=\"*80)\n",
    "for tier, details in intervention_framework.items():\n",
    "    print(f\"\\n{tier.upper()}\")\n",
    "    print(f\"Priority: {details['Priority']}\")\n",
    "    print(f\"Monitoring Frequency: {details['Frequency']}\")\n",
    "    print(f\"Interventions:\")\n",
    "    for intervention in details['Interventions']:\n",
    "        print(f\"  - {intervention}\")\n",
    "    print(f\"Escalation: {details['Escalation']}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 12: Generate Student-Level Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_student_recommendations(row, feature_importance=None):\n",
    "    \"\"\"\n",
    "    Generate personalized intervention recommendations based on student profile.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    risk_factors = []\n",
    "    \n",
    "    # Check academic performance\n",
    "    if row['GPA_2'] < 2.0:\n",
    "        risk_factors.append('Low GPA (below 2.0)')\n",
    "        recommendations.append('Mandatory academic advising')\n",
    "        recommendations.append('Tutoring center enrollment')\n",
    "    \n",
    "    if row['DFW_RATE_2'] > 0.3:\n",
    "        risk_factors.append('High DFW rate')\n",
    "        recommendations.append('Course load evaluation')\n",
    "        recommendations.append('Study skills workshop')\n",
    "    \n",
    "    if row['GPA_TREND'] < -0.5:\n",
    "        risk_factors.append('Declining GPA trend')\n",
    "        recommendations.append('Academic recovery program')\n",
    "    \n",
    "    # Check for first-gen specific support\n",
    "    if row['FIRST_GEN_STATUS'] == 'First Generation':\n",
    "        recommendations.append('First-gen student support services referral')\n",
    "    \n",
    "    # Check unit completion\n",
    "    if row['OVERALL_COMPLETION_RATE'] < 0.8:\n",
    "        risk_factors.append('Low unit completion rate')\n",
    "        recommendations.append('Withdrawal pattern review')\n",
    "    \n",
    "    return {\n",
    "        'risk_factors': risk_factors if risk_factors else ['No specific risk factors identified'],\n",
    "        'recommendations': recommendations if recommendations else ['Continue standard monitoring']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations for high-risk students\n",
    "high_risk_students = test_results[test_results['Risk_Tier'] == 'High Risk'].head(10)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE STUDENT INTERVENTION REPORTS (High Risk Students)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in high_risk_students.iterrows():\n",
    "    rec = generate_student_recommendations(row)\n",
    "    print(f\"\\nStudent ID: {row['SID']}\")\n",
    "    print(f\"Risk Score: {row['Risk_Score']:.2%}\")\n",
    "    print(f\"Risk Tier: {row['Risk_Tier']}\")\n",
    "    print(f\"Current GPA: {row['GPA_2']:.2f}\")\n",
    "    print(f\"\\nRisk Factors:\")\n",
    "    for factor in rec['risk_factors']:\n",
    "        print(f\"  - {factor}\")\n",
    "    print(f\"\\nRecommended Interventions:\")\n",
    "    for intervention in rec['recommendations']:\n",
    "        print(f\"  - {intervention}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create Deployment Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 13: Final Model Evaluation on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "final_pred = (calibrated_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL MODEL EVALUATION (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {best_model_name} (Calibrated)\")\n",
    "print(f\"Test Set Size: {len(y_test):,} students\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, final_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, final_pred):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, final_pred):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, final_pred):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, calibrated_prob):.4f}\")\n",
    "print(f\"  Brier Score: {brier_score_loss(y_test, calibrated_prob):.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_pred, target_names=['Enrolled', 'Departed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 14: Create Deployment Documentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate deployment documentation\n",
    "deployment_doc = {\n",
    "    'model_info': {\n",
    "        'name': 'Student Departure Early Warning System',\n",
    "        'version': '1.0',\n",
    "        'created_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'model_type': best_model_name,\n",
    "        'calibration': 'Isotonic Regression'\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_set_size': len(y_test),\n",
    "        'roc_auc': round(roc_auc_score(y_test, calibrated_prob), 4),\n",
    "        'f1_score': round(f1_score(y_test, final_pred), 4),\n",
    "        'precision': round(precision_score(y_test, final_pred), 4),\n",
    "        'recall': round(recall_score(y_test, final_pred), 4),\n",
    "        'brier_score': round(brier_score_loss(y_test, calibrated_prob), 4)\n",
    "    },\n",
    "    'features': {\n",
    "        'numeric': numeric_features,\n",
    "        'categorical': categorical_features,\n",
    "        'total_features': X_train.shape[1]\n",
    "    },\n",
    "    'risk_tiers': {\n",
    "        'high_risk': {'threshold': '>= 0.50', 'intervention': 'Immediate'},\n",
    "        'medium_risk': {'threshold': '0.25 - 0.49', 'intervention': 'Proactive'},\n",
    "        'low_risk': {'threshold': '< 0.25', 'intervention': 'Standard'}\n",
    "    },\n",
    "    'usage': {\n",
    "        'input_format': 'CSV with required feature columns',\n",
    "        'output_format': 'Risk score (0-1) and risk tier assignment',\n",
    "        'refresh_frequency': 'Weekly recommended'\n",
    "    },\n",
    "    'limitations': [\n",
    "        'Model trained on historical data; may not capture novel circumstances',\n",
    "        'Requires complete feature data for accurate predictions',\n",
    "        'Should be used as decision support, not automated decision-making',\n",
    "        'Regular retraining recommended (annually at minimum)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEPLOYMENT DOCUMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(deployment_doc, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 15: Produce Comprehensive Deployment Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable: Early Warning System Deployment Documentation\n",
    "\n",
    "Using the analyses above, write a comprehensive deployment report that addresses the following:\n",
    "\n",
    "1. **System Overview**: Describe the early warning system, including:\n",
    "   - Purpose and goals\n",
    "   - Model architecture (ensemble approach)\n",
    "   - Key performance metrics\n",
    "\n",
    "2. **Technical Specifications**: Document the technical details:\n",
    "   - Required input features and data formats\n",
    "   - Model components (base models, meta-learner)\n",
    "   - Calibration methodology\n",
    "   - Output interpretation\n",
    "\n",
    "3. **Risk Tier Framework**: Explain the intervention tiers:\n",
    "   - Threshold definitions and rationale\n",
    "   - Expected student distribution across tiers\n",
    "   - Validation of tier effectiveness\n",
    "\n",
    "4. **Intervention Recommendations**: Provide guidance on:\n",
    "   - Interventions for each risk tier\n",
    "   - Personalization based on risk factors\n",
    "   - Escalation procedures\n",
    "   - Success metrics for interventions\n",
    "\n",
    "5. **Operational Considerations**: Address implementation needs:\n",
    "   - Data pipeline requirements\n",
    "   - Refresh/update schedule\n",
    "   - Monitoring and alerting\n",
    "   - Staff training needs\n",
    "\n",
    "6. **Limitations and Ethical Guidelines**: Discuss:\n",
    "   - Known limitations of the model\n",
    "   - Appropriate and inappropriate uses\n",
    "   - Privacy considerations\n",
    "   - Bias monitoring and mitigation\n",
    "\n",
    "> **Rubric**: Your report should be 4-5 pages and include:\n",
    "> - Complete technical documentation\n",
    "> - Risk tier visualization and validation\n",
    "> - Intervention framework with specific recommendations\n",
    "> - Operational deployment plan\n",
    "> - Ethical guidelines and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Report (Write Below)\n",
    "\n",
    "*[Write your comprehensive deployment documentation here]*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
