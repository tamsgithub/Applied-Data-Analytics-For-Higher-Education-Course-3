{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Introduction to Gradient Boosting - Code Brief\n",
    "\n",
    "Condensed reference for gradient boosting concepts and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Sample Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate AdaBoost reweighting\n",
    "np.random.seed(42)\n",
    "n_samples = 10\n",
    "students = [f\"S{i+1}\" for i in range(n_samples)]\n",
    "\n",
    "# Initial uniform weights\n",
    "initial_weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Misclassified samples get higher weights\n",
    "misclassified_r1 = [2, 6, 7]\n",
    "weights_r1 = initial_weights.copy()\n",
    "weights_r1[misclassified_r1] *= 2.5\n",
    "weights_r1 = weights_r1 / weights_r1.sum()\n",
    "\n",
    "print(f\"Initial weights: {initial_weights}\")\n",
    "print(f\"After round 1: {weights_r1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting fits trees to residuals\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 50)\n",
    "y_true = np.sin(X) + 0.5 * X\n",
    "y = y_true + np.random.normal(0, 0.3, len(X))\n",
    "\n",
    "# Iteration 0: Start with mean\n",
    "pred_0 = np.full_like(y, y.mean())\n",
    "residuals_0 = y - pred_0\n",
    "\n",
    "# Iteration 1: Fit to residuals\n",
    "tree_1 = np.where(X < 5, residuals_0[X < 5].mean(), residuals_0[X >= 5].mean())\n",
    "pred_1 = pred_0 + 0.5 * tree_1  # learning_rate = 0.5\n",
    "residuals_1 = y - pred_1\n",
    "\n",
    "print(f\"MSE after iteration 0: {np.mean(residuals_0**2):.3f}\")\n",
    "print(f\"MSE after iteration 1: {np.mean(residuals_1**2):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of XGBoost, LightGBM, CatBoost\n",
    "comparison_data = {\n",
    "    'Feature': ['Release Year', 'Developer', 'Tree Growth', 'Categorical Handling', 'Speed'],\n",
    "    'XGBoost': ['2014', 'DMLC', 'Level-wise', 'Requires encoding', 'Fast'],\n",
    "    'LightGBM': ['2017', 'Microsoft', 'Leaf-wise', 'Native (integer)', 'Very Fast'],\n",
    "    'CatBoost': ['2017', 'Yandex', 'Symmetric', 'Native (strings OK)', 'Fast']\n",
    "}\n",
    "\n",
    "pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|:--------|:------------|\n",
    "| **Bagging** | Parallel training, reduces variance |\n",
    "| **Boosting** | Sequential training, reduces bias |\n",
    "| **AdaBoost** | Reweights samples |\n",
    "| **Gradient Boosting** | Fits residuals |\n",
    "| **XGBoost** | General-purpose, regularized |\n",
    "| **LightGBM** | Fast, memory-efficient |\n",
    "| **CatBoost** | Best for categorical features |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
