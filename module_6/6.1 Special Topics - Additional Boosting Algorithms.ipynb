{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Special Topics: Additional Boosting Algorithms\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nIn Module 2, we covered the three core tree-based models: Decision Trees, Random Forests, and XGBoost. This special topics module explores **additional algorithms** that are valuable to know but less commonly deployed in higher education settings.\n\nThis notebook covers:\n- **AdaBoost** \u2014 the original boosting algorithm\n- **LightGBM** \u2014 Microsoft's fast gradient boosting library\n- **CatBoost** \u2014 Yandex's categorical-feature-optimized boosting library\n\n### When to Use These Models\n\nThese models are worth exploring when:\n- You need faster training on very large datasets (LightGBM)\n- Your data has many categorical features (CatBoost)\n- You want to understand the historical foundations of boosting (AdaBoost)\n- You're benchmarking many algorithms for a research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AdaBoost: The Original Boosting Algorithm\n\n**AdaBoost** (Adaptive Boosting), introduced in 1996, works by:\n1. Giving all training samples equal weight\n2. Training a weak learner (typically a decision stump)\n3. Increasing weights of misclassified samples\n4. Training the next weak learner on reweighted data\n5. Combining all learners with weighted voting\n\n### Key Characteristics\n- Uses **decision stumps** (trees with depth=1) by default\n- Very sensitive to **outliers and noise**\n- Has been largely superseded by gradient boosting in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport numpy as np\n\n# Load data (same preparation as Module 2)\ntrain_df = pd.read_csv('../../data/training.csv')\ntest_df = pd.read_csv('../../data/testing.csv')\ntrain_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\ntest_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n\nnumeric_features = ['HS_GPA','HS_MATH_GPA','HS_ENGL_GPA','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2',\n    'UNITS_COMPLETED_1','UNITS_COMPLETED_2','DFW_UNITS_1','DFW_UNITS_2','GPA_1','GPA_2',\n    'DFW_RATE_1','DFW_RATE_2','GRADE_POINTS_1','GRADE_POINTS_2']\ncategorical_features = ['RACE_ETHNICITY','GENDER','FIRST_GEN_STATUS','COLLEGE']\n\ntrain_enc = pd.get_dummies(train_df[numeric_features + categorical_features],\n                           columns=categorical_features, drop_first=True)\ntest_enc = pd.get_dummies(test_df[numeric_features + categorical_features],\n                          columns=categorical_features, drop_first=True)\ntrain_enc, test_enc = train_enc.align(test_enc, join='left', axis=1, fill_value=0)\ntrain_enc = train_enc.fillna(train_enc.median())\ntest_enc = test_enc.fillna(test_enc.median())\n\nX_train, y_train = train_enc, train_df['DEPARTED']\nX_test, y_test = test_enc, test_df['DEPARTED']\n\n# AdaBoost\nada = AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=42)\nada.fit(X_train, y_train)\nada_prob = ada.predict_proba(X_test)[:, 1]\n\nprint(f\"AdaBoost ROC-AUC: {roc_auc_score(y_test, ada_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM: Fast Gradient Boosting\n\n**LightGBM** (Light Gradient Boosting Machine) by Microsoft focuses on speed and efficiency:\n\n- **Leaf-wise tree growth** (vs. level-wise in XGBoost) \u2014 grows the leaf with maximum loss reduction\n- **Histogram-based splitting** \u2014 bins continuous features for faster splits\n- **GOSS** (Gradient-based One-Side Sampling) \u2014 keeps high-gradient samples, samples low-gradient ones\n- **EFB** (Exclusive Feature Bundling) \u2014 bundles mutually exclusive features\n\n### When to Choose LightGBM over XGBoost\n- Very large datasets (millions of rows)\n- Training speed is critical\n- Memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n    from lightgbm import LGBMClassifier\n\n    lgbm = LGBMClassifier(\n        n_estimators=150, learning_rate=0.1, max_depth=5,\n        num_leaves=31, min_child_samples=20,\n        subsample=0.8, colsample_bytree=0.8,\n        class_weight='balanced', random_state=42, verbose=-1\n    )\n    lgbm.fit(X_train, y_train)\n    lgbm_prob = lgbm.predict_proba(X_test)[:, 1]\n\n    print(f\"LightGBM ROC-AUC: {roc_auc_score(y_test, lgbm_prob):.4f}\")\n\nexcept ImportError:\n    print(\"LightGBM not installed. Install with: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CatBoost: Native Categorical Feature Handling\n\n**CatBoost** (Categorical Boosting) by Yandex is designed for data with many categorical features:\n\n- **Native categorical encoding** \u2014 no need for one-hot encoding\n- **Ordered boosting** \u2014 prevents target leakage\n- **Symmetric trees** \u2014 faster prediction\n- **Good out-of-the-box performance** with minimal tuning\n\n### When to Choose CatBoost\n- Data with many categorical features\n- You want minimal preprocessing\n- Small datasets where overfitting is a concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n    from catboost import CatBoostClassifier\n\n    cat = CatBoostClassifier(\n        iterations=150, learning_rate=0.1, depth=5,\n        auto_class_weights='Balanced',\n        random_seed=42, verbose=0\n    )\n    cat.fit(X_train, y_train)\n    cat_prob = cat.predict_proba(X_test)[:, 1]\n\n    print(f\"CatBoost ROC-AUC: {roc_auc_score(y_test, cat_prob):.4f}\")\n\nexcept ImportError:\n    print(\"CatBoost not installed. Install with: pip install catboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of All Boosting Methods\n\n| Feature | AdaBoost | XGBoost | LightGBM | CatBoost |\n|:--------|:---------|:--------|:---------|:---------|\n| **Year** | 1996 | 2014 | 2017 | 2017 |\n| **Strategy** | Sample reweighting | Gradient on residuals | Gradient + GOSS | Ordered boosting |\n| **Speed** | Moderate | Fast | Very fast | Fast |\n| **Categorical handling** | Needs encoding | Needs encoding | Native (integer) | Native (string) |\n| **Missing data** | Needs imputation | Native | Native | Native |\n| **Best for** | Historical interest | General purpose | Large data, speed | Categorical features |\n| **In this course** | Special topic | Core model (Module 2) | Special topic | Special topic |\n\n### Bottom Line\n\nFor most higher education use cases, **XGBoost** (covered in Module 2) is sufficient. LightGBM and CatBoost offer marginal improvements in specific scenarios but add complexity to your toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n\n- **AdaBoost**: Historical importance, largely superseded by gradient boosting\n- **LightGBM**: Choose for very large datasets or when speed is critical\n- **CatBoost**: Choose when you have many categorical features and want easy setup\n- **For this course**: XGBoost is the recommended boosting algorithm for practical use\n\n**Proceed to:** `6.2 Special Topics: Neural Networks`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}