{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Special Topics: Introduction to Neural Networks\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\n**Neural networks** are a family of algorithms inspired by biological neurons. They are the foundation of deep learning and have achieved remarkable success in areas like image recognition, natural language processing, and game playing.\n\nFor **tabular data** (like student records), neural networks are generally **not the first choice**\u2014tree-based models typically perform as well or better with less effort. However, understanding neural networks is valuable for:\n\n1. Completeness of your ML knowledge\n2. Understanding deep learning hype and limitations\n3. Scenarios with very large datasets and complex interactions\n4. Transfer learning and embedding approaches\n\n### Learning Objectives\n\n1. Understand the basic structure of a neural network\n2. Build a simple neural network using scikit-learn's MLPClassifier\n3. Compare neural network performance to tree-based models\n4. Know when neural networks are and aren't appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Basics\n\nA neural network consists of:\n- **Input layer**: One neuron per feature\n- **Hidden layers**: Neurons that transform inputs through weighted sums and activation functions\n- **Output layer**: Produces the final prediction\n\nEach neuron computes: $output = activation(\\sum(weights \\times inputs) + bias)$\n\n### Common Activation Functions\n- **ReLU**: $f(x) = max(0, x)$ \u2014 most common for hidden layers\n- **Sigmoid**: $f(x) = 1/(1+e^{-x})$ \u2014 used for binary classification output\n- **Softmax**: Used for multi-class output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network using scikit-learn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load and prepare data\ntrain_df = pd.read_csv('../../data/training.csv')\ntest_df = pd.read_csv('../../data/testing.csv')\ntrain_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\ntest_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n\nnumeric_features = ['HS_GPA','HS_MATH_GPA','HS_ENGL_GPA','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2',\n    'UNITS_COMPLETED_1','UNITS_COMPLETED_2','DFW_UNITS_1','DFW_UNITS_2','GPA_1','GPA_2',\n    'DFW_RATE_1','DFW_RATE_2','GRADE_POINTS_1','GRADE_POINTS_2']\ncategorical_features = ['RACE_ETHNICITY','GENDER','FIRST_GEN_STATUS','COLLEGE']\n\ntrain_enc = pd.get_dummies(train_df[numeric_features + categorical_features],\n                           columns=categorical_features, drop_first=True)\ntest_enc = pd.get_dummies(test_df[numeric_features + categorical_features],\n                          columns=categorical_features, drop_first=True)\ntrain_enc, test_enc = train_enc.align(test_enc, join='left', axis=1, fill_value=0)\ntrain_enc = train_enc.fillna(train_enc.median())\ntest_enc = test_enc.fillna(test_enc.median())\n\nX_train, y_train = train_enc, train_df['DEPARTED']\nX_test, y_test = test_enc, test_df['DEPARTED']\n\n# IMPORTANT: Neural networks REQUIRE feature scaling (unlike tree-based models)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Build neural network \u2014 still follows instantiate/fit/predict!\nnn = MLPClassifier(\n    hidden_layer_sizes=(64, 32, 16),  # Three hidden layers\n    activation='relu',\n    solver='adam',\n    alpha=0.001,  # L2 regularization\n    batch_size=32,\n    learning_rate='adaptive',\n    max_iter=500,\n    early_stopping=True,\n    validation_fraction=0.1,\n    random_state=42\n)\n\nnn.fit(X_train_scaled, y_train)\nnn_prob = nn.predict_proba(X_test_scaled)[:, 1]\n\nprint(f\"Neural Network ROC-AUC: {roc_auc_score(y_test, nn_prob):.4f}\")\nprint(f\"\\nNote: Neural networks require scaled features!\")\nprint(f\"Note: They also take longer to train and have more hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Networks vs. Tree-Based Models for Tabular Data\n\n| Aspect | Neural Networks | Tree-Based Models |\n|:-------|:---------------|:-----------------|\n| **Preprocessing** | Requires scaling | None needed |\n| **Interpretability** | Black box | Moderate to high |\n| **Tuning effort** | High (many hyperparameters) | Moderate |\n| **Performance on tabular data** | Good, sometimes great | Typically best |\n| **Training speed** | Slow | Moderate to fast |\n| **Sample efficiency** | Needs more data | Works with less |\n\n### The Practical Reality\n\nFor most tabular data problems (including student analytics), tree-based models are preferred because:\n1. They perform at least as well\n2. They require less preprocessing\n3. They're easier to tune\n4. They provide feature importances natively\n\nNeural networks shine in other domains: images (CNNs), text (Transformers), sequences (RNNs/LSTMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n\n- Neural networks follow the same scikit-learn pattern: `MLPClassifier().fit().predict()`\n- They **require feature scaling** (unlike tree models)\n- For tabular student data, they rarely outperform Random Forest or XGBoost\n- They're worth understanding for completeness and for non-tabular applications\n- Deep learning (TensorFlow, PyTorch) extends beyond scikit-learn for larger-scale work\n\n### When to Use Neural Networks in Higher Education\n- Very large datasets (100K+ students)\n- Unstructured data (text from student surveys, images)\n- Research projects exploring cutting-edge methods\n- When combined with other data types (multimodal learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}