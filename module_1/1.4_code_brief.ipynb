{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Code Brief: Tune Regularization Hyperparameters\n",
    "\n",
    "Quick reference for hyperparameter tuning with GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_models = f'{root_filepath}course_3/models/'\n",
    "\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "X_train, y_train = df_training, df_training['SEM_3_STATUS']\n",
    "X_test, y_test = df_testing, df_testing['SEM_3_STATUS']\n",
    "\n",
    "l2_model = pickle.load(open(f'{course3_models}l2_ridge_logistic_model.pkl', 'rb'))\n",
    "l1_model = pickle.load(open(f'{course3_models}l1_lasso_logistic_model.pkl', 'rb'))\n",
    "elasticnet_model = pickle.load(open(f'{course3_models}elasticnet_logistic_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, pos_label='N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for L2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "l2_param_grid = {'classifier__C': C_values}\n",
    "\n",
    "l2_grid_search = GridSearchCV(l2_model, l2_param_grid, cv=cv, scoring=f1_scorer, return_train_score=True, n_jobs=-1)\n",
    "l2_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"L2 Best C: {l2_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"L2 Best CV F1: {l2_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for L1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_param_grid = {'classifier__C': C_values}\n",
    "\n",
    "l1_grid_search = GridSearchCV(l1_model, l1_param_grid, cv=cv, scoring=f1_scorer, return_train_score=True, n_jobs=-1)\n",
    "l1_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"L1 Best C: {l1_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"L1 Best CV F1: {l1_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for ElasticNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratio_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "elasticnet_param_grid = {\n",
    "    'classifier__C': C_values,\n",
    "    'classifier__l1_ratio': l1_ratio_values\n",
    "}\n",
    "\n",
    "elasticnet_grid_search = GridSearchCV(elasticnet_model, elasticnet_param_grid, cv=cv, scoring=f1_scorer, return_train_score=True, n_jobs=-1)\n",
    "elasticnet_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"ElasticNet Best C: {elasticnet_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"ElasticNet Best l1_ratio: {elasticnet_grid_search.best_params_['classifier__l1_ratio']}\")\n",
    "print(f\"ElasticNet Best CV F1: {elasticnet_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = [\n",
    "    {'Model': 'L2 (Ridge)', 'Best Score': l2_grid_search.best_score_},\n",
    "    {'Model': 'L1 (Lasso)', 'Best Score': l1_grid_search.best_score_},\n",
    "    {'Model': 'ElasticNet', 'Best Score': elasticnet_grid_search.best_score_}\n",
    "]\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "display(comparison_df)\n",
    "\n",
    "best_idx = comparison_df['Best Score'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "\n",
    "if best_model_name == 'L2 (Ridge)':\n",
    "    best_model = l2_grid_search.best_estimator_\n",
    "elif best_model_name == 'L1 (Lasso)':\n",
    "    best_model = l1_grid_search.best_estimator_\n",
    "else:\n",
    "    best_model = elasticnet_grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Test F1 Score: {f1_score(y_test, y_pred, pos_label='N'):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_pred, pos_label='N'):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_pred, pos_label='N'):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_filename = best_model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "save_path = f'{course3_models}{best_model_filename}_tuned.pkl'\n",
    "pickle.dump(best_model, open(save_path, 'wb'))\n",
    "print(f\"Saved: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
