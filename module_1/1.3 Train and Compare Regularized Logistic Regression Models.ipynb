{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 **Train** and Compare Regularized Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the pipeline with regularization.  \n",
    "### **2. Train the Model : Fit the model on the training data.**  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### 5. Improve the Model : Tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "\n",
    "<div style=\"overflow-x: auto;\">\n",
    "\n",
    "- [Introduction](#scrollTo=intro)\n",
    "- [1. Load Dependencies and Data](#scrollTo=section1)\n",
    "- [2. Load the Regularized Models](#scrollTo=section2)\n",
    "- [3. Train All Models](#scrollTo=section3)\n",
    "- [4. Compare Coefficients](#scrollTo=section4)\n",
    "  - [4.1 Coefficient Magnitudes](#scrollTo=section4_1)\n",
    "  - [4.2 Feature Selection with L1](#scrollTo=section4_2)\n",
    "- [5. Cross-Validation Comparison](#scrollTo=section5)\n",
    "- [6. Save Trained Models](#scrollTo=section6)\n",
    "- [7. Summary](#scrollTo=section7)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we train the three regularized logistic regression models we built in the previous notebook and compare their behavior:\n",
    "\n",
    "1. How do coefficients differ across regularization types?\n",
    "2. Which features does L1 regularization select?\n",
    "3. How do the models perform in cross-validation?\n",
    "\n",
    "We also compare to the unregularized baseline from Course 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_models = f'{root_filepath}course_3/models/'\n",
    "course2_models = f'{root_filepath}models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Regularized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regularized models from Course 3\n",
    "l2_model = pickle.load(open(f'{course3_models}l2_ridge_logistic_model.pkl', 'rb'))\n",
    "l1_model = pickle.load(open(f'{course3_models}l1_lasso_logistic_model.pkl', 'rb'))\n",
    "elasticnet_model = pickle.load(open(f'{course3_models}elasticnet_logistic_model.pkl', 'rb'))\n",
    "\n",
    "# Load baseline from Course 2 for comparison\n",
    "baseline_model = pickle.load(open(f'{course2_models}baseline_logistic_model.pkl', 'rb'))\n",
    "\n",
    "print(\"All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of all models for easy iteration\n",
    "models = {\n",
    "    'Baseline (No Penalty)': baseline_model,\n",
    "    'L2 (Ridge)': l2_model,\n",
    "    'L1 (Lasso)': l1_model,\n",
    "    'ElasticNet': elasticnet_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(\"Done!\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Coefficients\n",
    "\n",
    "One of the key differences between regularization types is how they affect coefficient values. Let's examine this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Coefficient Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from the preprocessor\n",
    "preprocessor = trained_models['Baseline (No Penalty)'].named_steps['preprocessing']\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Clean up feature names for display\n",
    "feature_names_clean = [name.split('__')[-1] for name in feature_names]\n",
    "print(f\"Number of features: {len(feature_names_clean)}\")\n",
    "print(f\"\\nFeatures: {feature_names_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from each model\n",
    "coef_data = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    classifier = model.named_steps['classifier']\n",
    "    coefficients = classifier.coef_[0]\n",
    "    \n",
    "    for feat, coef in zip(feature_names_clean, coefficients):\n",
    "        coef_data.append({\n",
    "            'Model': name,\n",
    "            'Feature': feat,\n",
    "            'Coefficient': coef\n",
    "        })\n",
    "\n",
    "coef_df = pd.DataFrame(coef_data)\n",
    "coef_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coefficient comparison visualization\n",
    "fig = px.bar(\n",
    "    coef_df, \n",
    "    x='Feature', \n",
    "    y='Coefficient', \n",
    "    color='Model',\n",
    "    barmode='group',\n",
    "    title='Coefficient Comparison Across Regularization Types',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficient magnitudes (L2 norm per model)\n",
    "coef_norms = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    classifier = model.named_steps['classifier']\n",
    "    coefficients = classifier.coef_[0]\n",
    "    l2_norm = np.sqrt(np.sum(coefficients**2))\n",
    "    l1_norm = np.sum(np.abs(coefficients))\n",
    "    coef_norms[name] = {'L2 Norm': l2_norm, 'L1 Norm': l1_norm}\n",
    "\n",
    "norms_df = pd.DataFrame(coef_norms).T\n",
    "print(\"Coefficient Norms (measure of model complexity):\")\n",
    "display(norms_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The **baseline model** (no regularization) typically has the largest coefficient norms\n",
    "- **L2 regularization** shrinks coefficients proportionally\n",
    "- **L1 regularization** shrinks some coefficients to exactly zero, reducing overall norm\n",
    "- **ElasticNet** combines both behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Selection with L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine which features L1 keeps vs zeros out\n",
    "l1_classifier = trained_models['L1 (Lasso)'].named_steps['classifier']\n",
    "l1_coefficients = l1_classifier.coef_[0]\n",
    "\n",
    "l1_selection = pd.DataFrame({\n",
    "    'Feature': feature_names_clean,\n",
    "    'Coefficient': l1_coefficients,\n",
    "    'Selected': l1_coefficients != 0\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"L1 (Lasso) Feature Selection:\")\n",
    "print(f\"Total features: {len(l1_selection)}\")\n",
    "print(f\"Selected (non-zero): {l1_selection['Selected'].sum()}\")\n",
    "print(f\"Eliminated (zero): {(~l1_selection['Selected']).sum()}\")\n",
    "print(\"\\nFeatures by importance:\")\n",
    "display(l1_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize L1 feature selection\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['green' if s else 'lightgray' for s in l1_selection['Selected']]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=l1_selection['Feature'],\n",
    "    y=l1_selection['Coefficient'],\n",
    "    marker_color=colors\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='L1 (Lasso) Coefficients: Green = Selected, Gray = Eliminated',\n",
    "    xaxis_tickangle=45,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Comparison\n",
    "\n",
    "Let's compare model performance using cross-validation, similar to what we did in Course 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation scorer for minority class\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label='N')\n",
    "precision_scorer = make_scorer(precision_score, pos_label='N')\n",
    "recall_scorer = make_scorer(recall_score, pos_label='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation for all models\n",
    "cv_results = []\n",
    "\n",
    "for name, model in models.items():  # Use original (untrained) models\n",
    "    print(f\"Cross-validating {name}...\", end=\" \")\n",
    "    \n",
    "    f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=f1_scorer)\n",
    "    precision_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=precision_scorer)\n",
    "    recall_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=recall_scorer)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'F1 Mean': f1_scores.mean(),\n",
    "        'F1 Std': f1_scores.std(),\n",
    "        'Precision Mean': precision_scores.mean(),\n",
    "        'Precision Std': precision_scores.std(),\n",
    "        'Recall Mean': recall_scores.mean(),\n",
    "        'Recall Std': recall_scores.std()\n",
    "    })\n",
    "    print(\"Done!\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nCross-Validation Results (Positive class: 'N' - Students who leave):\")\n",
    "display(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('F1 Score', 'Precision', 'Recall'))\n",
    "\n",
    "for i, metric in enumerate(['F1', 'Precision', 'Recall'], 1):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=cv_df['Model'],\n",
    "            y=cv_df[f'{metric} Mean'],\n",
    "            error_y=dict(type='data', array=cv_df[f'{metric} Std']),\n",
    "            name=metric,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text='Cross-Validation Results by Regularization Type'\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "for name, model in trained_models.items():\n",
    "    filename = name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filepath = f'{course3_models}{filename}_trained.pkl'\n",
    "    pickle.dump(model, open(filepath, 'wb'))\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we trained and compared regularized logistic regression models:\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Coefficient Shrinkage**: Regularization reduces coefficient magnitudes compared to the baseline\n",
    "\n",
    "2. **Feature Selection**: L1 (Lasso) naturally performs feature selection by zeroing out less important features\n",
    "\n",
    "3. **Cross-Validation Performance**: Regularized models often show similar or improved performance with better generalization\n",
    "\n",
    "### Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "summary = cv_df[['Model', 'F1 Mean', 'Precision Mean', 'Recall Mean']].copy()\n",
    "summary = summary.round(3)\n",
    "print(\"Cross-Validation Summary:\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Tune the regularization strength (`C` parameter) using GridSearch\n",
    "2. Find optimal hyperparameters for each regularization type\n",
    "3. Evaluate final models on the test set\n",
    "\n",
    "**Proceed to:** `1.4 Tune Regularization Hyperparameters`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
