{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.4 Tune Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Model Cycle: The 5 Key Steps\n",
    "\n",
    "### 1. Build the Model : Create the pipeline with regularization.  \n",
    "### 2. Train the Model : Fit the model on the training data.  \n",
    "### 3. Generate Predictions : Use the trained model to make predictions.  \n",
    "### 4. Evaluate the Model : Assess performance using evaluation metrics.  \n",
    "### **5. Improve the Model : Tune hyperparameters for optimal performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "\n",
    "<div style=\"overflow-x: auto;\">\n",
    "\n",
    "- [Introduction](#scrollTo=intro)\n",
    "- [1. Load Dependencies and Data](#scrollTo=section1)\n",
    "- [2. Understanding Hyperparameter Tuning](#scrollTo=section2)\n",
    "  - [2.1 What is GridSearchCV?](#scrollTo=section2_1)\n",
    "  - [2.2 Key Hyperparameters to Tune](#scrollTo=section2_2)\n",
    "- [3. Tune the C Parameter](#scrollTo=section3)\n",
    "  - [3.1 L2 (Ridge) Model Tuning](#scrollTo=section3_1)\n",
    "  - [3.2 L1 (Lasso) Model Tuning](#scrollTo=section3_2)\n",
    "- [4. Tune ElasticNet Parameters](#scrollTo=section4)\n",
    "  - [4.1 Tuning C and l1_ratio Together](#scrollTo=section4_1)\n",
    "  - [4.2 Visualize Hyperparameter Grid](#scrollTo=section4_2)\n",
    "- [5. Select the Best Model](#scrollTo=section5)\n",
    "  - [5.1 Compare All Tuned Models](#scrollTo=section5_1)\n",
    "  - [5.2 Final Model Selection](#scrollTo=section5_2)\n",
    "- [6. Evaluate on Test Set](#scrollTo=section6)\n",
    "  - [6.1 Final Performance Metrics](#scrollTo=section6_1)\n",
    "  - [6.2 Confusion Matrix](#scrollTo=section6_2)\n",
    "- [7. Visualize Hyperparameter Performance](#scrollTo=section7)\n",
    "- [8. Summary](#scrollTo=section8)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous notebooks, we built and trained regularized logistic regression models using default hyperparameter values (`C=1.0`, `l1_ratio=0.5`). However, these default values may not be optimal for our specific dataset.\n",
    "\n",
    "In this notebook, we use **GridSearchCV** to systematically search for the best hyperparameter values. This is a critical step in the machine learning workflow that can significantly improve model performance.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Use GridSearchCV to tune the regularization strength (`C` parameter)\n",
    "2. Tune the ElasticNet mixing parameter (`l1_ratio`)\n",
    "3. Visualize hyperparameter performance\n",
    "4. Select the best model based on cross-validation results\n",
    "5. Evaluate the final model on the held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    classification_report, confusion_matrix, make_scorer\n",
    ")\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "root_filepath = '/content/drive/MyDrive/projects/Applied-Data-Analytics-For-Higher-Education-Course-2/'\n",
    "data_filepath = f'{root_filepath}data/'\n",
    "course3_models = f'{root_filepath}course_3/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "df_training = pd.read_csv(f'{data_filepath}training.csv')\n",
    "df_testing = pd.read_csv(f'{data_filepath}testing.csv')\n",
    "\n",
    "X_train = df_training\n",
    "y_train = df_training['SEM_3_STATUS']\n",
    "\n",
    "X_test = df_testing\n",
    "y_test = df_testing['SEM_3_STATUS']\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]} samples\")\n",
    "print(f\"Test data: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regularized models from Course 3\n",
    "l2_model = pickle.load(open(f'{course3_models}l2_ridge_logistic_model.pkl', 'rb'))\n",
    "l1_model = pickle.load(open(f'{course3_models}l1_lasso_logistic_model.pkl', 'rb'))\n",
    "elasticnet_model = pickle.load(open(f'{course3_models}elasticnet_logistic_model.pkl', 'rb'))\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 2. Understanding Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 2.1 What is GridSearchCV?\n",
    "\n",
    "**GridSearchCV** is a method for systematically working through multiple combinations of hyperparameter values, cross-validating as it goes, to determine which combination gives the best performance.\n",
    "\n",
    "**How it works:**\n",
    "1. Define a grid of hyperparameter values to try\n",
    "2. For each combination, perform k-fold cross-validation\n",
    "3. Compute the average score across folds\n",
    "4. Select the combination with the best average score\n",
    "\n",
    "**Why use GridSearchCV instead of manual tuning?**\n",
    "- Exhaustively searches all combinations\n",
    "- Uses cross-validation to avoid overfitting to validation data\n",
    "- Automatically tracks results for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 2.2 Key Hyperparameters to Tune\n",
    "\n",
    "| Hyperparameter | Description | Range to Search |\n",
    "|:---------------|:------------|:----------------|\n",
    "| `C` | Inverse of regularization strength | 0.001 to 100 (log scale) |\n",
    "| `l1_ratio` | ElasticNet mixing (0=L2, 1=L1) | 0.0 to 1.0 |\n",
    "\n",
    "**Important Notes:**\n",
    "- **Small C** = Strong regularization (simpler model, may underfit)\n",
    "- **Large C** = Weak regularization (complex model, may overfit)\n",
    "- We search on a **logarithmic scale** because the effect of C is multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scorer - we optimize for F1 on the minority class ('N' = students who leave)\n",
    "f1_scorer = make_scorer(f1_score, pos_label='N')\n",
    "\n",
    "print(\"Cross-validation: 5-fold stratified\")\n",
    "print(\"Optimization metric: F1 score (positive class = 'N')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 3. Tune the C Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 3.1 L2 (Ridge) Model Tuning\n",
    "\n",
    "First, we tune the `C` parameter for the L2 regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define C values to search (logarithmic scale)\n",
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Create parameter grid for L2 model\n",
    "# Note: parameters inside pipeline use 'step__parameter' naming\n",
    "l2_param_grid = {\n",
    "    'classifier__C': C_values\n",
    "}\n",
    "\n",
    "print(f\"L2 Parameter Grid:\")\n",
    "print(f\"  C values: {C_values}\")\n",
    "print(f\"  Total combinations to try: {len(C_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV for L2 model\n",
    "print(\"Tuning L2 (Ridge) model...\")\n",
    "\n",
    "l2_grid_search = GridSearchCV(\n",
    "    estimator=l2_model,\n",
    "    param_grid=l2_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "l2_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest C value: {l2_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"Best CV F1 score: {l2_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all L2 results\n",
    "l2_results = pd.DataFrame(l2_grid_search.cv_results_)\n",
    "l2_results_display = l2_results[[\n",
    "    'param_classifier__C', 'mean_train_score', 'mean_test_score', 'std_test_score', 'rank_test_score'\n",
    "]].sort_values('rank_test_score')\n",
    "\n",
    "l2_results_display.columns = ['C', 'Train F1 (Mean)', 'CV F1 (Mean)', 'CV F1 (Std)', 'Rank']\n",
    "print(\"L2 (Ridge) GridSearch Results:\")\n",
    "display(l2_results_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 3.2 L1 (Lasso) Model Tuning\n",
    "\n",
    "Now we tune the L1 regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid for L1 model\n",
    "l1_param_grid = {\n",
    "    'classifier__C': C_values\n",
    "}\n",
    "\n",
    "print(f\"L1 Parameter Grid:\")\n",
    "print(f\"  C values: {C_values}\")\n",
    "print(f\"  Total combinations to try: {len(C_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV for L1 model\n",
    "print(\"Tuning L1 (Lasso) model...\")\n",
    "\n",
    "l1_grid_search = GridSearchCV(\n",
    "    estimator=l1_model,\n",
    "    param_grid=l1_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "l1_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest C value: {l1_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"Best CV F1 score: {l1_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all L1 results\n",
    "l1_results = pd.DataFrame(l1_grid_search.cv_results_)\n",
    "l1_results_display = l1_results[[\n",
    "    'param_classifier__C', 'mean_train_score', 'mean_test_score', 'std_test_score', 'rank_test_score'\n",
    "]].sort_values('rank_test_score')\n",
    "\n",
    "l1_results_display.columns = ['C', 'Train F1 (Mean)', 'CV F1 (Mean)', 'CV F1 (Std)', 'Rank']\n",
    "print(\"L1 (Lasso) GridSearch Results:\")\n",
    "display(l1_results_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 4. Tune ElasticNet Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 4.1 Tuning C and l1_ratio Together\n",
    "\n",
    "ElasticNet has two hyperparameters to tune:\n",
    "- **C**: Regularization strength\n",
    "- **l1_ratio**: Balance between L1 and L2 penalties\n",
    "  - `l1_ratio=0`: Pure L2 (Ridge)\n",
    "  - `l1_ratio=1`: Pure L1 (Lasso)\n",
    "  - `l1_ratio=0.5`: Equal mix\n",
    "\n",
    "We use a 2D grid search to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define l1_ratio values to search\n",
    "l1_ratio_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Create parameter grid for ElasticNet model\n",
    "elasticnet_param_grid = {\n",
    "    'classifier__C': C_values,\n",
    "    'classifier__l1_ratio': l1_ratio_values\n",
    "}\n",
    "\n",
    "print(f\"ElasticNet Parameter Grid:\")\n",
    "print(f\"  C values: {C_values}\")\n",
    "print(f\"  l1_ratio values: {l1_ratio_values}\")\n",
    "print(f\"  Total combinations to try: {len(C_values) * len(l1_ratio_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV for ElasticNet model\n",
    "print(\"Tuning ElasticNet model (this may take a moment)...\")\n",
    "\n",
    "elasticnet_grid_search = GridSearchCV(\n",
    "    estimator=elasticnet_model,\n",
    "    param_grid=elasticnet_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elasticnet_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters:\")\n",
    "print(f\"  C: {elasticnet_grid_search.best_params_['classifier__C']}\")\n",
    "print(f\"  l1_ratio: {elasticnet_grid_search.best_params_['classifier__l1_ratio']}\")\n",
    "print(f\"Best CV F1 score: {elasticnet_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all ElasticNet results\n",
    "elasticnet_results = pd.DataFrame(elasticnet_grid_search.cv_results_)\n",
    "elasticnet_results_display = elasticnet_results[[\n",
    "    'param_classifier__C', 'param_classifier__l1_ratio', \n",
    "    'mean_train_score', 'mean_test_score', 'std_test_score', 'rank_test_score'\n",
    "]].sort_values('rank_test_score').head(10)\n",
    "\n",
    "elasticnet_results_display.columns = ['C', 'l1_ratio', 'Train F1 (Mean)', 'CV F1 (Mean)', 'CV F1 (Std)', 'Rank']\n",
    "print(\"ElasticNet GridSearch Results (Top 10):\")\n",
    "display(elasticnet_results_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Hyperparameter Grid\n",
    "\n",
    "Let's visualize how different combinations of C and l1_ratio affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of ElasticNet CV scores\n",
    "elasticnet_pivot = elasticnet_results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_classifier__l1_ratio',\n",
    "    columns='param_classifier__C'\n",
    ")\n",
    "\n",
    "fig = px.imshow(\n",
    "    elasticnet_pivot,\n",
    "    labels=dict(x='C (Regularization Strength)', y='l1_ratio', color='CV F1 Score'),\n",
    "    x=[str(c) for c in C_values],\n",
    "    y=[str(r) for r in l1_ratio_values],\n",
    "    color_continuous_scale='Viridis',\n",
    "    aspect='auto',\n",
    "    title='ElasticNet Hyperparameter Grid: CV F1 Scores'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D surface plot of ElasticNet CV scores\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    z=elasticnet_pivot.values,\n",
    "    x=np.log10(C_values),  # Log scale for better visualization\n",
    "    y=l1_ratio_values,\n",
    "    colorscale='Viridis'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ElasticNet Hyperparameter Surface',\n",
    "    scene=dict(\n",
    "        xaxis_title='log10(C)',\n",
    "        yaxis_title='l1_ratio',\n",
    "        zaxis_title='CV F1 Score'\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 5. Select the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### 5.1 Compare All Tuned Models\n",
    "\n",
    "Now let's compare the best performing model from each regularization type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect best results from each model\n",
    "comparison_results = [\n",
    "    {\n",
    "        'Model': 'L2 (Ridge)',\n",
    "        'Best C': l2_grid_search.best_params_['classifier__C'],\n",
    "        'Best l1_ratio': 'N/A',\n",
    "        'CV F1 (Mean)': l2_grid_search.best_score_,\n",
    "        'CV F1 (Std)': l2_results.loc[l2_results['rank_test_score']==1, 'std_test_score'].values[0]\n",
    "    },\n",
    "    {\n",
    "        'Model': 'L1 (Lasso)',\n",
    "        'Best C': l1_grid_search.best_params_['classifier__C'],\n",
    "        'Best l1_ratio': 'N/A',\n",
    "        'CV F1 (Mean)': l1_grid_search.best_score_,\n",
    "        'CV F1 (Std)': l1_results.loc[l1_results['rank_test_score']==1, 'std_test_score'].values[0]\n",
    "    },\n",
    "    {\n",
    "        'Model': 'ElasticNet',\n",
    "        'Best C': elasticnet_grid_search.best_params_['classifier__C'],\n",
    "        'Best l1_ratio': elasticnet_grid_search.best_params_['classifier__l1_ratio'],\n",
    "        'CV F1 (Mean)': elasticnet_grid_search.best_score_,\n",
    "        'CV F1 (Std)': elasticnet_results.loc[elasticnet_results['rank_test_score']==1, 'std_test_score'].values[0]\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Best Tuned Models Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison_df['Model'],\n",
    "    y=comparison_df['CV F1 (Mean)'],\n",
    "    error_y=dict(type='data', array=comparison_df['CV F1 (Std)']),\n",
    "    marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparison of Tuned Models: Cross-Validation F1 Scores',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='CV F1 Score (Mean +/- Std)',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### 5.2 Final Model Selection\n",
    "\n",
    "We select the model with the highest cross-validation F1 score. In case of a tie, we prefer simpler models (fewer features or stronger regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best overall model\n",
    "best_idx = comparison_df['CV F1 (Mean)'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "\n",
    "# Get the best estimator from the corresponding grid search\n",
    "if best_model_name == 'L2 (Ridge)':\n",
    "    best_model = l2_grid_search.best_estimator_\n",
    "elif best_model_name == 'L1 (Lasso)':\n",
    "    best_model = l1_grid_search.best_estimator_\n",
    "else:\n",
    "    best_model = elasticnet_grid_search.best_estimator_\n",
    "\n",
    "print(f\"Selected Best Model: {best_model_name}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "print(f\"  C: {comparison_df.loc[best_idx, 'Best C']}\")\n",
    "print(f\"  l1_ratio: {comparison_df.loc[best_idx, 'Best l1_ratio']}\")\n",
    "print(f\"\\nCV F1 Score: {comparison_df.loc[best_idx, 'CV F1 (Mean)']:.4f} (+/- {comparison_df.loc[best_idx, 'CV F1 (Std)']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set\n",
    "\n",
    "Now that we've selected the best model using cross-validation, we evaluate it on the **held-out test set**. This gives us an unbiased estimate of how the model will perform on new data.\n",
    "\n",
    "**Important**: We only evaluate on the test set ONCE after all tuning is complete to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### 6.1 Final Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# Calculate metrics for minority class ('N')\n",
    "test_f1 = f1_score(y_test, y_pred, pos_label='N')\n",
    "test_precision = precision_score(y_test, y_pred, pos_label='N')\n",
    "test_recall = recall_score(y_test, y_pred, pos_label='N')\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"FINAL TEST SET EVALUATION: {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nMetrics for Minority Class ('N' = Students who leave):\")\n",
    "print(f\"  F1 Score:    {test_f1:.4f}\")\n",
    "print(f\"  Precision:   {test_precision:.4f}\")\n",
    "print(f\"  Recall:      {test_recall:.4f}\")\n",
    "print(f\"\\nOverall Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classification report\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['N', 'Y'])\n",
    "\n",
    "# Create annotated heatmap\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    labels=dict(x='Predicted', y='Actual', color='Count'),\n",
    "    x=['Left (N)', 'Stayed (Y)'],\n",
    "    y=['Left (N)', 'Stayed (Y)'],\n",
    "    color_continuous_scale='Blues',\n",
    "    text_auto=True,\n",
    "    aspect='equal',\n",
    "    title=f'Confusion Matrix: {best_model_name} on Test Set'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix Interpretation:\")\n",
    "print(f\"\\n  True Positives (correctly identified departures):  {tp}\")\n",
    "print(f\"  True Negatives (correctly identified stayers):     {tn}\")\n",
    "print(f\"  False Positives (predicted departure, but stayed): {fp}\")\n",
    "print(f\"  False Negatives (predicted stay, but left):        {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## 7. Visualize Hyperparameter Performance\n",
    "\n",
    "Let's create comprehensive visualizations to understand how hyperparameters affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare C parameter effect across all models\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('L2 (Ridge)', 'L1 (Lasso)', 'ElasticNet (best l1_ratio)'))\n",
    "\n",
    "# L2 Results\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.log10(l2_results['param_classifier__C'].astype(float)),\n",
    "        y=l2_results['mean_test_score'],\n",
    "        mode='lines+markers',\n",
    "        error_y=dict(type='data', array=l2_results['std_test_score']),\n",
    "        name='L2 CV Score',\n",
    "        line=dict(color='#1f77b4')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# L1 Results\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.log10(l1_results['param_classifier__C'].astype(float)),\n",
    "        y=l1_results['mean_test_score'],\n",
    "        mode='lines+markers',\n",
    "        error_y=dict(type='data', array=l1_results['std_test_score']),\n",
    "        name='L1 CV Score',\n",
    "        line=dict(color='#ff7f0e')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# ElasticNet Results (best l1_ratio for each C)\n",
    "elasticnet_best_per_c = elasticnet_results.loc[\n",
    "    elasticnet_results.groupby('param_classifier__C')['mean_test_score'].idxmax()\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.log10(elasticnet_best_per_c['param_classifier__C'].astype(float)),\n",
    "        y=elasticnet_best_per_c['mean_test_score'],\n",
    "        mode='lines+markers',\n",
    "        error_y=dict(type='data', array=elasticnet_best_per_c['std_test_score']),\n",
    "        name='ElasticNet CV Score',\n",
    "        line=dict(color='#2ca02c')\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='log10(C)')\n",
    "fig.update_yaxes(title_text='CV F1 Score', row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text='Effect of Regularization Strength (C) on Model Performance',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize l1_ratio effect for ElasticNet at best C\n",
    "best_c = elasticnet_grid_search.best_params_['classifier__C']\n",
    "elasticnet_at_best_c = elasticnet_results[\n",
    "    elasticnet_results['param_classifier__C'] == best_c\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=elasticnet_at_best_c['param_classifier__l1_ratio'].astype(float),\n",
    "    y=elasticnet_at_best_c['mean_test_score'],\n",
    "    mode='lines+markers',\n",
    "    error_y=dict(type='data', array=elasticnet_at_best_c['std_test_score']),\n",
    "    marker=dict(size=10),\n",
    "    line=dict(color='#2ca02c', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'ElasticNet: Effect of l1_ratio at C={best_c}',\n",
    "    xaxis_title='l1_ratio (0=Ridge, 1=Lasso)',\n",
    "    yaxis_title='CV F1 Score',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training vs Validation Score (checking for overfitting)\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('L2 (Ridge)', 'L1 (Lasso)', 'ElasticNet'))\n",
    "\n",
    "for i, (results, name, color) in enumerate([\n",
    "    (l2_results, 'L2', '#1f77b4'),\n",
    "    (l1_results, 'L1', '#ff7f0e'),\n",
    "    (elasticnet_best_per_c, 'ElasticNet', '#2ca02c')\n",
    "], 1):\n",
    "    x_vals = np.log10(results['param_classifier__C'].astype(float))\n",
    "    \n",
    "    # Training score\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_vals,\n",
    "            y=results['mean_train_score'],\n",
    "            mode='lines+markers',\n",
    "            name=f'{name} Train',\n",
    "            line=dict(color=color, dash='dash')\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "    \n",
    "    # Validation score\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_vals,\n",
    "            y=results['mean_test_score'],\n",
    "            mode='lines+markers',\n",
    "            name=f'{name} Validation',\n",
    "            line=dict(color=color)\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='log10(C)')\n",
    "fig.update_yaxes(title_text='F1 Score', row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text='Training vs Validation Scores: Checking for Overfitting',\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- When training and validation curves are close together, the model generalizes well\n",
    "- A large gap (training >> validation) indicates overfitting\n",
    "- Strong regularization (small C) prevents overfitting but may underfit\n",
    "- The optimal C balances bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best tuned model\n",
    "best_model_filename = best_model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "save_path = f'{course3_models}{best_model_filename}_tuned.pkl'\n",
    "\n",
    "pickle.dump(best_model, open(save_path, 'wb'))\n",
    "print(f\"Best model saved to: {save_path}\")\n",
    "\n",
    "# Also save all grid search results for reference\n",
    "grid_search_results = {\n",
    "    'l2_grid_search': l2_grid_search,\n",
    "    'l1_grid_search': l1_grid_search,\n",
    "    'elasticnet_grid_search': elasticnet_grid_search,\n",
    "    'best_model_name': best_model_name\n",
    "}\n",
    "\n",
    "pickle.dump(grid_search_results, open(f'{course3_models}grid_search_results.pkl', 'wb'))\n",
    "print(f\"Grid search results saved to: {course3_models}grid_search_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we tuned regularization hyperparameters using GridSearchCV and evaluated the best model on the test set.\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary_data = {\n",
    "    'Metric': ['Best Model', 'Best C', 'Best l1_ratio', 'CV F1 Score', 'Test F1 Score', 'Test Precision', 'Test Recall'],\n",
    "    'Value': [\n",
    "        best_model_name,\n",
    "        str(comparison_df.loc[best_idx, 'Best C']),\n",
    "        str(comparison_df.loc[best_idx, 'Best l1_ratio']),\n",
    "        f\"{comparison_df.loc[best_idx, 'CV F1 (Mean)']:.4f}\",\n",
    "        f\"{test_f1:.4f}\",\n",
    "        f\"{test_precision:.4f}\",\n",
    "        f\"{test_recall:.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Final Model Summary:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **GridSearchCV** systematically searches hyperparameter combinations using cross-validation\n",
    "\n",
    "2. **The C parameter** controls regularization strength:\n",
    "   - Small C = strong regularization (simpler model)\n",
    "   - Large C = weak regularization (more complex model)\n",
    "\n",
    "3. **ElasticNet's l1_ratio** controls the balance between L1 and L2 penalties\n",
    "\n",
    "4. **Visualizations** help understand how hyperparameters affect performance\n",
    "\n",
    "5. **Test set evaluation** should only be done once, after all tuning is complete\n",
    "\n",
    "### What We Learned About Our Data\n",
    "\n",
    "| Regularization Type | Best Performance | What This Suggests |\n",
    "|:--------------------|:-----------------|:------------------|\n",
    "| L2 (Ridge) | Good | All features contribute somewhat |\n",
    "| L1 (Lasso) | Good | Some feature selection beneficial |\n",
    "| ElasticNet | Best of both | Combination of shrinkage and selection helps |\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- The tuned model can be used to identify at-risk students early\n",
    "- Regularization helped improve generalization beyond the default settings\n",
    "- The model's interpretability is maintained while improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Congratulations on completing Module 1 of Course 3! You have learned how to:\n",
    "\n",
    "1. Understand regularization and its importance (Notebook 1.1)\n",
    "2. Build regularized logistic regression models (Notebook 1.2)\n",
    "3. Train and compare different regularization types (Notebook 1.3)\n",
    "4. Tune hyperparameters and select the best model (This notebook)\n",
    "\n",
    "In the next module, we will explore more advanced topics including:\n",
    "- Additional classification algorithms\n",
    "- Feature engineering techniques\n",
    "- Model deployment considerations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
