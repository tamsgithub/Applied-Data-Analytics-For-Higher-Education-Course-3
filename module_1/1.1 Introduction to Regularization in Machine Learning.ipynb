{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introduction to Regularization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "\n",
    "<div style=\"overflow-x: auto;\">\n",
    "\n",
    "- [Introduction](#scrollTo=intro)\n",
    "- [1. The Overfitting Problem](#scrollTo=section1)\n",
    "  - [1.1 What is Overfitting?](#scrollTo=section1_1)\n",
    "  - [1.2 Detecting Overfitting](#scrollTo=section1_2)\n",
    "- [2. Regularization: The Solution](#scrollTo=section2)\n",
    "  - [2.1 The Intuition Behind Regularization](#scrollTo=section2_1)\n",
    "  - [2.2 The Bias-Variance Trade-off](#scrollTo=section2_2)\n",
    "- [3. Types of Regularization](#scrollTo=section3)\n",
    "  - [3.1 L2 Regularization (Ridge)](#scrollTo=section3_1)\n",
    "  - [3.2 L1 Regularization (Lasso)](#scrollTo=section3_2)\n",
    "  - [3.3 ElasticNet: Combining L1 and L2](#scrollTo=section3_3)\n",
    "- [4. Regularization in the Context of Logistic Regression](#scrollTo=section4)\n",
    "- [5. Summary](#scrollTo=section5)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Course 2, we built logistic regression models to predict student departure. While these models performed well, we used `penalty=None`, meaning we did not apply any regularization. In this module, we explore **regularization**—a powerful technique that improves model performance by preventing overfitting and, in some cases, performing automatic feature selection.\n",
    "\n",
    "This notebook introduces the concepts behind regularization. In subsequent notebooks, we will implement regularized logistic regression models and compare their performance to our baseline.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain what overfitting is and why it's problematic\n",
    "2. Understand the bias-variance trade-off\n",
    "3. Describe how regularization addresses overfitting\n",
    "4. Differentiate between L1, L2, and ElasticNet regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Overfitting Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is Overfitting?\n",
    "\n",
    "**Overfitting** occurs when a model learns the training data too well—including its noise and random fluctuations—rather than learning the underlying patterns. An overfitted model performs excellently on training data but poorly on new, unseen data.\n",
    "\n",
    "Think of it like a student who memorizes every answer on practice tests but can't generalize to new exam questions. The student has \"overfit\" to the practice material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Generate synthetic data to illustrate overfitting\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 20)\n",
    "y_true = 2 * X + 1  # True relationship\n",
    "y_noisy = y_true + np.random.normal(0, 3, len(X))  # Add noise\n",
    "\n",
    "# Create three fits: underfitting, good fit, overfitting\n",
    "X_smooth = np.linspace(0, 10, 100)\n",
    "\n",
    "# Underfitting: constant (degree 0)\n",
    "y_underfit = np.ones_like(X_smooth) * np.mean(y_noisy)\n",
    "\n",
    "# Good fit: linear (degree 1)\n",
    "coeffs_good = np.polyfit(X, y_noisy, 1)\n",
    "y_goodfit = np.polyval(coeffs_good, X_smooth)\n",
    "\n",
    "# Overfitting: high-degree polynomial\n",
    "coeffs_overfit = np.polyfit(X, y_noisy, 15)\n",
    "y_overfit = np.polyval(coeffs_overfit, X_smooth)\n",
    "\n",
    "# Create subplot figure\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('Underfitting', 'Good Fit', 'Overfitting'))\n",
    "\n",
    "# Add data points to all subplots\n",
    "for col in [1, 2, 3]:\n",
    "    fig.add_trace(go.Scatter(x=X, y=y_noisy, mode='markers', name='Data', \n",
    "                             marker=dict(color='blue', size=8), showlegend=(col==1)), row=1, col=col)\n",
    "\n",
    "# Add fits\n",
    "fig.add_trace(go.Scatter(x=X_smooth, y=y_underfit, mode='lines', name='Model', \n",
    "                         line=dict(color='red', width=2), showlegend=False), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=X_smooth, y=y_goodfit, mode='lines', name='Model', \n",
    "                         line=dict(color='green', width=2), showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=X_smooth, y=y_overfit, mode='lines', name='Model', \n",
    "                         line=dict(color='red', width=2), showlegend=False), row=1, col=3)\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"The Fitting Spectrum: From Underfitting to Overfitting\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- **Underfitting (Left)**: The model is too simple. It doesn't capture the trend in the data.\n",
    "- **Good Fit (Center)**: The model captures the underlying pattern without being distracted by noise.\n",
    "- **Overfitting (Right)**: The model follows every data point, including the noise. It won't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Detecting Overfitting\n",
    "\n",
    "The classic sign of overfitting is a significant gap between training performance and validation/test performance:\n",
    "\n",
    "| Scenario | Training Performance | Test Performance | Diagnosis |\n",
    "|:---------|:--------------------|:-----------------|:----------|\n",
    "| Good fit | High | High | Model generalizes well |\n",
    "| Overfitting | Very High | Low | Model memorized training data |\n",
    "| Underfitting | Low | Low | Model is too simple |\n",
    "\n",
    "In Course 2, we used **cross-validation** to detect this. If a model performs much better on training folds than validation folds, it's likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Intuition Behind Regularization\n",
    "\n",
    "**Regularization** adds a penalty term to the loss function that discourages the model from having large coefficient values. The key insight is that complex, overfitted models tend to have large coefficients—they're working hard to fit every data point.\n",
    "\n",
    "By penalizing large coefficients, we encourage simpler models that generalize better.\n",
    "\n",
    "**Without regularization (Course 2):**\n",
    "$$\\text{Loss} = \\text{Prediction Error}$$\n",
    "\n",
    "**With regularization:**\n",
    "$$\\text{Loss} = \\text{Prediction Error} + \\lambda \\times \\text{Penalty on Coefficients}$$\n",
    "\n",
    "Where $\\lambda$ (lambda) controls the strength of regularization:\n",
    "- $\\lambda = 0$: No regularization (same as Course 2)\n",
    "- Small $\\lambda$: Weak regularization\n",
    "- Large $\\lambda$: Strong regularization (simpler model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Bias-Variance Trade-off\n",
    "\n",
    "Regularization is intimately connected to the **bias-variance trade-off**, a fundamental concept in machine learning.\n",
    "\n",
    "**Total Prediction Error = Bias² + Variance + Irreducible Noise**\n",
    "\n",
    "- **Bias**: Error from overly simplistic assumptions. High bias = underfitting.\n",
    "- **Variance**: Error from sensitivity to fluctuations in training data. High variance = overfitting.\n",
    "\n",
    "Regularization *increases bias* slightly but *decreases variance* substantially, often leading to better overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias-variance trade-off\n",
    "complexity = np.linspace(0.1, 3, 100)\n",
    "bias_squared = 1 / complexity\n",
    "variance = 0.3 * complexity ** 2\n",
    "total_error = bias_squared + variance + 0.1  # 0.1 is irreducible noise\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=complexity, y=bias_squared, mode='lines', name='Bias²', \n",
    "                         line=dict(color='blue', width=2)))\n",
    "fig.add_trace(go.Scatter(x=complexity, y=variance, mode='lines', name='Variance', \n",
    "                         line=dict(color='orange', width=2)))\n",
    "fig.add_trace(go.Scatter(x=complexity, y=total_error, mode='lines', name='Total Error', \n",
    "                         line=dict(color='red', width=3)))\n",
    "\n",
    "# Add vertical line at optimal point\n",
    "optimal_idx = np.argmin(total_error)\n",
    "fig.add_vline(x=complexity[optimal_idx], line_dash=\"dash\", line_color=\"green\", \n",
    "              annotation_text=\"Optimal Complexity\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='The Bias-Variance Trade-off',\n",
    "    xaxis_title='Model Complexity',\n",
    "    yaxis_title='Error',\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Regularization helps us stay near the optimal complexity by preventing the model from becoming too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 L2 Regularization (Ridge)\n",
    "\n",
    "**L2 regularization** adds a penalty equal to the *sum of squared coefficients*:\n",
    "\n",
    "$$\\text{Loss}_{L2} = \\text{Prediction Error} + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Shrinks all coefficients toward zero, but rarely to exactly zero\n",
    "- Keeps all features in the model\n",
    "- Works well when many features contribute to prediction\n",
    "- Handles multicollinearity by shrinking correlated feature coefficients\n",
    "\n",
    "**Analogy**: Ridge regression is like putting all coefficients on a diet—everyone loses weight, but no one disappears entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 L1 Regularization (Lasso)\n",
    "\n",
    "**L1 regularization** adds a penalty equal to the *sum of absolute values of coefficients*:\n",
    "\n",
    "$$\\text{Loss}_{L1} = \\text{Prediction Error} + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Can shrink coefficients to exactly zero, effectively removing features\n",
    "- Performs **automatic feature selection**\n",
    "- Produces sparse models (fewer active features)\n",
    "- Useful when you suspect only a few features matter\n",
    "\n",
    "**Analogy**: Lasso is like a reality TV show elimination—weak coefficients get voted off the island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ElasticNet: Combining L1 and L2\n",
    "\n",
    "**ElasticNet** combines both L1 and L2 penalties:\n",
    "\n",
    "$$\\text{Loss}_{ElasticNet} = \\text{Prediction Error} + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Or equivalently, controlled by a mixing parameter $\\alpha$:\n",
    "- $\\alpha = 1$: Pure L1 (Lasso)\n",
    "- $\\alpha = 0$: Pure L2 (Ridge)\n",
    "- $0 < \\alpha < 1$: Mixture of both\n",
    "\n",
    "**Characteristics:**\n",
    "- Gets the best of both worlds\n",
    "- Can select groups of correlated features (unlike pure Lasso)\n",
    "- Often performs better than either L1 or L2 alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Table\n",
    "\n",
    "| Property | L2 (Ridge) | L1 (Lasso) | ElasticNet |\n",
    "|:---------|:-----------|:-----------|:-----------|\n",
    "| Penalty Term | Sum of squared coefficients | Sum of absolute coefficients | Both |\n",
    "| Feature Selection | No | Yes (zeros out coefficients) | Yes |\n",
    "| Handles Multicollinearity | Well | May arbitrarily select one | Well |\n",
    "| Sparse Model | No | Yes | Depends on mixing |\n",
    "| Best When | Many small effects | Few large effects | Groups of correlated features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the geometry of L1 vs L2 regularization\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# L2 (circle)\n",
    "x_l2 = np.cos(theta)\n",
    "y_l2 = np.sin(theta)\n",
    "\n",
    "# L1 (diamond)\n",
    "t = np.linspace(0, 1, 25)\n",
    "x_l1 = np.concatenate([1-t, -t, -1+t, t])\n",
    "y_l1 = np.concatenate([t, 1-t, -t, -1+t])\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('L2 (Ridge) Constraint', 'L1 (Lasso) Constraint'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_l2, y=y_l2, mode='lines', fill='toself', \n",
    "                         fillcolor='rgba(0,100,200,0.3)', line=dict(color='blue', width=2),\n",
    "                         name='L2 Region'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_l1, y=y_l1, mode='lines', fill='toself', \n",
    "                         fillcolor='rgba(200,100,0,0.3)', line=dict(color='orange', width=2),\n",
    "                         name='L1 Region'), row=1, col=2)\n",
    "\n",
    "# Add axes\n",
    "for col in [1, 2]:\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=col)\n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=col)\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Geometry of Regularization Constraints\",\n",
    "                  showlegend=False)\n",
    "fig.update_xaxes(title_text=\"β₁\", range=[-1.5, 1.5])\n",
    "fig.update_yaxes(title_text=\"β₂\", range=[-1.5, 1.5])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geometric Interpretation:**\n",
    "\n",
    "The regularization constraint defines a region where coefficients must live. The diamond shape of L1 has corners on the axes—this is why L1 naturally produces zero coefficients (the solution often lands on a corner). The circular L2 constraint has no corners, so solutions rarely hit exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization in the Context of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we minimize the negative log-likelihood (or equivalently, maximize log-likelihood). With regularization, this becomes:\n",
    "\n",
    "$$\\text{Loss} = -\\log L(\\vec{\\beta}) + \\lambda \\cdot \\text{Penalty}(\\vec{\\beta})$$\n",
    "\n",
    "Where $\\log L(\\vec{\\beta})$ is the log-likelihood from our logistic model.\n",
    "\n",
    "**In scikit-learn's LogisticRegression:**\n",
    "\n",
    "```python\n",
    "# No regularization (Course 2)\n",
    "LogisticRegression(penalty=None)\n",
    "\n",
    "# L2 regularization (Ridge) - default in scikit-learn\n",
    "LogisticRegression(penalty='l2', C=1.0)\n",
    "\n",
    "# L1 regularization (Lasso)\n",
    "LogisticRegression(penalty='l1', C=1.0, solver='saga')\n",
    "\n",
    "# ElasticNet\n",
    "LogisticRegression(penalty='elasticnet', C=1.0, solver='saga', l1_ratio=0.5)\n",
    "```\n",
    "\n",
    "**Note on C parameter**: In scikit-learn, `C` is the *inverse* of regularization strength:\n",
    "- Large C = weak regularization (closer to unregularized model)\n",
    "- Small C = strong regularization (simpler model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Regularization Matters for Student Departure Prediction\n",
    "\n",
    "In our student departure prediction problem:\n",
    "\n",
    "1. **Feature Selection**: L1 regularization can help identify which features (GPA, DFW rate, demographics) are most predictive\n",
    "2. **Handling Correlations**: L2/ElasticNet handle correlated features (e.g., GPA_1 and GPA_2) gracefully\n",
    "3. **Improved Generalization**: Regularization often improves performance on new cohorts of students\n",
    "4. **Interpretability**: Sparser models from L1 are easier to explain to stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we covered:\n",
    "\n",
    "1. **Overfitting**: When models learn noise instead of signal, they fail to generalize\n",
    "\n",
    "2. **Bias-Variance Trade-off**: Total error = Bias² + Variance; regularization trades some bias for reduced variance\n",
    "\n",
    "3. **L2 (Ridge) Regularization**: Shrinks coefficients; keeps all features\n",
    "\n",
    "4. **L1 (Lasso) Regularization**: Can zero out coefficients; performs feature selection\n",
    "\n",
    "5. **ElasticNet**: Combines L1 and L2; often best of both worlds\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|:--------|:---------|\n",
    "| Regularization | Adds penalty to prevent large coefficients |\n",
    "| L2 (Ridge) | Shrinks but keeps all features |\n",
    "| L1 (Lasso) | Feature selection via zeroing coefficients |\n",
    "| ElasticNet | Combines both penalties |\n",
    "| C parameter | Inverse of regularization strength (small C = more regularization) |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will implement regularized logistic regression models on our student departure dataset and compare their performance to the unregularized baseline from Course 2.\n",
    "\n",
    "**Proceed to:** `1.2 Build a Regularized Logistic Regression Model`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
