{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Systematic Model Comparison: Logistic Regression vs. Random Forest vs. XGBoost\n\n## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nThroughout this course, we have built three families of classification models:\n\n1. **Module 1**: Regularized Logistic Regression (L1, L2, ElasticNet)\n2. **Module 2**: Tree-Based Models (Decision Tree, Random Forest, XGBoost)\n\nIn this module, we bring the **three most practical models** together for a head-to-head comparison:\n\n- **Regularized Logistic Regression** \u2014 the interpretable baseline\n- **Random Forest** \u2014 the robust ensemble\n- **XGBoost** \u2014 the performance champion\n\n### Why These Three?\n\nThese represent the models you will use most often in practice. They cover the full spectrum of the interpretability-performance trade-off, and each excels in different scenarios. Other models (LightGBM, CatBoost, Neural Networks) are covered in the Special Topics module.\n\n### Learning Objectives\n\n1. Train and evaluate all three models on the same dataset with optimized hyperparameters\n2. Compare performance across multiple metrics (AUC, F1, Precision, Recall)\n3. Analyze the interpretability vs. performance trade-off\n4. Apply model selection criteria appropriate for higher education\n5. Make a justified recommendation for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n    confusion_matrix, brier_score_loss, log_loss, classification_report)\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nimport time\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Load data\ntrain_df = pd.read_csv('../../data/training.csv')\ntest_df = pd.read_csv('../../data/testing.csv')\ntrain_df['DEPARTED'] = (train_df['SEM_3_STATUS'] != 'E').astype(int)\ntest_df['DEPARTED'] = (test_df['SEM_3_STATUS'] != 'E').astype(int)\n\nnumeric_features = ['HS_GPA','HS_MATH_GPA','HS_ENGL_GPA','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2',\n    'UNITS_COMPLETED_1','UNITS_COMPLETED_2','DFW_UNITS_1','DFW_UNITS_2','GPA_1','GPA_2',\n    'DFW_RATE_1','DFW_RATE_2','GRADE_POINTS_1','GRADE_POINTS_2']\ncategorical_features = ['RACE_ETHNICITY','GENDER','FIRST_GEN_STATUS','COLLEGE']\n\ntrain_enc = pd.get_dummies(train_df[numeric_features + categorical_features],\n                           columns=categorical_features, drop_first=True)\ntest_enc = pd.get_dummies(test_df[numeric_features + categorical_features],\n                          columns=categorical_features, drop_first=True)\ntrain_enc, test_enc = train_enc.align(test_enc, join='left', axis=1, fill_value=0)\ntrain_enc = train_enc.fillna(train_enc.median())\ntest_enc = test_enc.fillna(test_enc.median())\n\nX_train, y_train = train_enc, train_df['DEPARTED']\nX_test, y_test = test_enc, test_df['DEPARTED']\n\n# Scale for logistic regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Training: {X_train.shape[0]:,} | Testing: {X_test.shape[0]:,} | Features: {X_train.shape[1]}\")\nprint(f\"Departure rate: {y_train.mean():.2%} (train), {y_test.mean():.2%} (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 1: Regularized Logistic Regression (L2) ===\nstart = time.time()\nlr = LogisticRegression(penalty='l2', C=0.1, solver='lbfgs', max_iter=1000, random_state=RANDOM_STATE)\nlr.fit(X_train_scaled, y_train)\nlr_time = time.time() - start\nlr_prob = lr.predict_proba(X_test_scaled)[:, 1]\nlr_pred = lr.predict(X_test_scaled)\n\n# === MODEL 2: Random Forest ===\nstart = time.time()\nrf = RandomForestClassifier(n_estimators=200, max_depth=12, min_samples_leaf=5,\n    class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)\nrf.fit(X_train, y_train)\nrf_time = time.time() - start\nrf_prob = rf.predict_proba(X_test)[:, 1]\nrf_pred = rf.predict(X_test)\n\n# === MODEL 3: XGBoost ===\nstart = time.time()\nxgb = XGBClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, subsample=0.8,\n    colsample_bytree=0.8, scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n    use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)\nxgb.fit(X_train, y_train)\nxgb_time = time.time() - start\nxgb_prob = xgb.predict_proba(X_test)[:, 1]\nxgb_pred = xgb.predict(X_test)\n\nprint(\"All three models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive metrics\nresults = []\nfor name, pred, prob, t in [('Regularized Logistic', lr_pred, lr_prob, lr_time),\n                              ('Random Forest', rf_pred, rf_prob, rf_time),\n                              ('XGBoost', xgb_pred, xgb_prob, xgb_time)]:\n    results.append({\n        'Model': name,\n        'Accuracy': accuracy_score(y_test, pred),\n        'Precision': precision_score(y_test, pred),\n        'Recall': recall_score(y_test, pred),\n        'F1 Score': f1_score(y_test, pred),\n        'ROC-AUC': roc_auc_score(y_test, prob),\n        'Avg Precision': average_precision_score(y_test, prob),\n        'Brier Score': brier_score_loss(y_test, prob),\n        'Train Time (s)': t\n    })\n\nresults_df = pd.DataFrame(results).set_index('Model')\n\nprint(\"=\" * 90)\nprint(\"HEAD-TO-HEAD MODEL COMPARISON\")\nprint(\"=\" * 90)\nprint(results_df.round(4).to_string())\nprint(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance bar chart\nfig = go.Figure()\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\ncolors = ['#9b59b6', '#3498db', '#e74c3c']\n\nfor i, model in enumerate(results_df.index):\n    fig.add_trace(go.Bar(name=model, x=metrics,\n        y=[results_df.loc[model, m] for m in metrics], marker_color=colors[i]))\n\nfig.update_layout(title='Three-Way Model Comparison', barmode='group', height=450,\n    yaxis_title='Score')\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('ROC Curves', 'Precision-Recall Curves'))\n\nfor i, (name, prob) in enumerate([('Regularized Logistic', lr_prob),\n                                    ('Random Forest', rf_prob), ('XGBoost', xgb_prob)]):\n    # ROC\n    fpr, tpr, _ = roc_curve(y_test, prob)\n    auc = roc_auc_score(y_test, prob)\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n        name=f'{name} (AUC={auc:.3f})', line=dict(color=colors[i], width=2)), row=1, col=1)\n\n    # PR\n    prec, rec, _ = precision_recall_curve(y_test, prob)\n    ap = average_precision_score(y_test, prob)\n    fig.add_trace(go.Scatter(x=rec, y=prec, mode='lines',\n        name=f'{name} (AP={ap:.3f})', line=dict(color=colors[i], width=2, dash='dot'),\n        showlegend=False), row=1, col=2)\n\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', line=dict(color='gray', dash='dash'),\n    showlegend=False), row=1, col=1)\n\nfig.update_layout(height=450, title_text='ROC and Precision-Recall Curves')\nfig.update_xaxes(title_text='FPR', row=1, col=1)\nfig.update_yaxes(title_text='TPR', row=1, col=1)\nfig.update_xaxes(title_text='Recall', row=1, col=2)\nfig.update_yaxes(title_text='Precision', row=1, col=2)\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability vs. Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparing the three models\ncategories = ['ROC-AUC', 'Interpretability', 'Training Speed',\n              'Handles Non-linearity', 'Ease of Deployment', 'Stakeholder Trust']\n\nscores = {\n    'Regularized Logistic': [results_df.loc['Regularized Logistic', 'ROC-AUC']*10, 9, 10, 3, 10, 9],\n    'Random Forest': [results_df.loc['Random Forest', 'ROC-AUC']*10, 5, 6, 9, 7, 6],\n    'XGBoost': [results_df.loc['XGBoost', 'ROC-AUC']*10, 4, 7, 10, 6, 4]\n}\n\nfig = go.Figure()\nfor i, (name, vals) in enumerate(scores.items()):\n    fig.add_trace(go.Scatterpolar(r=vals + [vals[0]], theta=categories + [categories[0]],\n        name=name, line=dict(color=colors[i], width=2), fill='toself', opacity=0.3))\n\nfig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 10])),\n    title='Model Capabilities Radar Chart', height=550)\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations for Higher Education\n\n### Decision Framework\n\n| Your Priority | Recommended Model | Why |\n|:-------------|:-----------------|:----|\n| **Explainability to advisors** | Regularized Logistic Regression | Coefficients show clear factor contributions |\n| **Institutional reports & compliance** | Regularized Logistic Regression | Auditable, transparent |\n| **Reliable risk scoring** | Random Forest | Robust, handles messy data well |\n| **Maximum predictive accuracy** | XGBoost | Typically highest AUC and F1 |\n| **Limited IT resources** | Regularized Logistic Regression | Simplest to deploy and maintain |\n| **Research publications** | XGBoost | Best metrics for academic papers |\n\n### Practical Recommendation\n\nFor most higher education institutions, we recommend a **two-model approach**:\n\n1. **Regularized Logistic Regression** for stakeholder-facing outputs (reports, advisor dashboards, compliance)\n2. **Random Forest or XGBoost** for backend risk scoring where performance matters most\n\nThis gives you the best of both worlds: interpretability where it's needed, and performance where it counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n\n### Key Findings\n\n1. All three models are strong performers on student departure prediction\n2. The performance gap between models is often smaller than expected\u2014interpretability and deployment considerations often matter more\n3. Regularized Logistic Regression remains highly competitive while being fully transparent\n4. XGBoost typically edges ahead on raw metrics but requires more infrastructure\n5. Random Forest provides an excellent middle ground\n\n### Next Module\n\n**Proceed to:** `Module 4: Unsupervised Learning` (coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}