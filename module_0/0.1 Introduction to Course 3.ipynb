{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 - Introduction to Course 3: Advanced Machine Learning for Higher Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to *Applied Data Analytics and Machine Learning - Course 3*, an advanced course developed by the **Institutional Research and Analytics Department, CSULB**.\n",
    "\n",
    "This course builds upon the foundational knowledge you gained in Course 2, where you learned to build, train, evaluate, and deploy classification and regression models using Logistic Regression and Linear Regression in scikit-learn. In Course 3, we expand your machine learning toolkit with more powerful and sophisticated algorithms.\n",
    "\n",
    "\n",
    "\n",
    "### Objective\n",
    "To understand the overall course design, learning objectives, and progression through advanced machine learning techniques for higher education analytics.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "This course assumes you have completed Course 2 and are comfortable with:\n",
    "- The machine learning cycle (Build, Train, Predict, Evaluate, Improve)\n",
    "- Data preprocessing and feature engineering\n",
    "- Model evaluation metrics (Accuracy, Precision, Recall, F1-Score, AUC)\n",
    "- Cross-validation and model selection\n",
    "- Python and scikit-learn fundamentals\n",
    "\n",
    "\n",
    ">Each notebook in this course is self-contained yet builds upon concepts from earlier modules. Completing them in sequence ensures conceptual continuity and practical readiness for subsequent modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Welcome and Course Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Welcome back to your machine learning journey with the *Applied Data Analytics and Machine Learning* series from the **Institutional Research and Analytics Department, CSULB**. \n",
    "\n",
    "In Course 2, you mastered the fundamentals of supervised learning by building logistic regression and linear regression models to predict student outcomes. You learned to navigate the complete machine learning cycle—from data preparation through model deployment.\n",
    "\n",
    "Course 3 takes you deeper into the world of machine learning by introducing more sophisticated algorithms that often outperform the baseline models you've already built. These advanced techniques are widely used in both industry and research settings to tackle complex prediction problems.\n",
    "\n",
    "\n",
    "\n",
    "### Why Advanced Models Matter\n",
    "\n",
    "While logistic regression provides an excellent baseline and is highly interpretable, more complex models can:\n",
    "\n",
    "- **Capture non-linear relationships** that linear models cannot detect\n",
    "- **Handle feature interactions** automatically without manual feature engineering\n",
    "- **Improve predictive accuracy** on complex real-world problems\n",
    "- **Reduce overfitting** through regularization and ensemble techniques\n",
    "\n",
    "\n",
    "\n",
    "### Our Continued Focus: Student Success\n",
    "\n",
    "We will continue working with student departure prediction—the same problem you addressed in Course 2. This continuity allows you to:\n",
    "\n",
    "1. **Directly compare** how different algorithms perform on the same problem\n",
    "2. **Build intuition** for when to choose one model over another\n",
    "3. **Develop expertise** in a domain-specific application of machine learning\n",
    "\n",
    "\n",
    "\n",
    "### Learning Philosophy\n",
    "\n",
    "> *Understand the theory, master the practice, compare the results.*\n",
    "\n",
    "This course maintains the practice-first approach from Course 2 while introducing more theoretical foundations where necessary. You will not only learn how to implement advanced algorithms but also understand *why* they work and *when* to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What You Will Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nThis course focuses on **practical skills** for building and deploying machine learning models in higher education. We emphasize the common scikit-learn workflow—`instantiate → fit → predict`—across all model families, so you can confidently switch between algorithms.\n\n\n\n### Learning Outcomes\n\nUpon completing this course, learners will be able to:\n\n1. **Apply regularization techniques** (L1, L2, ElasticNet) to prevent overfitting and perform feature selection\n2. **Build tree-based models** (Decision Trees, Random Forests, XGBoost) using a consistent scikit-learn workflow\n3. **Compare and select models** systematically—Regularized Logistic Regression vs. Random Forest vs. XGBoost\n4. **Apply unsupervised learning** techniques for student segmentation and pattern discovery\n5. **Use AI-assisted coding tools** (Codex, Antigravity) to accelerate model development\n6. **Explore special topics** including additional boosting methods and neural networks\n7. **Apply advanced techniques** to higher education prediction problems through capstone projects\n\n\n\n### Key Concepts Covered\n\n| Concept | Description |\n|:--------|:------------|\n| **Regularization** | Techniques to prevent overfitting by adding penalty terms to the loss function |\n| **Decision Trees** | Non-linear models that learn hierarchical decision rules from data |\n| **Ensemble Learning** | Combining multiple models to improve predictive performance |\n| **Bagging** | Bootstrap aggregating to reduce variance (Random Forest) |\n| **Boosting** | Sequential model building where each tree corrects predecessor errors (XGBoost) |\n| **The scikit-learn Pattern** | Consistent API: instantiate → fit → predict across all models |\n| **AI-Assisted Coding** | Using tools like Codex and Antigravity to write ML code from natural language |\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Course Structure Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nThe course is organized into **seven instructional modules** followed by **capstone projects** that integrate all learned techniques.\n\n| Module | Title | Primary Focus |\n|:-------|:------|:--------------|\n| **0** | Course Introduction | Orientation and overview of advanced ML techniques |\n| **1** | Regularized Logistic Regression | L1 (Lasso), L2 (Ridge), and ElasticNet regularization |\n| **2** | Tree-Based Models | Decision Trees, Random Forests, and XGBoost — unified practical workflow |\n| **3** | Model Comparison & Selection | Systematic comparison: Reg. Logistic vs. Random Forest vs. XGBoost |\n| **4** | Unsupervised Learning | Clustering, dimensionality reduction, student segmentation |\n| **5** | AI-Assisted Coding | Using Codex, Antigravity, and other tools to vibecode ML workflows |\n| **6** | Special Topics | Additional boosting algorithms (LightGBM, CatBoost, AdaBoost) and Neural Networks |\n| **Capstones** | Applied Projects | End-to-end projects using multiple techniques |\n\n\n\n### Module Progression\n\nThe course follows a practical progression focused on the three core models:\n\n```\nModule 1: Regularized Logistic Regression (Linear + Penalty)\n     ↓\nModule 2: Tree-Based Models (Decision Tree → Random Forest → XGBoost)\n     ↓         Common pattern: instantiate → fit → predict\nModule 3: Model Comparison (pick the right model for your use case)\n     ↓\nModule 4: Unsupervised Learning (discover hidden patterns)\n     ↓\nModule 5: AI-Assisted Coding (accelerate your workflow)\n     ↓\nModule 6: Special Topics (explore additional algorithms)\n```\n\nThis progression allows you to:\n- Master the most practical and widely-used models first\n- Learn the common scikit-learn API that works across all models\n- Compare models systematically before exploring additional methods\n- Leverage AI tools to accelerate your work\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Evolution from Basic to Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nUnderstanding *why* we need advanced models helps motivate the techniques covered in this course.\n\n\n\n### Limitations of Basic Logistic Regression\n\nIn Course 2, you built logistic regression models that performed well but had certain limitations:\n\n1. **Linear decision boundaries**: Cannot capture complex, non-linear relationships\n2. **No automatic feature selection**: All features contribute to the prediction\n3. **Sensitivity to multicollinearity**: Correlated features can destabilize coefficient estimates\n4. **No interaction detection**: Manual feature engineering required for interactions\n\n\n\n### How Advanced Models Address These Limitations\n\n| Limitation | Solution | Covered In |\n|:-----------|:---------|:-----------|\n| Overfitting | Regularization (L1, L2, ElasticNet) | Module 1 |\n| Feature Selection | Lasso (L1) regularization | Module 1 |\n| Non-linearity | Decision Trees, Random Forest, XGBoost | Module 2 |\n| Interactions | Tree-based models (automatic) | Module 2 |\n| Model Selection | Systematic comparison framework | Module 3 |\n| Hidden Patterns | Unsupervised learning | Module 4 |\n| Coding Speed | AI-assisted coding tools | Module 5 |\n| Additional Methods | LightGBM, CatBoost, Neural Networks | Module 6 |\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: Model Complexity Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a visual representation of model complexity\n",
    "models = ['Logistic\\nRegression', 'Regularized\\nLogistic', 'Decision\\nTree', \n",
    "          'Random\\nForest', 'Gradient\\nBoosting', 'Neural\\nNetwork']\n",
    "complexity = [1, 2, 3, 4, 5, 6]\n",
    "interpretability = [6, 5, 5, 3, 2, 1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Complexity',\n",
    "    x=models,\n",
    "    y=complexity,\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Interpretability',\n",
    "    x=models,\n",
    "    y=interpretability,\n",
    "    marker_color='coral'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Complexity vs. Interpretability Trade-off',\n",
    "    barmode='group',\n",
    "    yaxis_title='Score (1-6)',\n",
    "    xaxis_title='Model Type',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above illustrates a fundamental trade-off in machine learning: as models become more complex and potentially more accurate, they often become less interpretable. Throughout this course, you will learn when to prioritize accuracy versus interpretability based on your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequently Asked Questions (FAQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n#### 1. Do I need to complete Course 2 before starting Course 3?\n\n>Yes, Course 3 assumes familiarity with concepts covered in Course 2, including the ML cycle, data preprocessing, model evaluation metrics, and scikit-learn fundamentals. However, each module includes brief recaps of essential concepts.\n\n\n#### 2. Will these advanced models always outperform logistic regression?\n\n>Not necessarily. More complex models can achieve higher accuracy on some problems, but they also risk overfitting and are harder to interpret. Regularized logistic regression often remains a strong baseline. Part of this course teaches you to evaluate this trade-off.\n\n\n#### 3. Which libraries will we use beyond scikit-learn?\n\n>In addition to scikit-learn, we will use:\n>- **XGBoost** for gradient boosting (core model in Module 2)\n>- **LightGBM, CatBoost** for alternative boosting methods (Special Topics)\n>- **SHAP** for model interpretation\n>- **AI coding tools** (Codex, Antigravity) for vibecoding workflows\n\n\n#### 4. What is \"vibecoding\"?\n\n>Vibecoding is the practice of using AI tools to generate code from natural language descriptions. Module 5 teaches you to use tools like Codex and Antigravity to build ML models faster by describing what you want rather than writing every line of code.\n\n\n#### 5. How much time should I allocate for this course?\n\n>Plan for **3-5 hours per week**. The practical, hands-on focus means you'll spend most of your time running and modifying code.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recommended Approach\n",
    "\n",
    "1. **Complete modules in order**: Each module builds upon previous concepts\n",
    "2. **Run all code cells**: Experimentation is essential for learning\n",
    "3. **Compare results**: Track how each model performs on the same dataset\n",
    "4. **Take notes**: Document observations about model behavior and performance\n",
    "5. **Explore variations**: Modify hyperparameters and observe changes\n",
    "\n",
    "\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Before proceeding, ensure your environment has the required packages installed. Run the cell below to install or upgrade necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for Course 3\n!pip install -q xgboost shap\n# Optional (for Special Topics module):\n# !pip install -q lightgbm catboost"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify core installations\nimport sklearn\nimport xgboost\n\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"XGBoost version: {xgboost.__version__}\")\n\n# Check optional packages\ntry:\n    import lightgbm\n    print(f\"LightGBM version: {lightgbm.__version__} (for Special Topics)\")\nexcept ImportError:\n    print(\"LightGBM: not installed (optional, for Special Topics module)\")\n\ntry:\n    import catboost\n    print(f\"CatBoost version: {catboost.__version__} (for Special Topics)\")\nexcept ImportError:\n    print(\"CatBoost: not installed (optional, for Special Topics module)\")\n\ntry:\n    import shap\n    print(f\"SHAP version: {shap.__version__}\")\nexcept ImportError:\n    print(\"SHAP: not installed (install with pip install shap)\")\n\nprint(\"\\nCore packages ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "You are now ready to begin the technical content of Course 3. In **Module 1**, we start by extending logistic regression with regularization techniques that improve model performance and enable automatic feature selection.\n",
    "\n",
    "**Proceed to:** `1.1 Introduction to Regularization in Machine Learning`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}